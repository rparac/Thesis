Thu Jun  9 14:28:51 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:06.0 Off |                    0 |
| N/A   32C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            Off  | 00000000:00:07.0 Off |                    0 |
| N/A   32C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14_mx46s0fx52.csv, 58_lvcsk0yy8g.csv, 133_c0kk72b3fm.csv, 132_s5i7ngx7bu.csv, 112_7g35kmtkhr.csv, 101_9l21auewdi.csv, 69_4tlegp20n1.csv, 35_g0ldvn0rjm.csv, 143_w11kkydg7n.csv, 20_1zahygarmq.csv, 53_4t2mju8m3y.csv, 166_5lv4sjg1qv.csv, 105_3acmrslh5t.csv, 198_shm86f7xzz.csv, 155_97dannvd9q.csv, 189_4mh7pjb6hj.csv, 187_kp3lfo9rti.csv, 3_5uxjfwf4c9.csv, 76_f4jqlrtl0d.csv, 152_uhh1hem3ei.csv, 14_apv4u3qebs.csv, 127_q2ifamcjop.csv, 100_5397borrxc.csv, 129_h8c8ny4x7u.csv, 123_g66af1qsqo.csv, 95_6ikda3av09.csv, 42_kr1ailiclk.csv, 162_gzcq0eli5q.csv, 148_nekma0liim.csv, 50_tmt7xio73x.csv, 183_ex67vzlp8e.csv, 64_gjf3ihxyfj.csv, 182_icy94tmqbq.csv, 96_fe55r9bnfd.csv, 37_7swqi7vgnl.csv, 17_qvtzqld86i.csv, 185_9bjzxvbwca.csv, 4_vet6mgh5md.csv, 109_ehl2s3ah2w.csv, 36_4bg4d4bf4q.csv, 165_vcamdtt7pl.csv, 136_dd735psc8g.csv, 24_1y633kulxy.csv, 7_swfaqmk6y6.csv, 46_eg7yytwm57.csv, 12_i0qb55ygnr.csv, 8_v4bqc7mvzy.csv, 120_tfdaxlk9n2.csv, 103_v129kmq3c5.csv, 144_6g4qagf0aw.csv, 72_37k8qe6uf3.csv, 111_4dftcjzv4j.csv, 1_z949c1i541.csv, 178_7rflrddsnn.csv, 6_blz9uaf562.csv, 15_j7tl3bmirm.csv, 83_slgfcmc4xj.csv, 174_vw5a8s9vzj.csv, 9_m11ut0fzvo.csv, 199_hzu91su6jo.csv, 39_3064xz3lgl.csv, 62_6v7alh2xza.csv, 137_eusnnkienm.csv, 118_66k0pevslq.csv, 113_s8v38lnghb.csv, 90_60znzzhvt3.csv, 161_qu7m9mil3a.csv, 32_jitoqenvci.csv, 176_cl4orgir7l.csv, 65_5y9bhpar44.csv, 27_adgb2e3ry7.csv, 33_zybtjc12ef.csv, 140_rpfb54del9.csv, 70_0b98yn7zuy.csv, 158_lhrhfgkazf.csv, 10_ten3l2nuwn.csv, 48_u6mx1k6p64.csv, 29_juqgc9mrcx.csv, 195_nwk7f5mq3h.csv, 119_hdkgwx4tgr.csv, 163_g4ownm07q9.csv, 134_behtf6grhp.csv, 128_y3nqk9k0x4.csv, 93_7s7jo61qgr.csv, 125_onkofvwzgk.csv, 196_h8ebmn0b4i.csv, 186_77g5px55z1.csv, 80_zbkdixjx1u.csv, 154_i7n85wzpa3.csv, 59_tvvx1filo9.csv, 91_6nvvo85k2r.csv, 26_kcs08kmlzv.csv, 146_umxfdzx3sg.csv, 157_7onih683qj.csv, 122_hmocfdllhz.csv, 102_r9vbr9i6qn.csv, 25_zdmhvwm1pk.csv, 16_omnxwedc88.csv, 116_wf72qb1ctj.csv, 87_zzzdc841sd.csv, 168_39vz7oesxk.csv, 24_pa58yc2bsn.csv, 51_km3hwahw1m.csv, 19_sevtqojtws.csv, 21_x8u5ofrlzo.csv, 75_gginvgfabf.csv, 45_4g2awfbbtn.csv, 4_xu9afrhg0o.csv, 25_ceghr7ntgk.csv, 15_jzm9dvtw73.csv, 89_28gvnlaz8j.csv, 57_l8ux51os5l.csv, 153_mwu7z89cr2.csv, 147_qz39am2soc.csv, 67_ip2ys2xhib.csv, 78_tshg4zfas1.csv, 142_d8jaik03id.csv, 5_wa4f0flvdd.csv, 181_wk573tzjde.csv, 193_kjtylxs5x6.csv, 34_egy98kczu6.csv, 85_i7f88e7b7g.csv, 106_oa27xk4eqt.csv, 156_yk739g3286.csv, 11_0na6xsa4cd.csv, 31_jhmyyhmjh5.csv, 149_tvntt1n3zf.csv, 6_qyv4vhzoe.csv, 63_j79b3nmr83.csv, 104_jos9y36cxr.csv, 151_8k3mbw3phz.csv, 60_zry6xyk2v2.csv, 86_kvt6l8gw8d.csv, 131_bivn5or23h.csv, 175_954mvwcwet.csv, 110_zq7vq985bq.csv, 52_wfyl6gdoxn.csv, 92_u8wrf1zhh5.csv, 71_jjxizibivj.csv, 169_2y8d92hhek.csv, 114_fkhid5yyst.csv, 173_op7k51t6eh.csv, 145_1p84lqxs5t.csv, 171_evt2lbu5wz.csv, 2_wrrttduqx0.csv, 107_jfii3uf345.csv, 164_qcoq7yni4y.csv, 160_9xq61i8z5q.csv, 150_hvfxdsp6uq.csv, 16_j1uppkkwpc.csv, 26_2k5thw2ykv.csv, 172_gg34wiyfxn.csv, 27_r6anjd4ie1.csv, 11_7vk8oa1o24.csv, 138_yx53ai19ar.csv, 44_zsqcy1dp0n.csv, 68_4fxf0cl1sa.csv, 77_gup8yvrjg8.csv, 23_lce4ue1yit.csv, 197_ni25hikvxf.csv, 55_o97cc78tyd.csv, 13_xb7ph2k0m6.csv, 126_s6vu9wsf2o.csv, 124_4p9vzltfyn.csv, 135_7g73f18y8h.csv, 194_tnwwh3zbnj.csv, 99_7fcky2iv54.csv, 49_8fcz4c6oqy.csv, 84_q4rh6yklie.csv, 28_7svph4yobz.csv, 38_38doqz3unu.csv, 5_lcampqa5d8.csv, 82_3qr5zxdcxe.csv, 180_jm6cn0n08c.csv, 66_8gquz8hbpz.csv, 40_zh8fjx6tiy.csv, 141_tvldgnl6pt.csv, 30_rb578trmsj.csv, 184_hzlbex83q3.csv, 56_lxflttkcgn.csv, 22_udsmb087i5.csv, 8_hfxm8dkunn.csv, 61_l3jkp84irv.csv, 139_1qp7vpj70o.csv, 73_4s2wt6mgps.csv, 7_viucp080gt.csv, 108_qkxdz8djgt.csv, 54_07djfakfxh.csv, 98_8et5hi6nqf.csv, 74_08dh1ae49m.csv, 21_x90g7ie9t7.csv, 81_dbvr7ry56o.csv, 121_aobrexg5sv.csv, 191_yxm7fc16uc.csv, 94_gcso78g1mh.csv, 167_bk26q01ex6.csv, 41_rrwls6y9fl.csv, 170_szpuak9x3b.csv, 47_qrdef2nnjn.csv, 13_y1i6zh8y9w.csv, 188_5sjzeb4w03.csv, 130_7a6m1my07s.csv, 192_isa0drpmu4.csv, 79_wkvkhcnsn0.csv, 2_9actgs9mi7.csv, 18_o65lxoj85y.csv, 3_j0hn3a0xzt.csv, 43_1nemhlm74b.csv, 20_z3zjd5sgm4.csv, 117_mgq0ymf5hc.csv, 0_imfn3fsnqb.csv, 88_p169fqr8hl.csv, 1_ofoaso8yxd.csv, 97_ksnb8zg7e2.csv, 159_wmf9y5lb3d.csv, 179_enmmbxkd9b.csv, 18_omkydub7qy.csv, 115_iocn2dpj2o.csv,   0%|          | 0/2 [00:00<?, ?ba/s] 50%|█████     | 1/2 [00:00<00:00,  5.86ba/s]100%|██████████| 2/2 [00:00<00:00,  9.71ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 46.57ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:06<31:17,  6.98s/it]  1%|          | 2/270 [00:07<15:10,  3.40s/it]  1%|          | 3/270 [00:08<09:59,  2.24s/it]  1%|▏         | 4/270 [00:09<07:34,  1.71s/it]  2%|▏         | 5/270 [00:10<06:13,  1.41s/it]  2%|▏         | 6/270 [00:11<05:25,  1.23s/it]  3%|▎         | 7/270 [00:12<04:54,  1.12s/it]  3%|▎         | 8/270 [00:13<04:34,  1.05s/it]  3%|▎         | 9/270 [00:14<04:20,  1.00it/s]  4%|▎         | 10/270 [00:14<04:10,  1.04it/s]  4%|▍         | 11/270 [00:15<04:03,  1.06it/s]  4%|▍         | 12/270 [00:16<03:59,  1.08it/s]  5%|▍         | 13/270 [00:17<03:55,  1.09it/s]  5%|▌         | 14/270 [00:18<03:52,  1.10it/s]  6%|▌         | 15/270 [00:19<03:50,  1.11it/s]  6%|▌         | 16/270 [00:20<03:49,  1.11it/s]  6%|▋         | 17/270 [00:21<03:47,  1.11it/s]  7%|▋         | 18/270 [00:22<03:46,  1.11it/s]  7%|▋         | 19/270 [00:23<03:46,  1.11it/s]  7%|▋         | 20/270 [00:23<03:44,  1.11it/s]  8%|▊         | 21/270 [00:24<03:44,  1.11it/s]  8%|▊         | 22/270 [00:25<03:42,  1.11it/s]  9%|▊         | 23/270 [00:26<03:42,  1.11it/s]  9%|▉         | 24/270 [00:27<03:40,  1.12it/s]  9%|▉         | 25/270 [00:28<03:39,  1.12it/s] 10%|▉         | 26/270 [00:29<03:38,  1.11it/s] 10%|█         | 27/270 [00:29<03:15,  1.24it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.37it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.50it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.95it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.63it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.48it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.73it/s][A                                                
                                             [A 10%|█         | 27/270 [00:32<03:15,  1.24it/s]
100%|██████████| 7/7 [00:02<00:00,  3.73it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|█         | 28/270 [00:49<25:37,  6.35s/it] 11%|█         | 29/270 [00:50<19:21,  4.82s/it] 11%|█         | 30/270 [00:51<14:34,  3.64s/it] 11%|█▏        | 31/270 [00:52<11:13,  2.82s/it] 12%|█▏        | 32/270 [00:53<08:54,  2.24s/it] 12%|█▏        | 33/270 [00:53<07:15,  1.84s/it] 13%|█▎        | 34/270 [00:54<06:07,  1.56s/it] 13%|█▎        | 35/270 [00:55<05:20,  1.36s/it] 13%|█▎        | 36/270 [00:56<04:46,  1.22s/it] 14%|█▎        | 37/270 [00:57<04:23,  1.13s/it] 14%|█▍        | 38/270 [00:58<04:06,  1.06s/it] 14%|█▍        | 39/270 [00:59<03:54,  1.01s/it] 15%|█▍        | 40/270 [01:00<03:45,  1.02it/s] 15%|█▌        | 41/270 [01:01<03:39,  1.04it/s] 16%|█▌        | 42/270 [01:02<03:35,  1.06it/s] 16%|█▌        | 43/270 [01:03<03:31,  1.07it/s] 16%|█▋        | 44/270 [01:03<03:29,  1.08it/s] 17%|█▋        | 45/270 [01:04<03:27,  1.09it/s] 17%|█▋        | 46/270 [01:05<03:25,  1.09it/s] 17%|█▋        | 47/270 [01:06<03:23,  1.10it/s] 18%|█▊        | 48/270 [01:07<03:21,  1.10it/s] 18%|█▊        | 49/270 [01:08<03:21,  1.10it/s] 19%|█▊        | 50/270 [01:09<03:20,  1.10it/s]                                                 19%|█▊        | 50/270 [01:09<03:20,  1.10it/s] 19%|█▉        | 51/270 [01:10<03:20,  1.09it/s] 19%|█▉        | 52/270 [01:11<03:19,  1.09it/s] 20%|█▉        | 53/270 [01:12<03:18,  1.09it/s] 20%|██        | 54/270 [01:12<02:56,  1.22it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 1.3962358236312866, 'eval_f1': 0.33125758321036236, 'eval_runtime': 2.9305, 'eval_samples_per_second': 145.37, 'eval_steps_per_second': 2.389, 'epoch': 1.0}
{'loss': 1.3734, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.47it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.46it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.84it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.56it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.42it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.69it/s][A                                                
                                             [A 20%|██        | 54/270 [01:15<02:56,  1.22it/s]
100%|██████████| 7/7 [00:02<00:00,  3.69it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██        | 55/270 [01:31<22:41,  6.33s/it] 21%|██        | 56/270 [01:32<16:47,  4.71s/it] 21%|██        | 57/270 [01:33<12:39,  3.57s/it] 21%|██▏       | 58/270 [01:34<09:47,  2.77s/it] 22%|██▏       | 59/270 [01:35<07:47,  2.21s/it] 22%|██▏       | 60/270 [01:36<06:22,  1.82s/it] 23%|██▎       | 61/270 [01:37<05:24,  1.55s/it] 23%|██▎       | 62/270 [01:38<04:43,  1.36s/it] 23%|██▎       | 63/270 [01:39<04:13,  1.23s/it] 24%|██▎       | 64/270 [01:40<03:53,  1.13s/it] 24%|██▍       | 65/270 [01:41<03:38,  1.07s/it] 24%|██▍       | 66/270 [01:41<03:27,  1.02s/it] 25%|██▍       | 67/270 [01:42<03:20,  1.01it/s] 25%|██▌       | 68/270 [01:43<03:15,  1.03it/s] 26%|██▌       | 69/270 [01:44<03:11,  1.05it/s] 26%|██▌       | 70/270 [01:45<03:08,  1.06it/s] 26%|██▋       | 71/270 [01:46<03:06,  1.07it/s] 27%|██▋       | 72/270 [01:47<03:04,  1.07it/s] 27%|██▋       | 73/270 [01:48<03:02,  1.08it/s] 27%|██▋       | 74/270 [01:49<03:01,  1.08it/s] 28%|██▊       | 75/270 [01:50<03:00,  1.08it/s] 28%|██▊       | 76/270 [01:51<02:59,  1.08it/s] 29%|██▊       | 77/270 [01:52<02:58,  1.08it/s] 29%|██▉       | 78/270 [01:53<02:58,  1.08it/s] 29%|██▉       | 79/270 [01:53<02:56,  1.08it/s] 30%|██▉       | 80/270 [01:54<02:56,  1.08it/s] 30%|███       | 81/270 [01:55<02:37,  1.20it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.7254599928855896, 'eval_f1': 0.6739345407354017, 'eval_runtime': 2.5874, 'eval_samples_per_second': 164.645, 'eval_steps_per_second': 2.705, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.19it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.39it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.82it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.53it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.37it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.65it/s][A                                                
                                             [A 30%|███       | 81/270 [01:58<02:37,  1.20it/s]
100%|██████████| 7/7 [00:02<00:00,  3.65it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|███       | 82/270 [02:15<20:26,  6.52s/it] 31%|███       | 83/270 [02:16<15:05,  4.84s/it] 31%|███       | 84/270 [02:17<11:22,  3.67s/it] 31%|███▏      | 85/270 [02:18<08:46,  2.85s/it] 32%|███▏      | 86/270 [02:19<06:57,  2.27s/it] 32%|███▏      | 87/270 [02:19<05:41,  1.87s/it] 33%|███▎      | 88/270 [02:20<04:48,  1.59s/it] 33%|███▎      | 89/270 [02:21<04:11,  1.39s/it] 33%|███▎      | 90/270 [02:22<03:44,  1.25s/it] 34%|███▎      | 91/270 [02:23<03:26,  1.15s/it] 34%|███▍      | 92/270 [02:24<03:13,  1.09s/it] 34%|███▍      | 93/270 [02:25<03:04,  1.04s/it] 35%|███▍      | 94/270 [02:26<02:57,  1.01s/it] 35%|███▌      | 95/270 [02:27<02:52,  1.02it/s] 36%|███▌      | 96/270 [02:28<02:48,  1.03it/s] 36%|███▌      | 97/270 [02:29<02:45,  1.04it/s] 36%|███▋      | 98/270 [02:30<02:43,  1.05it/s] 37%|███▋      | 99/270 [02:31<02:40,  1.06it/s] 37%|███▋      | 100/270 [02:32<02:39,  1.07it/s]                                                  37%|███▋      | 100/270 [02:32<02:39,  1.07it/s] 37%|███▋      | 101/270 [02:32<02:38,  1.07it/s] 38%|███▊      | 102/270 [02:33<02:37,  1.07it/s] 38%|███▊      | 103/270 [02:34<02:36,  1.07it/s] 39%|███▊      | 104/270 [02:35<02:34,  1.07it/s] 39%|███▉      | 105/270 [02:36<02:34,  1.07it/s] 39%|███▉      | 106/270 [02:37<02:34,  1.06it/s] 40%|███▉      | 107/270 [02:38<02:33,  1.06it/s] 40%|████      | 108/270 [02:39<02:15,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.4433732032775879, 'eval_f1': 0.8469513765930843, 'eval_runtime': 2.6051, 'eval_samples_per_second': 163.528, 'eval_steps_per_second': 2.687, 'epoch': 3.0}
{'loss': 0.5714, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.11it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.30it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.83it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.54it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.37it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.60it/s][A                                                 
                                             [A 40%|████      | 108/270 [02:41<02:15,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.60it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████      | 109/270 [02:57<16:23,  6.11s/it] 41%|████      | 110/270 [02:58<12:08,  4.56s/it] 41%|████      | 111/270 [02:59<09:11,  3.47s/it] 41%|████▏     | 112/270 [03:00<07:07,  2.71s/it] 42%|████▏     | 113/270 [03:01<05:41,  2.17s/it] 42%|████▏     | 114/270 [03:02<04:40,  1.80s/it] 43%|████▎     | 115/270 [03:03<03:59,  1.54s/it] 43%|████▎     | 116/270 [03:04<03:29,  1.36s/it] 43%|████▎     | 117/270 [03:05<03:08,  1.23s/it] 44%|████▎     | 118/270 [03:05<02:53,  1.14s/it] 44%|████▍     | 119/270 [03:06<02:42,  1.08s/it] 44%|████▍     | 120/270 [03:07<02:34,  1.03s/it] 45%|████▍     | 121/270 [03:08<02:29,  1.01s/it] 45%|████▌     | 122/270 [03:09<02:25,  1.02it/s] 46%|████▌     | 123/270 [03:10<02:22,  1.03it/s] 46%|████▌     | 124/270 [03:11<02:19,  1.05it/s] 46%|████▋     | 125/270 [03:12<02:17,  1.05it/s] 47%|████▋     | 126/270 [03:13<02:16,  1.06it/s] 47%|████▋     | 127/270 [03:14<02:15,  1.06it/s] 47%|████▋     | 128/270 [03:15<02:13,  1.06it/s] 48%|████▊     | 129/270 [03:16<02:12,  1.06it/s] 48%|████▊     | 130/270 [03:17<02:11,  1.06it/s] 49%|████▊     | 131/270 [03:18<02:10,  1.07it/s] 49%|████▉     | 132/270 [03:19<02:17,  1.00it/s] 49%|████▉     | 133/270 [03:20<02:14,  1.02it/s] 50%|████▉     | 134/270 [03:21<02:12,  1.03it/s] 50%|█████     | 135/270 [03:21<01:56,  1.16it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.30211183428764343, 'eval_f1': 0.9043129305562327, 'eval_runtime': 2.6055, 'eval_samples_per_second': 163.499, 'eval_steps_per_second': 2.687, 'epoch': 4.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.03it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.27it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.70it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.44it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.31it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.58it/s][A                                                 
                                             [A 50%|█████     | 135/270 [03:24<01:56,  1.16it/s]
100%|██████████| 7/7 [00:02<00:00,  3.58it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 136/270 [03:39<13:32,  6.07s/it] 51%|█████     | 137/270 [03:40<10:02,  4.53s/it] 51%|█████     | 138/270 [03:41<07:35,  3.45s/it] 51%|█████▏    | 139/270 [03:42<05:52,  2.69s/it] 52%|█████▏    | 140/270 [03:43<04:41,  2.17s/it] 52%|█████▏    | 141/270 [03:44<03:51,  1.80s/it] 53%|█████▎    | 142/270 [03:45<03:17,  1.54s/it] 53%|█████▎    | 143/270 [03:46<02:52,  1.36s/it] 53%|█████▎    | 144/270 [03:47<02:35,  1.23s/it] 54%|█████▎    | 145/270 [03:48<02:22,  1.14s/it] 54%|█████▍    | 146/270 [03:49<02:13,  1.08s/it] 54%|█████▍    | 147/270 [03:50<02:07,  1.04s/it] 55%|█████▍    | 148/270 [03:51<02:02,  1.00s/it] 55%|█████▌    | 149/270 [03:52<01:59,  1.01it/s] 56%|█████▌    | 150/270 [03:53<01:56,  1.03it/s]                                                  56%|█████▌    | 150/270 [03:53<01:56,  1.03it/s] 56%|█████▌    | 151/270 [03:54<01:55,  1.03it/s] 56%|█████▋    | 152/270 [03:54<01:53,  1.04it/s] 57%|█████▋    | 153/270 [03:55<01:51,  1.05it/s] 57%|█████▋    | 154/270 [03:56<01:50,  1.05it/s] 57%|█████▋    | 155/270 [03:57<01:48,  1.06it/s] 58%|█████▊    | 156/270 [03:58<01:48,  1.05it/s] 58%|█████▊    | 157/270 [03:59<01:47,  1.06it/s] 59%|█████▊    | 158/270 [04:00<01:45,  1.06it/s] 59%|█████▉    | 159/270 [04:01<01:44,  1.06it/s] 59%|█████▉    | 160/270 [04:02<01:44,  1.05it/s] 60%|█████▉    | 161/270 [04:03<01:43,  1.05it/s] 60%|██████    | 162/270 [04:04<01:31,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.2535764276981354, 'eval_f1': 0.9203271909917653, 'eval_runtime': 2.6237, 'eval_samples_per_second': 162.366, 'eval_steps_per_second': 2.668, 'epoch': 5.0}
{'loss': 0.2683, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.96it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.63it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                 
                                             [A 60%|██████    | 162/270 [04:06<01:31,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████    | 163/270 [04:23<11:21,  6.37s/it] 61%|██████    | 164/270 [04:24<08:22,  4.74s/it] 61%|██████    | 165/270 [04:25<06:17,  3.60s/it] 61%|██████▏   | 166/270 [04:26<04:51,  2.80s/it] 62%|██████▏   | 167/270 [04:27<03:50,  2.24s/it] 62%|██████▏   | 168/270 [04:28<03:08,  1.85s/it] 63%|██████▎   | 169/270 [04:28<02:38,  1.57s/it] 63%|██████▎   | 170/270 [04:29<02:18,  1.39s/it] 63%|██████▎   | 171/270 [04:30<02:04,  1.25s/it] 64%|██████▎   | 172/270 [04:31<01:53,  1.16s/it] 64%|██████▍   | 173/270 [04:32<01:46,  1.09s/it] 64%|██████▍   | 174/270 [04:33<01:40,  1.05s/it] 65%|██████▍   | 175/270 [04:34<01:36,  1.01s/it] 65%|██████▌   | 176/270 [04:35<01:33,  1.01it/s] 66%|██████▌   | 177/270 [04:36<01:30,  1.03it/s] 66%|██████▌   | 178/270 [04:37<01:28,  1.04it/s] 66%|██████▋   | 179/270 [04:38<01:27,  1.04it/s] 67%|██████▋   | 180/270 [04:39<01:25,  1.05it/s] 67%|██████▋   | 181/270 [04:40<01:24,  1.05it/s] 67%|██████▋   | 182/270 [04:41<01:23,  1.05it/s] 68%|██████▊   | 183/270 [04:42<01:22,  1.05it/s] 68%|██████▊   | 184/270 [04:43<01:21,  1.05it/s] 69%|██████▊   | 185/270 [04:44<01:20,  1.06it/s] 69%|██████▉   | 186/270 [04:44<01:19,  1.06it/s] 69%|██████▉   | 187/270 [04:45<01:18,  1.06it/s] 70%|██████▉   | 188/270 [04:46<01:17,  1.06it/s] 70%|███████   | 189/270 [04:47<01:08,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.21799050271511078, 'eval_f1': 0.9349208070045814, 'eval_runtime': 2.6724, 'eval_samples_per_second': 159.41, 'eval_steps_per_second': 2.619, 'epoch': 6.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.23it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.32it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.76it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.46it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.33it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.59it/s][A                                                 
                                             [A 70%|███████   | 189/270 [04:50<01:08,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.59it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|███████   | 190/270 [05:07<08:54,  6.68s/it] 71%|███████   | 191/270 [05:08<06:31,  4.95s/it] 71%|███████   | 192/270 [05:09<04:52,  3.75s/it] 71%|███████▏  | 193/270 [05:10<03:44,  2.91s/it] 72%|███████▏  | 194/270 [05:11<02:56,  2.32s/it] 72%|███████▏  | 195/270 [05:12<02:23,  1.91s/it] 73%|███████▎  | 196/270 [05:13<01:59,  1.62s/it] 73%|███████▎  | 197/270 [05:14<01:43,  1.41s/it] 73%|███████▎  | 198/270 [05:15<01:31,  1.27s/it] 74%|███████▎  | 199/270 [05:16<01:23,  1.18s/it] 74%|███████▍  | 200/270 [05:17<01:17,  1.11s/it]                                                  74%|███████▍  | 200/270 [05:17<01:17,  1.11s/it] 74%|███████▍  | 201/270 [05:18<01:13,  1.06s/it] 75%|███████▍  | 202/270 [05:19<01:10,  1.03s/it] 75%|███████▌  | 203/270 [05:20<01:07,  1.01s/it] 76%|███████▌  | 204/270 [05:21<01:05,  1.01it/s] 76%|███████▌  | 205/270 [05:21<01:03,  1.02it/s] 76%|███████▋  | 206/270 [05:22<01:01,  1.04it/s] 77%|███████▋  | 207/270 [05:23<01:00,  1.04it/s] 77%|███████▋  | 208/270 [05:24<00:59,  1.04it/s] 77%|███████▋  | 209/270 [05:25<00:58,  1.05it/s] 78%|███████▊  | 210/270 [05:26<00:57,  1.05it/s] 78%|███████▊  | 211/270 [05:27<00:56,  1.05it/s] 79%|███████▊  | 212/270 [05:28<00:55,  1.05it/s] 79%|███████▉  | 213/270 [05:29<00:53,  1.06it/s] 79%|███████▉  | 214/270 [05:30<00:52,  1.06it/s] 80%|███████▉  | 215/270 [05:31<00:52,  1.06it/s] 80%|████████  | 216/270 [05:32<00:45,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.2083725929260254, 'eval_f1': 0.9396932924845226, 'eval_runtime': 2.6109, 'eval_samples_per_second': 163.164, 'eval_steps_per_second': 2.681, 'epoch': 7.0}
{'loss': 0.1739, 'learning_rate': 1.5687918106563326e-06, 'epoch': 7.41}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.05it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.26it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.71it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.42it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.29it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                 
                                             [A 80%|████████  | 216/270 [05:34<00:45,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-216
Configuration saved in training-output/checkpoint-216/config.json
Model weights saved in training-output/checkpoint-216/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-216/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-216/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████  | 217/270 [05:50<05:16,  5.98s/it] 81%|████████  | 218/270 [05:50<03:52,  4.46s/it] 81%|████████  | 219/270 [05:51<02:53,  3.41s/it] 81%|████████▏ | 220/270 [05:52<02:13,  2.66s/it] 82%|████████▏ | 221/270 [05:53<01:45,  2.14s/it] 82%|████████▏ | 222/270 [05:54<01:25,  1.78s/it] 83%|████████▎ | 223/270 [05:55<01:11,  1.53s/it] 83%|████████▎ | 224/270 [05:56<01:02,  1.35s/it] 83%|████████▎ | 225/270 [05:57<00:55,  1.23s/it] 84%|████████▎ | 226/270 [05:58<00:50,  1.15s/it] 84%|████████▍ | 227/270 [05:59<00:46,  1.08s/it] 84%|████████▍ | 228/270 [06:00<00:43,  1.04s/it] 85%|████████▍ | 229/270 [06:01<00:41,  1.01s/it] 85%|████████▌ | 230/270 [06:02<00:39,  1.01it/s] 86%|████████▌ | 231/270 [06:03<00:37,  1.03it/s] 86%|████████▌ | 232/270 [06:04<00:36,  1.04it/s] 86%|████████▋ | 233/270 [06:05<00:35,  1.05it/s] 87%|████████▋ | 234/270 [06:05<00:34,  1.05it/s] 87%|████████▋ | 235/270 [06:06<00:33,  1.05it/s] 87%|████████▋ | 236/270 [06:07<00:32,  1.05it/s] 88%|████████▊ | 237/270 [06:08<00:31,  1.05it/s] 88%|████████▊ | 238/270 [06:09<00:30,  1.06it/s] 89%|████████▊ | 239/270 [06:10<00:29,  1.06it/s] 89%|████████▉ | 240/270 [06:11<00:28,  1.06it/s] 89%|████████▉ | 241/270 [06:12<00:27,  1.06it/s] 90%|████████▉ | 242/270 [06:13<00:26,  1.06it/s] 90%|█████████ | 243/270 [06:14<00:22,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.21136675775051117, 'eval_f1': 0.939553116803688, 'eval_runtime': 2.7708, 'eval_samples_per_second': 153.747, 'eval_steps_per_second': 2.526, 'epoch': 8.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.01it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.29it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.71it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.45it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  2.73it/s][A
100%|██████████| 7/7 [00:02<00:00,  3.08it/s][A                                                 
                                             [A 90%|█████████ | 243/270 [06:16<00:22,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.08it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-243
Configuration saved in training-output/checkpoint-243/config.json
Model weights saved in training-output/checkpoint-243/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-243/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-243/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|█████████ | 244/270 [06:31<02:32,  5.88s/it] 91%|█████████ | 245/270 [06:32<01:49,  4.40s/it] 91%|█████████ | 246/270 [06:33<01:20,  3.36s/it] 91%|█████████▏| 247/270 [06:34<01:00,  2.63s/it] 92%|█████████▏| 248/270 [06:35<00:46,  2.12s/it] 92%|█████████▏| 249/270 [06:36<00:37,  1.76s/it] 93%|█████████▎| 250/270 [06:37<00:30,  1.52s/it]                                                  93%|█████████▎| 250/270 [06:37<00:30,  1.52s/it] 93%|█████████▎| 251/270 [06:38<00:25,  1.34s/it] 93%|█████████▎| 252/270 [06:39<00:21,  1.22s/it] 94%|█████████▎| 253/270 [06:40<00:19,  1.14s/it] 94%|█████████▍| 254/270 [06:41<00:17,  1.08s/it] 94%|█████████▍| 255/270 [06:42<00:15,  1.04s/it] 95%|█████████▍| 256/270 [06:43<00:14,  1.01s/it] 95%|█████████▌| 257/270 [06:43<00:12,  1.01it/s] 96%|█████████▌| 258/270 [06:44<00:11,  1.03it/s] 96%|█████████▌| 259/270 [06:45<00:10,  1.04it/s] 96%|█████████▋| 260/270 [06:46<00:09,  1.05it/s] 97%|█████████▋| 261/270 [06:47<00:08,  1.05it/s] 97%|█████████▋| 262/270 [06:48<00:07,  1.06it/s] 97%|█████████▋| 263/270 [06:49<00:06,  1.06it/s] 98%|█████████▊| 264/270 [06:50<00:05,  1.05it/s] 98%|█████████▊| 265/270 [06:51<00:04,  1.06it/s] 99%|█████████▊| 266/270 [06:52<00:03,  1.06it/s] 99%|█████████▉| 267/270 [06:53<00:02,  1.06it/s] 99%|█████████▉| 268/270 [06:54<00:01,  1.06it/s]100%|█████████▉| 269/270 [06:55<00:00,  1.06it/s]100%|██████████| 270/270 [06:55<00:00,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.20390105247497559, 'eval_f1': 0.937068389319552, 'eval_runtime': 2.8252, 'eval_samples_per_second': 150.786, 'eval_steps_per_second': 2.478, 'epoch': 9.0}
{'loss': 0.1406, 'learning_rate': 1.3477564710088097e-07, 'epoch': 9.26}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.12it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.25it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.42it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.31it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.57it/s][A                                                 
                                             [A100%|██████████| 270/270 [06:58<00:00,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.57it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-270
Configuration saved in training-output/checkpoint-270/config.json
Model weights saved in training-output/checkpoint-270/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-270/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-270/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-189 (score: 0.9396932924845226).
                                                 100%|██████████| 270/270 [07:12<00:00,  1.18it/s]100%|██████████| 270/270 [07:12<00:00,  1.60s/it]
{'eval_loss': 0.20071053504943848, 'eval_f1': 0.937068389319552, 'eval_runtime': 2.6279, 'eval_samples_per_second': 162.104, 'eval_steps_per_second': 2.664, 'epoch': 10.0}
{'train_runtime': 432.7534, 'train_samples_per_second': 39.283, 'train_steps_per_second': 0.624, 'train_loss': 0.4766688117274532, 'epoch': 10.0}
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:07, 13.70it/s]  4%|▎         | 4/107 [00:00<00:06, 16.73it/s]  7%|▋         | 7/107 [00:00<00:04, 20.15it/s]  9%|▉         | 10/107 [00:00<00:04, 23.05it/s] 12%|█▏        | 13/107 [00:00<00:03, 24.87it/s] 15%|█▍        | 16/107 [00:00<00:03, 25.93it/s] 18%|█▊        | 19/107 [00:00<00:03, 26.54it/s] 21%|██        | 22/107 [00:00<00:03, 27.06it/s] 23%|██▎       | 25/107 [00:01<00:02, 27.43it/s] 26%|██▌       | 28/107 [00:01<00:02, 27.65it/s] 29%|██▉       | 31/107 [00:01<00:02, 27.61it/s] 32%|███▏      | 34/107 [00:01<00:02, 27.54it/s] 35%|███▍      | 37/107 [00:01<00:02, 27.48it/s] 37%|███▋      | 40/107 [00:01<00:02, 27.64it/s] 40%|████      | 43/107 [00:01<00:02, 27.76it/s] 43%|████▎     | 46/107 [00:01<00:02, 27.76it/s] 46%|████▌     | 49/107 [00:01<00:02, 27.74it/s] 49%|████▊     | 52/107 [00:01<00:01, 27.87it/s] 51%|█████▏    | 55/107 [00:02<00:01, 27.96it/s] 54%|█████▍    | 58/107 [00:02<00:01, 27.82it/s] 57%|█████▋    | 61/107 [00:02<00:01, 27.76it/s] 60%|█████▉    | 64/107 [00:02<00:01, 27.76it/s] 63%|██████▎   | 67/107 [00:02<00:01, 27.76it/s] 65%|██████▌   | 70/107 [00:02<00:01, 27.85it/s] 68%|██████▊   | 73/107 [00:02<00:01, 27.96it/s] 71%|███████   | 76/107 [00:02<00:01, 27.96it/s] 74%|███████▍  | 79/107 [00:02<00:01, 27.91it/s] 77%|███████▋  | 82/107 [00:03<00:00, 28.04it/s] 79%|███████▉  | 85/107 [00:03<00:00, 27.94it/s] 82%|████████▏ | 88/107 [00:03<00:00, 27.85it/s] 85%|████████▌ | 91/107 [00:03<00:00, 27.74it/s] 88%|████████▊ | 94/107 [00:03<00:00, 27.81it/s] 91%|█████████ | 97/107 [00:03<00:00, 27.75it/s] 93%|█████████▎| 100/107 [00:03<00:00, 27.80it/s] 96%|█████████▋| 103/107 [00:03<00:00, 27.83it/s] 99%|█████████▉| 106/107 [00:03<00:00, 27.90it/s]100%|██████████| 107/107 [00:03<00:00, 27.13it/s]
Model finished with accuracy: 0.9413145539906104, macro-f1: 0.9396932924845226
Confusion matrix:
[[ 81   2   0   1   1]
 [  1  69   1   4   0]
 [  0   2  57   3   0]
 [  0   1   3 103   0]
 [  5   0   0   1  91]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14_mx46s0fx52.csv, 58_lvcsk0yy8g.csv, 133_c0kk72b3fm.csv, 132_s5i7ngx7bu.csv, 112_7g35kmtkhr.csv, 101_9l21auewdi.csv, 69_4tlegp20n1.csv, 35_g0ldvn0rjm.csv, 143_w11kkydg7n.csv, 20_1zahygarmq.csv, 53_4t2mju8m3y.csv, 166_5lv4sjg1qv.csv, 105_3acmrslh5t.csv, 198_shm86f7xzz.csv, 155_97dannvd9q.csv, 189_4mh7pjb6hj.csv, 187_kp3lfo9rti.csv, 3_5uxjfwf4c9.csv, 76_f4jqlrtl0d.csv, 152_uhh1hem3ei.csv, 14_apv4u3qebs.csv, 127_q2ifamcjop.csv, 100_5397borrxc.csv, 129_h8c8ny4x7u.csv, 123_g66af1qsqo.csv, 95_6ikda3av09.csv, 42_kr1ailiclk.csv, 162_gzcq0eli5q.csv, 148_nekma0liim.csv, 50_tmt7xio73x.csv, 183_ex67vzlp8e.csv, 64_gjf3ihxyfj.csv, 182_icy94tmqbq.csv, 96_fe55r9bnfd.csv, 37_7swqi7vgnl.csv, 17_qvtzqld86i.csv, 185_9bjzxvbwca.csv, 4_vet6mgh5md.csv, 109_ehl2s3ah2w.csv, 36_4bg4d4bf4q.csv, 165_vcamdtt7pl.csv, 136_dd735psc8g.csv, 24_1y633kulxy.csv, 7_swfaqmk6y6.csv, 46_eg7yytwm57.csv, 12_i0qb55ygnr.csv, 8_v4bqc7mvzy.csv, 120_tfdaxlk9n2.csv, 103_v129kmq3c5.csv, 144_6g4qagf0aw.csv, 72_37k8qe6uf3.csv, 111_4dftcjzv4j.csv, 1_z949c1i541.csv, 178_7rflrddsnn.csv, 6_blz9uaf562.csv, 15_j7tl3bmirm.csv, 83_slgfcmc4xj.csv, 174_vw5a8s9vzj.csv, 9_m11ut0fzvo.csv, 199_hzu91su6jo.csv, 39_3064xz3lgl.csv, 62_6v7alh2xza.csv, 137_eusnnkienm.csv, 118_66k0pevslq.csv, 113_s8v38lnghb.csv, 90_60znzzhvt3.csv, 161_qu7m9mil3a.csv, 32_jitoqenvci.csv, 176_cl4orgir7l.csv, 65_5y9bhpar44.csv, 27_adgb2e3ry7.csv, 33_zybtjc12ef.csv, 140_rpfb54del9.csv, 70_0b98yn7zuy.csv, 158_lhrhfgkazf.csv, 10_ten3l2nuwn.csv, 48_u6mx1k6p64.csv, 29_juqgc9mrcx.csv, 195_nwk7f5mq3h.csv, 119_hdkgwx4tgr.csv, 163_g4ownm07q9.csv, 134_behtf6grhp.csv, 128_y3nqk9k0x4.csv, 93_7s7jo61qgr.csv, 125_onkofvwzgk.csv, 196_h8ebmn0b4i.csv, 186_77g5px55z1.csv, 80_zbkdixjx1u.csv, 154_i7n85wzpa3.csv, 59_tvvx1filo9.csv, 91_6nvvo85k2r.csv, 26_kcs08kmlzv.csv, 146_umxfdzx3sg.csv, 157_7onih683qj.csv, 122_hmocfdllhz.csv, 102_r9vbr9i6qn.csv, 25_zdmhvwm1pk.csv, 16_omnxwedc88.csv, 116_wf72qb1ctj.csv, 87_zzzdc841sd.csv, 168_39vz7oesxk.csv, 24_pa58yc2bsn.csv, 51_km3hwahw1m.csv, 19_sevtqojtws.csv, 21_x8u5ofrlzo.csv, 75_gginvgfabf.csv, 45_4g2awfbbtn.csv, 4_xu9afrhg0o.csv, 25_ceghr7ntgk.csv, 15_jzm9dvtw73.csv, 89_28gvnlaz8j.csv, 57_l8ux51os5l.csv, 153_mwu7z89cr2.csv, 147_qz39am2soc.csv, 67_ip2ys2xhib.csv, 78_tshg4zfas1.csv, 142_d8jaik03id.csv, 5_wa4f0flvdd.csv, 181_wk573tzjde.csv, 193_kjtylxs5x6.csv, 34_egy98kczu6.csv, 85_i7f88e7b7g.csv, 106_oa27xk4eqt.csv, 156_yk739g3286.csv, 11_0na6xsa4cd.csv, 31_jhmyyhmjh5.csv, 149_tvntt1n3zf.csv, 6_qyv4vhzoe.csv, 63_j79b3nmr83.csv, 104_jos9y36cxr.csv, 151_8k3mbw3phz.csv, 60_zry6xyk2v2.csv, 86_kvt6l8gw8d.csv, 131_bivn5or23h.csv, 175_954mvwcwet.csv, 110_zq7vq985bq.csv, 52_wfyl6gdoxn.csv, 92_u8wrf1zhh5.csv, 71_jjxizibivj.csv, 169_2y8d92hhek.csv, 114_fkhid5yyst.csv, 173_op7k51t6eh.csv, 145_1p84lqxs5t.csv, 171_evt2lbu5wz.csv, 2_wrrttduqx0.csv, 107_jfii3uf345.csv, 164_qcoq7yni4y.csv, 160_9xq61i8z5q.csv, 150_hvfxdsp6uq.csv, 16_j1uppkkwpc.csv, 26_2k5thw2ykv.csv, 172_gg34wiyfxn.csv, 27_r6anjd4ie1.csv, 11_7vk8oa1o24.csv, 138_yx53ai19ar.csv, 44_zsqcy1dp0n.csv, 68_4fxf0cl1sa.csv, 77_gup8yvrjg8.csv, 23_lce4ue1yit.csv, 197_ni25hikvxf.csv, 55_o97cc78tyd.csv, 13_xb7ph2k0m6.csv, 126_s6vu9wsf2o.csv, 124_4p9vzltfyn.csv, 135_7g73f18y8h.csv, 194_tnwwh3zbnj.csv, 99_7fcky2iv54.csv, 49_8fcz4c6oqy.csv, 84_q4rh6yklie.csv, 28_7svph4yobz.csv, 38_38doqz3unu.csv, 5_lcampqa5d8.csv, 82_3qr5zxdcxe.csv, 180_jm6cn0n08c.csv, 66_8gquz8hbpz.csv, 40_zh8fjx6tiy.csv, 141_tvldgnl6pt.csv, 30_rb578trmsj.csv, 184_hzlbex83q3.csv, 56_lxflttkcgn.csv, 22_udsmb087i5.csv, 8_hfxm8dkunn.csv, 61_l3jkp84irv.csv, 139_1qp7vpj70o.csv, 73_4s2wt6mgps.csv, 7_viucp080gt.csv, 108_qkxdz8djgt.csv, 54_07djfakfxh.csv, 98_8et5hi6nqf.csv, 74_08dh1ae49m.csv, 21_x90g7ie9t7.csv, 81_dbvr7ry56o.csv, 121_aobrexg5sv.csv, 191_yxm7fc16uc.csv, 94_gcso78g1mh.csv, 167_bk26q01ex6.csv, 41_rrwls6y9fl.csv, 170_szpuak9x3b.csv, 47_qrdef2nnjn.csv, 13_y1i6zh8y9w.csv, 188_5sjzeb4w03.csv, 130_7a6m1my07s.csv, 192_isa0drpmu4.csv, 79_wkvkhcnsn0.csv, 2_9actgs9mi7.csv, 18_o65lxoj85y.csv, 3_j0hn3a0xzt.csv, 43_1nemhlm74b.csv, 20_z3zjd5sgm4.csv, 117_mgq0ymf5hc.csv, 0_imfn3fsnqb.csv, 88_p169fqr8hl.csv, 1_ofoaso8yxd.csv, 97_ksnb8zg7e2.csv, 159_wmf9y5lb3d.csv, 179_enmmbxkd9b.csv, 18_omkydub7qy.csv, 115_iocn2dpj2o.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|██████████| 2/2 [00:00<00:00, 18.04ba/s]100%|██████████| 2/2 [00:00<00:00, 17.98ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 41.20ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:04<20:30,  4.58s/it]  1%|          | 2/270 [00:05<10:49,  2.42s/it]  1%|          | 3/270 [00:06<07:51,  1.77s/it]  1%|▏         | 4/270 [00:07<06:20,  1.43s/it]  2%|▏         | 5/270 [00:08<05:29,  1.24s/it]  2%|▏         | 6/270 [00:09<04:59,  1.13s/it]  3%|▎         | 7/270 [00:10<04:39,  1.06s/it]  3%|▎         | 8/270 [00:11<04:26,  1.02s/it]  3%|▎         | 9/270 [00:11<04:18,  1.01it/s]  4%|▎         | 10/270 [00:12<04:11,  1.03it/s]  4%|▍         | 11/270 [00:13<04:07,  1.04it/s]  4%|▍         | 12/270 [00:14<04:03,  1.06it/s]  5%|▍         | 13/270 [00:15<04:01,  1.07it/s]  5%|▌         | 14/270 [00:16<03:59,  1.07it/s]  6%|▌         | 15/270 [00:17<03:57,  1.07it/s]  6%|▌         | 16/270 [00:18<03:55,  1.08it/s]  6%|▋         | 17/270 [00:19<03:54,  1.08it/s]  7%|▋         | 18/270 [00:20<03:53,  1.08it/s]  7%|▋         | 19/270 [00:21<03:52,  1.08it/s]  7%|▋         | 20/270 [00:22<03:51,  1.08it/s]  8%|▊         | 21/270 [00:23<03:51,  1.08it/s]  8%|▊         | 22/270 [00:24<03:51,  1.07it/s]  9%|▊         | 23/270 [00:24<03:50,  1.07it/s]  9%|▉         | 24/270 [00:25<03:48,  1.08it/s]  9%|▉         | 25/270 [00:26<03:47,  1.08it/s] 10%|▉         | 26/270 [00:27<03:47,  1.07it/s] 10%|█         | 27/270 [00:28<03:22,  1.20it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.25it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.30it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.72it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.47it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.30it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                
                                             [A 10%|█         | 27/270 [00:31<03:22,  1.20it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|█         | 28/270 [00:46<23:56,  5.94s/it] 11%|█         | 29/270 [00:47<18:01,  4.49s/it] 11%|█         | 30/270 [00:48<13:40,  3.42s/it] 11%|█▏        | 31/270 [00:49<10:37,  2.67s/it] 12%|█▏        | 32/270 [00:50<08:31,  2.15s/it] 12%|█▏        | 33/270 [00:51<07:02,  1.78s/it] 13%|█▎        | 34/270 [00:51<06:00,  1.53s/it] 13%|█▎        | 35/270 [00:52<05:16,  1.35s/it] 13%|█▎        | 36/270 [00:53<04:45,  1.22s/it] 14%|█▎        | 37/270 [00:54<04:24,  1.14s/it] 14%|█▍        | 38/270 [00:55<04:10,  1.08s/it] 14%|█▍        | 39/270 [00:56<03:59,  1.04s/it] 15%|█▍        | 40/270 [00:57<03:51,  1.01s/it] 15%|█▌        | 41/270 [00:58<03:45,  1.02it/s] 16%|█▌        | 42/270 [00:59<03:40,  1.03it/s] 16%|█▌        | 43/270 [01:00<03:37,  1.05it/s] 16%|█▋        | 44/270 [01:01<03:34,  1.05it/s] 17%|█▋        | 45/270 [01:02<03:32,  1.06it/s] 17%|█▋        | 46/270 [01:03<03:30,  1.06it/s] 17%|█▋        | 47/270 [01:04<03:29,  1.07it/s] 18%|█▊        | 48/270 [01:05<03:28,  1.07it/s] 18%|█▊        | 49/270 [01:05<03:27,  1.07it/s] 19%|█▊        | 50/270 [01:06<03:26,  1.07it/s]                                                 19%|█▊        | 50/270 [01:06<03:26,  1.07it/s] 19%|█▉        | 51/270 [01:07<03:26,  1.06it/s] 19%|█▉        | 52/270 [01:08<03:24,  1.07it/s] 20%|█▉        | 53/270 [01:09<03:23,  1.07it/s] 20%|██        | 54/270 [01:10<03:01,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 1.371059536933899, 'eval_f1': 0.5450749144509346, 'eval_runtime': 2.7678, 'eval_samples_per_second': 153.913, 'eval_steps_per_second': 2.529, 'epoch': 1.0}
{'loss': 1.352, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.08it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.27it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.72it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.45it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.30it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                
                                             [A 20%|██        | 54/270 [01:12<03:01,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██        | 55/270 [01:27<21:04,  5.88s/it] 21%|██        | 56/270 [01:28<15:41,  4.40s/it] 21%|██        | 57/270 [01:29<11:54,  3.36s/it] 21%|██▏       | 58/270 [01:30<09:17,  2.63s/it] 22%|██▏       | 59/270 [01:31<07:27,  2.12s/it] 22%|██▏       | 60/270 [01:32<06:10,  1.77s/it] 23%|██▎       | 61/270 [01:33<05:17,  1.52s/it] 23%|██▎       | 62/270 [01:34<04:38,  1.34s/it] 23%|██▎       | 63/270 [01:35<04:12,  1.22s/it] 24%|██▎       | 64/270 [01:36<03:54,  1.14s/it] 24%|██▍       | 65/270 [01:37<03:40,  1.08s/it] 24%|██▍       | 66/270 [01:38<03:30,  1.03s/it] 25%|██▍       | 67/270 [01:39<03:24,  1.01s/it] 25%|██▌       | 68/270 [01:40<03:19,  1.01it/s] 26%|██▌       | 69/270 [01:41<03:15,  1.03it/s] 26%|██▌       | 70/270 [01:42<03:12,  1.04it/s] 26%|██▋       | 71/270 [01:42<03:10,  1.04it/s] 27%|██▋       | 72/270 [01:43<03:08,  1.05it/s] 27%|██▋       | 73/270 [01:44<03:07,  1.05it/s] 27%|██▋       | 74/270 [01:45<03:05,  1.06it/s] 28%|██▊       | 75/270 [01:46<03:04,  1.06it/s] 28%|██▊       | 76/270 [01:47<03:02,  1.06it/s] 29%|██▊       | 77/270 [01:48<03:01,  1.06it/s] 29%|██▉       | 78/270 [01:49<03:00,  1.06it/s] 29%|██▉       | 79/270 [01:50<03:00,  1.06it/s] 30%|██▉       | 80/270 [01:51<02:57,  1.07it/s] 30%|███       | 81/270 [01:52<02:38,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.6261095404624939, 'eval_f1': 0.8144791860604348, 'eval_runtime': 2.6481, 'eval_samples_per_second': 160.869, 'eval_steps_per_second': 2.643, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.18it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.32it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.73it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.48it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.30it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.55it/s][A                                                
                                             [A 30%|███       | 81/270 [01:54<02:38,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.55it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|███       | 82/270 [02:09<18:11,  5.80s/it] 31%|███       | 83/270 [02:10<13:31,  4.34s/it] 31%|███       | 84/270 [02:11<10:17,  3.32s/it] 31%|███▏      | 85/270 [02:12<08:01,  2.60s/it] 32%|███▏      | 86/270 [02:13<06:26,  2.10s/it] 32%|███▏      | 87/270 [02:14<05:20,  1.75s/it] 33%|███▎      | 88/270 [02:15<04:34,  1.51s/it] 33%|███▎      | 89/270 [02:15<04:01,  1.34s/it] 33%|███▎      | 90/270 [02:16<03:38,  1.22s/it] 34%|███▎      | 91/270 [02:17<03:23,  1.13s/it] 34%|███▍      | 92/270 [02:18<03:11,  1.07s/it] 34%|███▍      | 93/270 [02:19<03:03,  1.04s/it] 35%|███▍      | 94/270 [02:20<02:57,  1.01s/it] 35%|███▌      | 95/270 [02:21<02:51,  1.02it/s] 36%|███▌      | 96/270 [02:22<02:48,  1.03it/s] 36%|███▌      | 97/270 [02:23<02:46,  1.04it/s] 36%|███▋      | 98/270 [02:24<02:44,  1.05it/s] 37%|███▋      | 99/270 [02:25<02:41,  1.06it/s] 37%|███▋      | 100/270 [02:26<02:40,  1.06it/s]                                                  37%|███▋      | 100/270 [02:26<02:40,  1.06it/s] 37%|███▋      | 101/270 [02:27<02:39,  1.06it/s] 38%|███▊      | 102/270 [02:28<02:38,  1.06it/s] 38%|███▊      | 103/270 [02:29<02:36,  1.07it/s] 39%|███▊      | 104/270 [02:29<02:36,  1.06it/s] 39%|███▉      | 105/270 [02:30<02:35,  1.06it/s] 39%|███▉      | 106/270 [02:31<02:34,  1.06it/s] 40%|███▉      | 107/270 [02:32<02:33,  1.06it/s] 40%|████      | 108/270 [02:33<02:16,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.3424864411354065, 'eval_f1': 0.8927882271753835, 'eval_runtime': 2.6283, 'eval_samples_per_second': 162.084, 'eval_steps_per_second': 2.663, 'epoch': 3.0}
{'loss': 0.489, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.16it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.28it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.71it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.44it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.30it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.55it/s][A                                                 
                                             [A 40%|████      | 108/270 [02:36<02:16,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.55it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████      | 109/270 [02:52<16:33,  6.17s/it] 41%|████      | 110/270 [02:52<12:16,  4.60s/it] 41%|████      | 111/270 [02:53<09:16,  3.50s/it] 41%|████▏     | 112/270 [02:54<07:11,  2.73s/it] 42%|████▏     | 113/270 [02:55<05:44,  2.19s/it] 42%|████▏     | 114/270 [02:56<04:43,  1.82s/it] 43%|████▎     | 115/270 [02:57<04:00,  1.55s/it] 43%|████▎     | 116/270 [02:58<03:31,  1.37s/it] 43%|████▎     | 117/270 [02:59<03:09,  1.24s/it] 44%|████▎     | 118/270 [03:00<02:54,  1.15s/it] 44%|████▍     | 119/270 [03:01<02:43,  1.08s/it] 44%|████▍     | 120/270 [03:02<02:36,  1.04s/it] 45%|████▍     | 121/270 [03:03<02:30,  1.01s/it] 45%|████▌     | 122/270 [03:04<02:26,  1.01it/s] 46%|████▌     | 123/270 [03:05<02:23,  1.02it/s] 46%|████▌     | 124/270 [03:06<02:21,  1.03it/s] 46%|████▋     | 125/270 [03:07<02:19,  1.04it/s] 47%|████▋     | 126/270 [03:08<02:17,  1.05it/s] 47%|████▋     | 127/270 [03:08<02:16,  1.05it/s] 47%|████▋     | 128/270 [03:09<02:15,  1.05it/s] 48%|████▊     | 129/270 [03:10<02:12,  1.06it/s] 48%|████▊     | 130/270 [03:11<02:11,  1.06it/s] 49%|████▊     | 131/270 [03:12<02:10,  1.06it/s] 49%|████▉     | 132/270 [03:13<02:17,  1.00it/s] 49%|████▉     | 133/270 [03:14<02:14,  1.02it/s] 50%|████▉     | 134/270 [03:15<02:11,  1.03it/s] 50%|█████     | 135/270 [03:16<01:56,  1.16it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.23007072508335114, 'eval_f1': 0.9458480915667422, 'eval_runtime': 2.6493, 'eval_samples_per_second': 160.8, 'eval_steps_per_second': 2.642, 'epoch': 4.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.19it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.30it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.72it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.28it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                 
                                             [A 50%|█████     | 135/270 [03:19<01:56,  1.16it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 136/270 [03:34<13:22,  5.99s/it] 51%|█████     | 137/270 [03:35<09:55,  4.47s/it] 51%|█████     | 138/270 [03:36<07:30,  3.42s/it] 51%|█████▏    | 139/270 [03:37<05:50,  2.67s/it] 52%|█████▏    | 140/270 [03:38<04:40,  2.15s/it] 52%|█████▏    | 141/270 [03:38<03:51,  1.79s/it] 53%|█████▎    | 142/270 [03:39<03:16,  1.54s/it] 53%|█████▎    | 143/270 [03:40<02:52,  1.36s/it] 53%|█████▎    | 144/270 [03:41<02:35,  1.23s/it] 54%|█████▎    | 145/270 [03:42<02:23,  1.15s/it] 54%|█████▍    | 146/270 [03:43<02:14,  1.09s/it] 54%|█████▍    | 147/270 [03:44<02:08,  1.05s/it] 55%|█████▍    | 148/270 [03:45<02:04,  1.02s/it] 55%|█████▌    | 149/270 [03:46<02:00,  1.00it/s] 56%|█████▌    | 150/270 [03:47<01:58,  1.01it/s]                                                  56%|█████▌    | 150/270 [03:47<01:58,  1.01it/s] 56%|█████▌    | 151/270 [03:48<01:56,  1.02it/s] 56%|█████▋    | 152/270 [03:49<01:54,  1.03it/s] 57%|█████▋    | 153/270 [03:50<01:52,  1.04it/s] 57%|█████▋    | 154/270 [03:51<01:51,  1.04it/s] 57%|█████▋    | 155/270 [03:52<01:49,  1.05it/s] 58%|█████▊    | 156/270 [03:53<01:48,  1.05it/s] 58%|█████▊    | 157/270 [03:54<01:47,  1.05it/s] 59%|█████▊    | 158/270 [03:55<01:46,  1.05it/s] 59%|█████▉    | 159/270 [03:56<01:45,  1.05it/s] 59%|█████▉    | 160/270 [03:57<01:44,  1.06it/s] 60%|█████▉    | 161/270 [03:57<01:43,  1.06it/s] 60%|██████    | 162/270 [03:58<01:31,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.19346372783184052, 'eval_f1': 0.95364619459399, 'eval_runtime': 2.7525, 'eval_samples_per_second': 154.77, 'eval_steps_per_second': 2.543, 'epoch': 5.0}
{'loss': 0.2112, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.15it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.24it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.72it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.46it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.30it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.55it/s][A                                                 
                                             [A 60%|██████    | 162/270 [04:01<01:31,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.55it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████    | 163/270 [04:16<10:25,  5.85s/it] 61%|██████    | 164/270 [04:17<07:43,  4.37s/it] 61%|██████    | 165/270 [04:17<05:50,  3.34s/it] 61%|██████▏   | 166/270 [04:18<04:32,  2.62s/it] 62%|██████▏   | 167/270 [04:19<03:38,  2.12s/it] 62%|██████▏   | 168/270 [04:20<02:59,  1.76s/it] 63%|██████▎   | 169/270 [04:21<02:33,  1.52s/it] 63%|██████▎   | 170/270 [04:22<02:14,  1.34s/it] 63%|██████▎   | 171/270 [04:23<02:01,  1.22s/it] 64%|██████▎   | 172/270 [04:24<01:51,  1.14s/it] 64%|██████▍   | 173/270 [04:25<01:44,  1.08s/it] 64%|██████▍   | 174/270 [04:26<01:39,  1.04s/it] 65%|██████▍   | 175/270 [04:27<01:35,  1.01s/it] 65%|██████▌   | 176/270 [04:28<01:33,  1.01it/s] 66%|██████▌   | 177/270 [04:29<01:30,  1.02it/s] 66%|██████▌   | 178/270 [04:30<01:29,  1.03it/s] 66%|██████▋   | 179/270 [04:31<01:27,  1.04it/s] 67%|██████▋   | 180/270 [04:32<01:26,  1.04it/s] 67%|██████▋   | 181/270 [04:33<01:24,  1.05it/s] 67%|██████▋   | 182/270 [04:33<01:23,  1.05it/s] 68%|██████▊   | 183/270 [04:34<01:22,  1.05it/s] 68%|██████▊   | 184/270 [04:35<01:21,  1.06it/s] 69%|██████▊   | 185/270 [04:36<01:20,  1.06it/s] 69%|██████▉   | 186/270 [04:37<01:19,  1.06it/s] 69%|██████▉   | 187/270 [04:38<01:18,  1.06it/s] 70%|██████▉   | 188/270 [04:39<01:17,  1.06it/s] 70%|███████   | 189/270 [04:40<01:08,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.16298606991767883, 'eval_f1': 0.951402871007558, 'eval_runtime': 2.6321, 'eval_samples_per_second': 161.851, 'eval_steps_per_second': 2.66, 'epoch': 6.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.11it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.28it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.42it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.32it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.57it/s][A                                                 
                                             [A 70%|███████   | 189/270 [04:42<01:08,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.57it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|███████   | 190/270 [04:59<08:23,  6.29s/it] 71%|███████   | 191/270 [05:00<06:10,  4.68s/it] 71%|███████   | 192/270 [05:01<04:37,  3.56s/it] 71%|███████▏  | 193/270 [05:02<03:33,  2.77s/it] 72%|███████▏  | 194/270 [05:03<02:49,  2.23s/it] 72%|███████▏  | 195/270 [05:03<02:18,  1.84s/it] 73%|███████▎  | 196/270 [05:04<01:56,  1.57s/it] 73%|███████▎  | 197/270 [05:05<01:40,  1.38s/it] 73%|███████▎  | 198/270 [05:06<01:29,  1.25s/it] 74%|███████▎  | 199/270 [05:07<01:22,  1.16s/it] 74%|███████▍  | 200/270 [05:08<01:16,  1.09s/it]                                                  74%|███████▍  | 200/270 [05:08<01:16,  1.09s/it] 74%|███████▍  | 201/270 [05:09<01:12,  1.05s/it] 75%|███████▍  | 202/270 [05:10<01:09,  1.02s/it] 75%|███████▌  | 203/270 [05:11<01:06,  1.00it/s] 76%|███████▌  | 204/270 [05:12<01:04,  1.02it/s] 76%|███████▌  | 205/270 [05:13<01:03,  1.03it/s] 76%|███████▋  | 206/270 [05:14<01:01,  1.04it/s] 77%|███████▋  | 207/270 [05:15<01:00,  1.04it/s] 77%|███████▋  | 208/270 [05:16<00:59,  1.05it/s] 77%|███████▋  | 209/270 [05:17<00:58,  1.05it/s] 78%|███████▊  | 210/270 [05:18<00:57,  1.05it/s] 78%|███████▊  | 211/270 [05:19<00:56,  1.05it/s] 79%|███████▊  | 212/270 [05:20<00:54,  1.06it/s] 79%|███████▉  | 213/270 [05:20<00:53,  1.06it/s] 79%|███████▉  | 214/270 [05:21<00:52,  1.06it/s] 80%|███████▉  | 215/270 [05:22<00:51,  1.06it/s] 80%|████████  | 216/270 [05:23<00:45,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.16900508105754852, 'eval_f1': 0.9608277590195821, 'eval_runtime': 2.6414, 'eval_samples_per_second': 161.277, 'eval_steps_per_second': 2.65, 'epoch': 7.0}
{'loss': 0.1472, 'learning_rate': 1.5687918106563326e-06, 'epoch': 7.41}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.18it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.66it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.45it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.28it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                 
                                             [A 80%|████████  | 216/270 [05:26<00:45,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-216
Configuration saved in training-output/checkpoint-216/config.json
Model weights saved in training-output/checkpoint-216/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-216/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-216/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████  | 217/270 [05:40<05:09,  5.84s/it] 81%|████████  | 218/270 [05:41<03:47,  4.37s/it] 81%|████████  | 219/270 [05:42<02:50,  3.34s/it] 81%|████████▏ | 220/270 [05:43<02:11,  2.62s/it] 82%|████████▏ | 221/270 [05:44<01:43,  2.12s/it] 82%|████████▏ | 222/270 [05:45<01:24,  1.77s/it] 83%|████████▎ | 223/270 [05:46<01:11,  1.52s/it] 83%|████████▎ | 224/270 [05:47<01:01,  1.34s/it] 83%|████████▎ | 225/270 [05:48<00:55,  1.22s/it] 84%|████████▎ | 226/270 [05:49<00:50,  1.14s/it] 84%|████████▍ | 227/270 [05:50<00:46,  1.08s/it] 84%|████████▍ | 228/270 [05:51<00:43,  1.04s/it] 85%|████████▍ | 229/270 [05:52<00:41,  1.01s/it] 85%|████████▌ | 230/270 [05:53<00:39,  1.01it/s] 86%|████████▌ | 231/270 [05:54<00:38,  1.02it/s] 86%|████████▌ | 232/270 [05:55<00:36,  1.03it/s] 86%|████████▋ | 233/270 [05:56<00:35,  1.04it/s] 87%|████████▋ | 234/270 [05:56<00:34,  1.05it/s] 87%|████████▋ | 235/270 [05:57<00:33,  1.05it/s] 87%|████████▋ | 236/270 [05:58<00:32,  1.05it/s] 88%|████████▊ | 237/270 [05:59<00:31,  1.05it/s] 88%|████████▊ | 238/270 [06:00<00:30,  1.06it/s] 89%|████████▊ | 239/270 [06:01<00:29,  1.06it/s] 89%|████████▉ | 240/270 [06:02<00:28,  1.06it/s] 89%|████████▉ | 241/270 [06:03<00:27,  1.06it/s] 90%|████████▉ | 242/270 [06:04<00:26,  1.06it/s] 90%|█████████ | 243/270 [06:05<00:22,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.15207545459270477, 'eval_f1': 0.9580286361287603, 'eval_runtime': 2.6433, 'eval_samples_per_second': 161.162, 'eval_steps_per_second': 2.648, 'epoch': 8.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.05it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.26it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.95it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.00it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  2.99it/s][A
100%|██████████| 7/7 [00:02<00:00,  3.29it/s][A                                                 
                                             [A 90%|█████████ | 243/270 [06:07<00:22,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.29it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-243
Configuration saved in training-output/checkpoint-243/config.json
Model weights saved in training-output/checkpoint-243/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-243/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-243/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|█████████ | 244/270 [06:26<03:00,  6.94s/it] 91%|█████████ | 245/270 [06:27<02:08,  5.14s/it] 91%|█████████ | 246/270 [06:28<01:33,  3.88s/it] 91%|█████████▏| 247/270 [06:29<01:08,  3.00s/it] 92%|█████████▏| 248/270 [06:30<00:52,  2.38s/it] 92%|█████████▏| 249/270 [06:30<00:40,  1.95s/it] 93%|█████████▎| 250/270 [06:31<00:32,  1.65s/it]                                                  93%|█████████▎| 250/270 [06:31<00:32,  1.65s/it] 93%|█████████▎| 251/270 [06:32<00:27,  1.44s/it] 93%|█████████▎| 252/270 [06:33<00:23,  1.29s/it] 94%|█████████▎| 253/270 [06:34<00:20,  1.19s/it] 94%|█████████▍| 254/270 [06:35<00:17,  1.12s/it] 94%|█████████▍| 255/270 [06:36<00:15,  1.06s/it] 95%|█████████▍| 256/270 [06:37<00:14,  1.02s/it] 95%|█████████▌| 257/270 [06:38<00:13,  1.00s/it] 96%|█████████▌| 258/270 [06:39<00:11,  1.01it/s] 96%|█████████▌| 259/270 [06:40<00:10,  1.02it/s] 96%|█████████▋| 260/270 [06:41<00:09,  1.03it/s] 97%|█████████▋| 261/270 [06:42<00:08,  1.04it/s] 97%|█████████▋| 262/270 [06:43<00:07,  1.04it/s] 97%|█████████▋| 263/270 [06:44<00:06,  1.05it/s] 98%|█████████▊| 264/270 [06:45<00:05,  1.05it/s] 98%|█████████▊| 265/270 [06:46<00:04,  1.05it/s] 99%|█████████▊| 266/270 [06:47<00:03,  1.05it/s] 99%|█████████▉| 267/270 [06:48<00:02,  1.05it/s] 99%|█████████▉| 268/270 [06:48<00:01,  1.05it/s]100%|█████████▉| 269/270 [06:49<00:00,  1.05it/s]100%|██████████| 270/270 [06:50<00:00,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.15279415249824524, 'eval_f1': 0.9608346289828911, 'eval_runtime': 2.823, 'eval_samples_per_second': 150.905, 'eval_steps_per_second': 2.48, 'epoch': 9.0}
{'loss': 0.1206, 'learning_rate': 1.3477564710088097e-07, 'epoch': 9.26}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.18it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.26it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.44it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.28it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                 
                                             [A100%|██████████| 270/270 [06:53<00:00,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-270
Configuration saved in training-output/checkpoint-270/config.json
Model weights saved in training-output/checkpoint-270/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-270/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-270/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-243 (score: 0.9608346289828911).
                                                 100%|██████████| 270/270 [07:08<00:00,  1.18it/s]100%|██████████| 270/270 [07:08<00:00,  1.59s/it]
{'eval_loss': 0.1527986228466034, 'eval_f1': 0.9608346289828911, 'eval_runtime': 2.6417, 'eval_samples_per_second': 161.257, 'eval_steps_per_second': 2.65, 'epoch': 10.0}
{'train_runtime': 428.5214, 'train_samples_per_second': 39.671, 'train_steps_per_second': 0.63, 'train_loss': 0.4387286027272542, 'epoch': 10.0}
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:06, 17.32it/s]  4%|▎         | 4/107 [00:00<00:05, 18.08it/s]  7%|▋         | 7/107 [00:00<00:04, 20.95it/s]  9%|▉         | 10/107 [00:00<00:04, 23.63it/s] 12%|█▏        | 13/107 [00:00<00:03, 25.18it/s] 15%|█▍        | 16/107 [00:00<00:03, 26.10it/s] 18%|█▊        | 19/107 [00:00<00:03, 26.77it/s] 21%|██        | 22/107 [00:00<00:03, 27.22it/s] 23%|██▎       | 25/107 [00:00<00:02, 27.45it/s] 26%|██▌       | 28/107 [00:01<00:02, 27.67it/s] 29%|██▉       | 31/107 [00:01<00:02, 27.67it/s] 32%|███▏      | 34/107 [00:01<00:02, 27.63it/s] 35%|███▍      | 37/107 [00:01<00:02, 27.66it/s] 37%|███▋      | 40/107 [00:01<00:02, 27.82it/s] 40%|████      | 43/107 [00:01<00:02, 27.93it/s] 43%|████▎     | 46/107 [00:01<00:02, 27.88it/s] 46%|████▌     | 49/107 [00:01<00:02, 27.90it/s] 49%|████▊     | 52/107 [00:01<00:01, 28.06it/s] 51%|█████▏    | 55/107 [00:02<00:01, 28.11it/s] 54%|█████▍    | 58/107 [00:02<00:01, 27.96it/s] 57%|█████▋    | 61/107 [00:02<00:01, 27.83it/s] 60%|█████▉    | 64/107 [00:02<00:01, 27.77it/s] 63%|██████▎   | 67/107 [00:02<00:01, 27.77it/s] 65%|██████▌   | 70/107 [00:02<00:01, 27.78it/s] 68%|██████▊   | 73/107 [00:02<00:01, 27.84it/s] 71%|███████   | 76/107 [00:02<00:01, 27.80it/s] 74%|███████▍  | 79/107 [00:02<00:01, 27.81it/s] 77%|███████▋  | 82/107 [00:03<00:00, 27.92it/s] 79%|███████▉  | 85/107 [00:03<00:00, 27.86it/s] 82%|████████▏ | 88/107 [00:03<00:00, 27.66it/s] 85%|████████▌ | 91/107 [00:03<00:00, 27.74it/s] 88%|████████▊ | 94/107 [00:03<00:00, 27.62it/s] 91%|█████████ | 97/107 [00:03<00:00, 27.70it/s] 93%|█████████▎| 100/107 [00:03<00:00, 27.83it/s] 96%|█████████▋| 103/107 [00:03<00:00, 27.97it/s] 99%|█████████▉| 106/107 [00:03<00:00, 27.85it/s]100%|██████████| 107/107 [00:03<00:00, 27.30it/s]
Model finished with accuracy: 0.960093896713615, macro-f1: 0.9608346289828911
Confusion matrix:
[[ 70   0   0   2   2]
 [  0  84   0   0   0]
 [  0   0  58   1   0]
 [  0   1   3  96   0]
 [  4   1   0   3 101]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14_mx46s0fx52.csv, 58_lvcsk0yy8g.csv, 133_c0kk72b3fm.csv, 132_s5i7ngx7bu.csv, 112_7g35kmtkhr.csv, 101_9l21auewdi.csv, 69_4tlegp20n1.csv, 35_g0ldvn0rjm.csv, 143_w11kkydg7n.csv, 20_1zahygarmq.csv, 53_4t2mju8m3y.csv, 166_5lv4sjg1qv.csv, 105_3acmrslh5t.csv, 198_shm86f7xzz.csv, 155_97dannvd9q.csv, 189_4mh7pjb6hj.csv, 187_kp3lfo9rti.csv, 3_5uxjfwf4c9.csv, 76_f4jqlrtl0d.csv, 152_uhh1hem3ei.csv, 14_apv4u3qebs.csv, 127_q2ifamcjop.csv, 100_5397borrxc.csv, 129_h8c8ny4x7u.csv, 123_g66af1qsqo.csv, 95_6ikda3av09.csv, 42_kr1ailiclk.csv, 162_gzcq0eli5q.csv, 148_nekma0liim.csv, 50_tmt7xio73x.csv, 183_ex67vzlp8e.csv, 64_gjf3ihxyfj.csv, 182_icy94tmqbq.csv, 96_fe55r9bnfd.csv, 37_7swqi7vgnl.csv, 17_qvtzqld86i.csv, 185_9bjzxvbwca.csv, 4_vet6mgh5md.csv, 109_ehl2s3ah2w.csv, 36_4bg4d4bf4q.csv, 165_vcamdtt7pl.csv, 136_dd735psc8g.csv, 24_1y633kulxy.csv, 7_swfaqmk6y6.csv, 46_eg7yytwm57.csv, 12_i0qb55ygnr.csv, 8_v4bqc7mvzy.csv, 120_tfdaxlk9n2.csv, 103_v129kmq3c5.csv, 144_6g4qagf0aw.csv, 72_37k8qe6uf3.csv, 111_4dftcjzv4j.csv, 1_z949c1i541.csv, 178_7rflrddsnn.csv, 6_blz9uaf562.csv, 15_j7tl3bmirm.csv, 83_slgfcmc4xj.csv, 174_vw5a8s9vzj.csv, 9_m11ut0fzvo.csv, 199_hzu91su6jo.csv, 39_3064xz3lgl.csv, 62_6v7alh2xza.csv, 137_eusnnkienm.csv, 118_66k0pevslq.csv, 113_s8v38lnghb.csv, 90_60znzzhvt3.csv, 161_qu7m9mil3a.csv, 32_jitoqenvci.csv, 176_cl4orgir7l.csv, 65_5y9bhpar44.csv, 27_adgb2e3ry7.csv, 33_zybtjc12ef.csv, 140_rpfb54del9.csv, 70_0b98yn7zuy.csv, 158_lhrhfgkazf.csv, 10_ten3l2nuwn.csv, 48_u6mx1k6p64.csv, 29_juqgc9mrcx.csv, 195_nwk7f5mq3h.csv, 119_hdkgwx4tgr.csv, 163_g4ownm07q9.csv, 134_behtf6grhp.csv, 128_y3nqk9k0x4.csv, 93_7s7jo61qgr.csv, 125_onkofvwzgk.csv, 196_h8ebmn0b4i.csv, 186_77g5px55z1.csv, 80_zbkdixjx1u.csv, 154_i7n85wzpa3.csv, 59_tvvx1filo9.csv, 91_6nvvo85k2r.csv, 26_kcs08kmlzv.csv, 146_umxfdzx3sg.csv, 157_7onih683qj.csv, 122_hmocfdllhz.csv, 102_r9vbr9i6qn.csv, 25_zdmhvwm1pk.csv, 16_omnxwedc88.csv, 116_wf72qb1ctj.csv, 87_zzzdc841sd.csv, 168_39vz7oesxk.csv, 24_pa58yc2bsn.csv, 51_km3hwahw1m.csv, 19_sevtqojtws.csv, 21_x8u5ofrlzo.csv, 75_gginvgfabf.csv, 45_4g2awfbbtn.csv, 4_xu9afrhg0o.csv, 25_ceghr7ntgk.csv, 15_jzm9dvtw73.csv, 89_28gvnlaz8j.csv, 57_l8ux51os5l.csv, 153_mwu7z89cr2.csv, 147_qz39am2soc.csv, 67_ip2ys2xhib.csv, 78_tshg4zfas1.csv, 142_d8jaik03id.csv, 5_wa4f0flvdd.csv, 181_wk573tzjde.csv, 193_kjtylxs5x6.csv, 34_egy98kczu6.csv, 85_i7f88e7b7g.csv, 106_oa27xk4eqt.csv, 156_yk739g3286.csv, 11_0na6xsa4cd.csv, 31_jhmyyhmjh5.csv, 149_tvntt1n3zf.csv, 6_qyv4vhzoe.csv, 63_j79b3nmr83.csv, 104_jos9y36cxr.csv, 151_8k3mbw3phz.csv, 60_zry6xyk2v2.csv, 86_kvt6l8gw8d.csv, 131_bivn5or23h.csv, 175_954mvwcwet.csv, 110_zq7vq985bq.csv, 52_wfyl6gdoxn.csv, 92_u8wrf1zhh5.csv, 71_jjxizibivj.csv, 169_2y8d92hhek.csv, 114_fkhid5yyst.csv, 173_op7k51t6eh.csv, 145_1p84lqxs5t.csv, 171_evt2lbu5wz.csv, 2_wrrttduqx0.csv, 107_jfii3uf345.csv, 164_qcoq7yni4y.csv, 160_9xq61i8z5q.csv, 150_hvfxdsp6uq.csv, 16_j1uppkkwpc.csv, 26_2k5thw2ykv.csv, 172_gg34wiyfxn.csv, 27_r6anjd4ie1.csv, 11_7vk8oa1o24.csv, 138_yx53ai19ar.csv, 44_zsqcy1dp0n.csv, 68_4fxf0cl1sa.csv, 77_gup8yvrjg8.csv, 23_lce4ue1yit.csv, 197_ni25hikvxf.csv, 55_o97cc78tyd.csv, 13_xb7ph2k0m6.csv, 126_s6vu9wsf2o.csv, 124_4p9vzltfyn.csv, 135_7g73f18y8h.csv, 194_tnwwh3zbnj.csv, 99_7fcky2iv54.csv, 49_8fcz4c6oqy.csv, 84_q4rh6yklie.csv, 28_7svph4yobz.csv, 38_38doqz3unu.csv, 5_lcampqa5d8.csv, 82_3qr5zxdcxe.csv, 180_jm6cn0n08c.csv, 66_8gquz8hbpz.csv, 40_zh8fjx6tiy.csv, 141_tvldgnl6pt.csv, 30_rb578trmsj.csv, 184_hzlbex83q3.csv, 56_lxflttkcgn.csv, 22_udsmb087i5.csv, 8_hfxm8dkunn.csv, 61_l3jkp84irv.csv, 139_1qp7vpj70o.csv, 73_4s2wt6mgps.csv, 7_viucp080gt.csv, 108_qkxdz8djgt.csv, 54_07djfakfxh.csv, 98_8et5hi6nqf.csv, 74_08dh1ae49m.csv, 21_x90g7ie9t7.csv, 81_dbvr7ry56o.csv, 121_aobrexg5sv.csv, 191_yxm7fc16uc.csv, 94_gcso78g1mh.csv, 167_bk26q01ex6.csv, 41_rrwls6y9fl.csv, 170_szpuak9x3b.csv, 47_qrdef2nnjn.csv, 13_y1i6zh8y9w.csv, 188_5sjzeb4w03.csv, 130_7a6m1my07s.csv, 192_isa0drpmu4.csv, 79_wkvkhcnsn0.csv, 2_9actgs9mi7.csv, 18_o65lxoj85y.csv, 3_j0hn3a0xzt.csv, 43_1nemhlm74b.csv, 20_z3zjd5sgm4.csv, 117_mgq0ymf5hc.csv, 0_imfn3fsnqb.csv, 88_p169fqr8hl.csv, 1_ofoaso8yxd.csv, 97_ksnb8zg7e2.csv, 159_wmf9y5lb3d.csv, 179_enmmbxkd9b.csv, 18_omkydub7qy.csv, 115_iocn2dpj2o.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|██████████| 2/2 [00:00<00:00, 18.77ba/s]100%|██████████| 2/2 [00:00<00:00, 18.71ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 45.42ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:04<19:58,  4.46s/it]  1%|          | 2/270 [00:05<10:36,  2.38s/it]  1%|          | 3/270 [00:06<07:35,  1.71s/it]  1%|▏         | 4/270 [00:07<06:11,  1.39s/it]  2%|▏         | 5/270 [00:08<05:23,  1.22s/it]  2%|▏         | 6/270 [00:09<04:55,  1.12s/it]  3%|▎         | 7/270 [00:09<04:37,  1.05s/it]  3%|▎         | 8/270 [00:10<04:24,  1.01s/it]  3%|▎         | 9/270 [00:11<04:15,  1.02it/s]  4%|▎         | 10/270 [00:12<04:10,  1.04it/s]  4%|▍         | 11/270 [00:13<04:05,  1.05it/s]  4%|▍         | 12/270 [00:14<04:02,  1.06it/s]  5%|▍         | 13/270 [00:15<04:00,  1.07it/s]  5%|▌         | 14/270 [00:16<03:59,  1.07it/s]  6%|▌         | 15/270 [00:17<03:57,  1.07it/s]  6%|▌         | 16/270 [00:18<03:55,  1.08it/s]  6%|▋         | 17/270 [00:19<03:54,  1.08it/s]  7%|▋         | 18/270 [00:20<03:53,  1.08it/s]  7%|▋         | 19/270 [00:21<03:52,  1.08it/s]  7%|▋         | 20/270 [00:21<03:50,  1.09it/s]  8%|▊         | 21/270 [00:22<03:49,  1.08it/s]  8%|▊         | 22/270 [00:23<03:48,  1.08it/s]  9%|▊         | 23/270 [00:24<03:48,  1.08it/s]  9%|▉         | 24/270 [00:25<03:48,  1.08it/s]  9%|▉         | 25/270 [00:26<03:48,  1.07it/s] 10%|▉         | 26/270 [00:27<03:46,  1.08it/s] 10%|█         | 27/270 [00:28<03:21,  1.20it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.16it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.28it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.50it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.37it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.62it/s][A                                                
                                             [A 10%|█         | 27/270 [00:30<03:21,  1.20it/s]
100%|██████████| 7/7 [00:02<00:00,  3.62it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|█         | 28/270 [00:46<24:47,  6.15s/it] 11%|█         | 29/270 [00:47<18:37,  4.64s/it] 11%|█         | 30/270 [00:48<14:05,  3.52s/it] 11%|█▏        | 31/270 [00:49<10:55,  2.74s/it] 12%|█▏        | 32/270 [00:50<08:43,  2.20s/it] 12%|█▏        | 33/270 [00:51<07:11,  1.82s/it] 13%|█▎        | 34/270 [00:52<06:06,  1.55s/it] 13%|█▎        | 35/270 [00:53<05:20,  1.36s/it] 13%|█▎        | 36/270 [00:54<04:48,  1.23s/it] 14%|█▎        | 37/270 [00:55<04:26,  1.14s/it] 14%|█▍        | 38/270 [00:56<04:10,  1.08s/it] 14%|█▍        | 39/270 [00:57<03:58,  1.03s/it] 15%|█▍        | 40/270 [00:58<03:50,  1.00s/it] 15%|█▌        | 41/270 [00:58<03:44,  1.02it/s] 16%|█▌        | 42/270 [00:59<03:40,  1.04it/s] 16%|█▌        | 43/270 [01:00<03:37,  1.05it/s] 16%|█▋        | 44/270 [01:01<03:35,  1.05it/s] 17%|█▋        | 45/270 [01:02<03:33,  1.05it/s] 17%|█▋        | 46/270 [01:03<03:32,  1.06it/s] 17%|█▋        | 47/270 [01:04<03:30,  1.06it/s] 18%|█▊        | 48/270 [01:05<03:28,  1.06it/s] 18%|█▊        | 49/270 [01:06<03:27,  1.07it/s] 19%|█▊        | 50/270 [01:07<03:26,  1.07it/s]                                                 19%|█▊        | 50/270 [01:07<03:26,  1.07it/s] 19%|█▉        | 51/270 [01:08<03:25,  1.07it/s] 19%|█▉        | 52/270 [01:09<03:24,  1.07it/s] 20%|█▉        | 53/270 [01:10<03:23,  1.07it/s] 20%|██        | 54/270 [01:10<03:01,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 1.3250912427902222, 'eval_f1': 0.3146547466913033, 'eval_runtime': 2.609, 'eval_samples_per_second': 163.278, 'eval_steps_per_second': 2.683, 'epoch': 1.0}
{'loss': 1.3495, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.05it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.26it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.70it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.45it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.32it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                
                                             [A 20%|██        | 54/270 [01:14<03:01,  1.19it/s]
100%|██████████| 7/7 [00:03<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██        | 55/270 [01:29<22:18,  6.23s/it] 21%|██        | 56/270 [01:30<16:32,  4.64s/it] 21%|██        | 57/270 [01:31<12:30,  3.52s/it] 21%|██▏       | 58/270 [01:32<09:42,  2.75s/it] 22%|██▏       | 59/270 [01:33<07:43,  2.20s/it] 22%|██▏       | 60/270 [01:34<06:21,  1.82s/it] 23%|██▎       | 61/270 [01:35<05:23,  1.55s/it] 23%|██▎       | 62/270 [01:36<04:44,  1.37s/it] 23%|██▎       | 63/270 [01:37<04:15,  1.24s/it] 24%|██▎       | 64/270 [01:37<03:55,  1.14s/it] 24%|██▍       | 65/270 [01:38<03:41,  1.08s/it] 24%|██▍       | 66/270 [01:39<03:31,  1.04s/it] 25%|██▍       | 67/270 [01:40<03:24,  1.01s/it] 25%|██▌       | 68/270 [01:41<03:18,  1.02it/s] 26%|██▌       | 69/270 [01:42<03:15,  1.03it/s] 26%|██▌       | 70/270 [01:43<03:12,  1.04it/s] 26%|██▋       | 71/270 [01:44<03:09,  1.05it/s] 27%|██▋       | 72/270 [01:45<03:08,  1.05it/s] 27%|██▋       | 73/270 [01:46<03:06,  1.06it/s] 27%|██▋       | 74/270 [01:47<03:05,  1.06it/s] 28%|██▊       | 75/270 [01:48<03:03,  1.06it/s] 28%|██▊       | 76/270 [01:49<03:02,  1.06it/s] 29%|██▊       | 77/270 [01:50<03:01,  1.06it/s] 29%|██▉       | 78/270 [01:51<02:59,  1.07it/s] 29%|██▉       | 79/270 [01:52<02:59,  1.06it/s] 30%|██▉       | 80/270 [01:52<02:59,  1.06it/s] 30%|███       | 81/270 [01:53<02:39,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.624811053276062, 'eval_f1': 0.7977017102599752, 'eval_runtime': 3.699, 'eval_samples_per_second': 115.166, 'eval_steps_per_second': 1.892, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.06it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.65it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                
                                             [A 30%|███       | 81/270 [01:56<02:39,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|███       | 82/270 [02:11<18:25,  5.88s/it] 31%|███       | 83/270 [02:12<13:42,  4.40s/it] 31%|███       | 84/270 [02:13<10:24,  3.36s/it] 31%|███▏      | 85/270 [02:14<08:06,  2.63s/it] 32%|███▏      | 86/270 [02:14<06:30,  2.12s/it] 32%|███▏      | 87/270 [02:15<05:23,  1.77s/it] 33%|███▎      | 88/270 [02:16<04:37,  1.52s/it] 33%|███▎      | 89/270 [02:17<04:04,  1.35s/it] 33%|███▎      | 90/270 [02:18<03:40,  1.23s/it] 34%|███▎      | 91/270 [02:19<03:24,  1.14s/it] 34%|███▍      | 92/270 [02:20<03:12,  1.08s/it] 34%|███▍      | 93/270 [02:21<03:03,  1.03s/it] 35%|███▍      | 94/270 [02:22<02:57,  1.01s/it] 35%|███▌      | 95/270 [02:23<02:53,  1.01it/s] 36%|███▌      | 96/270 [02:24<02:50,  1.02it/s] 36%|███▌      | 97/270 [02:25<02:47,  1.04it/s] 36%|███▋      | 98/270 [02:26<02:44,  1.04it/s] 37%|███▋      | 99/270 [02:27<02:43,  1.05it/s] 37%|███▋      | 100/270 [02:28<02:41,  1.05it/s]                                                  37%|███▋      | 100/270 [02:28<02:41,  1.05it/s] 37%|███▋      | 101/270 [02:29<02:40,  1.05it/s] 38%|███▊      | 102/270 [02:30<02:39,  1.05it/s] 38%|███▊      | 103/270 [02:30<02:38,  1.05it/s] 39%|███▊      | 104/270 [02:31<02:37,  1.05it/s] 39%|███▉      | 105/270 [02:32<02:35,  1.06it/s] 39%|███▉      | 106/270 [02:33<02:35,  1.06it/s] 40%|███▉      | 107/270 [02:34<02:34,  1.06it/s] 40%|████      | 108/270 [02:35<02:16,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.334524005651474, 'eval_f1': 0.8891751536929384, 'eval_runtime': 2.8223, 'eval_samples_per_second': 150.939, 'eval_steps_per_second': 2.48, 'epoch': 3.0}
{'loss': 0.5133, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.02it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.24it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                 
                                             [A 40%|████      | 108/270 [02:38<02:16,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████      | 109/270 [02:54<16:44,  6.24s/it] 41%|████      | 110/270 [02:55<12:23,  4.65s/it] 41%|████      | 111/270 [02:56<09:21,  3.53s/it] 41%|████▏     | 112/270 [02:57<07:15,  2.76s/it] 42%|████▏     | 113/270 [02:57<05:46,  2.21s/it] 42%|████▏     | 114/270 [02:58<04:45,  1.83s/it] 43%|████▎     | 115/270 [02:59<04:01,  1.56s/it] 43%|████▎     | 116/270 [03:00<03:31,  1.37s/it] 43%|████▎     | 117/270 [03:01<03:10,  1.24s/it] 44%|████▎     | 118/270 [03:02<02:55,  1.15s/it] 44%|████▍     | 119/270 [03:03<02:44,  1.09s/it] 44%|████▍     | 120/270 [03:04<02:36,  1.04s/it] 45%|████▍     | 121/270 [03:05<02:31,  1.01s/it] 45%|████▌     | 122/270 [03:06<02:26,  1.01it/s] 46%|████▌     | 123/270 [03:07<02:23,  1.02it/s] 46%|████▌     | 124/270 [03:08<02:20,  1.04it/s] 46%|████▋     | 125/270 [03:09<02:18,  1.04it/s] 47%|████▋     | 126/270 [03:10<02:17,  1.05it/s] 47%|████▋     | 127/270 [03:11<02:15,  1.05it/s] 47%|████▋     | 128/270 [03:12<02:14,  1.05it/s] 48%|████▊     | 129/270 [03:12<02:13,  1.06it/s] 48%|████▊     | 130/270 [03:13<02:12,  1.05it/s] 49%|████▊     | 131/270 [03:14<02:12,  1.05it/s] 49%|████▉     | 132/270 [03:16<02:19,  1.01s/it] 49%|████▉     | 133/270 [03:16<02:15,  1.01it/s] 50%|████▉     | 134/270 [03:17<02:13,  1.02it/s] 50%|█████     | 135/270 [03:18<01:57,  1.15it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.20800550282001495, 'eval_f1': 0.9406944160262853, 'eval_runtime': 2.649, 'eval_samples_per_second': 160.813, 'eval_steps_per_second': 2.642, 'epoch': 4.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.09it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.31it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.44it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.28it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.52it/s][A                                                 
                                             [A 50%|█████     | 135/270 [03:21<01:57,  1.15it/s]
100%|██████████| 7/7 [00:02<00:00,  3.52it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 136/270 [03:35<13:00,  5.82s/it] 51%|█████     | 137/270 [03:36<09:39,  4.36s/it] 51%|█████     | 138/270 [03:37<07:19,  3.33s/it] 51%|█████▏    | 139/270 [03:38<05:42,  2.61s/it] 52%|█████▏    | 140/270 [03:39<04:34,  2.11s/it] 52%|█████▏    | 141/270 [03:40<03:46,  1.76s/it] 53%|█████▎    | 142/270 [03:41<03:13,  1.51s/it] 53%|█████▎    | 143/270 [03:42<02:50,  1.34s/it] 53%|█████▎    | 144/270 [03:43<02:33,  1.22s/it] 54%|█████▎    | 145/270 [03:44<02:21,  1.13s/it] 54%|█████▍    | 146/270 [03:45<02:13,  1.08s/it] 54%|█████▍    | 147/270 [03:46<02:08,  1.04s/it] 55%|█████▍    | 148/270 [03:47<02:03,  1.01s/it] 55%|█████▌    | 149/270 [03:48<01:59,  1.01it/s] 56%|█████▌    | 150/270 [03:49<01:57,  1.02it/s]                                                  56%|█████▌    | 150/270 [03:49<01:57,  1.02it/s] 56%|█████▌    | 151/270 [03:50<01:55,  1.03it/s] 56%|█████▋    | 152/270 [03:50<01:53,  1.04it/s] 57%|█████▋    | 153/270 [03:51<01:51,  1.05it/s] 57%|█████▋    | 154/270 [03:52<01:50,  1.05it/s] 57%|█████▋    | 155/270 [03:53<01:49,  1.05it/s] 58%|█████▊    | 156/270 [03:54<01:48,  1.05it/s] 58%|█████▊    | 157/270 [03:55<01:47,  1.05it/s] 59%|█████▊    | 158/270 [03:56<01:46,  1.06it/s] 59%|█████▉    | 159/270 [03:57<01:45,  1.06it/s] 59%|█████▉    | 160/270 [03:58<01:44,  1.05it/s] 60%|█████▉    | 161/270 [03:59<01:43,  1.06it/s] 60%|██████    | 162/270 [04:00<01:31,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.15978442132472992, 'eval_f1': 0.9481013875725347, 'eval_runtime': 2.6542, 'eval_samples_per_second': 160.498, 'eval_steps_per_second': 2.637, 'epoch': 5.0}
{'loss': 0.2326, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.18it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.28it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.71it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.28it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                 
                                             [A 60%|██████    | 162/270 [04:02<01:31,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████    | 163/270 [04:17<10:15,  5.76s/it] 61%|██████    | 164/270 [04:18<07:36,  4.31s/it] 61%|██████    | 165/270 [04:19<05:46,  3.30s/it] 61%|██████▏   | 166/270 [04:20<04:29,  2.59s/it] 62%|██████▏   | 167/270 [04:21<03:35,  2.09s/it] 62%|██████▏   | 168/270 [04:22<02:57,  1.74s/it] 63%|██████▎   | 169/270 [04:22<02:31,  1.50s/it] 63%|██████▎   | 170/270 [04:23<02:13,  1.33s/it] 63%|██████▎   | 171/270 [04:24<02:00,  1.22s/it] 64%|██████▎   | 172/270 [04:25<01:51,  1.14s/it] 64%|██████▍   | 173/270 [04:26<01:44,  1.08s/it] 64%|██████▍   | 174/270 [04:27<01:39,  1.04s/it] 65%|██████▍   | 175/270 [04:28<01:35,  1.01s/it] 65%|██████▌   | 176/270 [04:29<01:32,  1.01it/s] 66%|██████▌   | 177/270 [04:30<01:30,  1.02it/s] 66%|██████▌   | 178/270 [04:31<01:28,  1.04it/s] 66%|██████▋   | 179/270 [04:32<01:27,  1.04it/s] 67%|██████▋   | 180/270 [04:33<01:25,  1.05it/s] 67%|██████▋   | 181/270 [04:34<01:24,  1.05it/s] 67%|██████▋   | 182/270 [04:35<01:23,  1.05it/s] 68%|██████▊   | 183/270 [04:36<01:22,  1.05it/s] 68%|██████▊   | 184/270 [04:37<01:21,  1.05it/s] 69%|██████▊   | 185/270 [04:38<01:20,  1.05it/s] 69%|██████▉   | 186/270 [04:39<01:19,  1.06it/s] 69%|██████▉   | 187/270 [04:39<01:18,  1.06it/s] 70%|██████▉   | 188/270 [04:40<01:17,  1.06it/s] 70%|███████   | 189/270 [04:41<01:08,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.14890289306640625, 'eval_f1': 0.9529441662238591, 'eval_runtime': 2.6541, 'eval_samples_per_second': 160.504, 'eval_steps_per_second': 2.637, 'epoch': 6.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.18it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.31it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.30it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.57it/s][A                                                 
                                             [A 70%|███████   | 189/270 [04:44<01:08,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.57it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|███████   | 190/270 [04:59<07:47,  5.85s/it] 71%|███████   | 191/270 [04:59<05:45,  4.37s/it] 71%|███████   | 192/270 [05:00<04:20,  3.34s/it] 71%|███████▏  | 193/270 [05:01<03:22,  2.63s/it] 72%|███████▏  | 194/270 [05:02<02:41,  2.12s/it] 72%|███████▏  | 195/270 [05:03<02:12,  1.77s/it] 73%|███████▎  | 196/270 [05:04<01:52,  1.52s/it] 73%|███████▎  | 197/270 [05:05<01:38,  1.35s/it] 73%|███████▎  | 198/270 [05:06<01:28,  1.23s/it] 74%|███████▎  | 199/270 [05:07<01:21,  1.14s/it] 74%|███████▍  | 200/270 [05:08<01:15,  1.08s/it]                                                  74%|███████▍  | 200/270 [05:08<01:15,  1.08s/it] 74%|███████▍  | 201/270 [05:09<01:12,  1.04s/it] 75%|███████▍  | 202/270 [05:10<01:09,  1.02s/it] 75%|███████▌  | 203/270 [05:11<01:06,  1.00it/s] 76%|███████▌  | 204/270 [05:12<01:05,  1.01it/s] 76%|███████▌  | 205/270 [05:13<01:03,  1.02it/s] 76%|███████▋  | 206/270 [05:14<01:01,  1.03it/s] 77%|███████▋  | 207/270 [05:15<01:00,  1.04it/s] 77%|███████▋  | 208/270 [05:16<00:59,  1.05it/s] 77%|███████▋  | 209/270 [05:17<00:58,  1.05it/s] 78%|███████▊  | 210/270 [05:17<00:56,  1.06it/s] 78%|███████▊  | 211/270 [05:18<00:55,  1.06it/s] 79%|███████▊  | 212/270 [05:19<00:55,  1.05it/s] 79%|███████▉  | 213/270 [05:20<00:54,  1.05it/s] 79%|███████▉  | 214/270 [05:21<00:53,  1.05it/s] 80%|███████▉  | 215/270 [05:22<00:52,  1.05it/s] 80%|████████  | 216/270 [05:23<00:45,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.146599680185318, 'eval_f1': 0.9542284396979565, 'eval_runtime': 2.6352, 'eval_samples_per_second': 161.657, 'eval_steps_per_second': 2.656, 'epoch': 7.0}
{'loss': 0.1501, 'learning_rate': 1.5687918106563326e-06, 'epoch': 7.41}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.95it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.52it/s][A                                                 
                                             [A 80%|████████  | 216/270 [05:25<00:45,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.52it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-216
Configuration saved in training-output/checkpoint-216/config.json
Model weights saved in training-output/checkpoint-216/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-216/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-216/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████  | 217/270 [05:41<05:13,  5.92s/it] 81%|████████  | 218/270 [05:41<03:49,  4.42s/it] 81%|████████  | 219/270 [05:42<02:52,  3.38s/it] 81%|████████▏ | 220/270 [05:43<02:12,  2.65s/it] 82%|████████▏ | 221/270 [05:44<01:44,  2.14s/it] 82%|████████▏ | 222/270 [05:45<01:25,  1.78s/it] 83%|████████▎ | 223/270 [05:46<01:11,  1.53s/it] 83%|████████▎ | 224/270 [05:47<01:02,  1.35s/it] 83%|████████▎ | 225/270 [05:48<00:55,  1.23s/it] 84%|████████▎ | 226/270 [05:49<00:50,  1.14s/it] 84%|████████▍ | 227/270 [05:50<00:46,  1.08s/it] 84%|████████▍ | 228/270 [05:51<00:43,  1.04s/it] 85%|████████▍ | 229/270 [05:52<00:41,  1.01s/it] 85%|████████▌ | 230/270 [05:53<00:39,  1.01it/s] 86%|████████▌ | 231/270 [05:54<00:38,  1.02it/s] 86%|████████▌ | 232/270 [05:55<00:36,  1.03it/s] 86%|████████▋ | 233/270 [05:56<00:35,  1.04it/s] 87%|████████▋ | 234/270 [05:57<00:34,  1.04it/s] 87%|████████▋ | 235/270 [05:58<00:33,  1.05it/s] 87%|████████▋ | 236/270 [05:58<00:32,  1.05it/s] 88%|████████▊ | 237/270 [05:59<00:31,  1.05it/s] 88%|████████▊ | 238/270 [06:00<00:30,  1.05it/s] 89%|████████▊ | 239/270 [06:01<00:29,  1.05it/s] 89%|████████▉ | 240/270 [06:02<00:28,  1.05it/s] 89%|████████▉ | 241/270 [06:03<00:27,  1.05it/s] 90%|████████▉ | 242/270 [06:04<00:26,  1.06it/s] 90%|█████████ | 243/270 [06:05<00:22,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.149600550532341, 'eval_f1': 0.9552565643048029, 'eval_runtime': 2.6441, 'eval_samples_per_second': 161.114, 'eval_steps_per_second': 2.647, 'epoch': 8.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.11it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.27it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.68it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.42it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  2.75it/s][A
100%|██████████| 7/7 [00:02<00:00,  3.10it/s][A                                                 
                                             [A 90%|█████████ | 243/270 [06:08<00:22,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.10it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-243
Configuration saved in training-output/checkpoint-243/config.json
Model weights saved in training-output/checkpoint-243/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-243/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-243/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|█████████ | 244/270 [06:23<02:34,  5.95s/it] 91%|█████████ | 245/270 [06:24<01:51,  4.45s/it] 91%|█████████ | 246/270 [06:25<01:21,  3.40s/it] 91%|█████████▏| 247/270 [06:25<01:01,  2.66s/it] 92%|█████████▏| 248/270 [06:26<00:47,  2.14s/it] 92%|█████████▏| 249/270 [06:27<00:37,  1.78s/it] 93%|█████████▎| 250/270 [06:28<00:30,  1.53s/it]                                                  93%|█████████▎| 250/270 [06:28<00:30,  1.53s/it] 93%|█████████▎| 251/270 [06:29<00:25,  1.35s/it] 93%|█████████▎| 252/270 [06:30<00:22,  1.23s/it] 94%|█████████▎| 253/270 [06:31<00:19,  1.14s/it] 94%|█████████▍| 254/270 [06:32<00:17,  1.09s/it] 94%|█████████▍| 255/270 [06:33<00:15,  1.04s/it] 95%|█████████▍| 256/270 [06:34<00:14,  1.02s/it] 95%|█████████▌| 257/270 [06:35<00:12,  1.01it/s] 96%|█████████▌| 258/270 [06:36<00:11,  1.02it/s] 96%|█████████▌| 259/270 [06:37<00:10,  1.03it/s] 96%|█████████▋| 260/270 [06:38<00:09,  1.04it/s] 97%|█████████▋| 261/270 [06:39<00:08,  1.04it/s] 97%|█████████▋| 262/270 [06:40<00:07,  1.05it/s] 97%|█████████▋| 263/270 [06:41<00:06,  1.05it/s] 98%|█████████▊| 264/270 [06:42<00:05,  1.05it/s] 98%|█████████▊| 265/270 [06:42<00:04,  1.05it/s] 99%|█████████▊| 266/270 [06:43<00:03,  1.05it/s] 99%|█████████▉| 267/270 [06:44<00:02,  1.05it/s] 99%|█████████▉| 268/270 [06:45<00:01,  1.05it/s]100%|█████████▉| 269/270 [06:46<00:00,  1.05it/s]100%|██████████| 270/270 [06:47<00:00,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.14788906276226044, 'eval_f1': 0.9552565643048029, 'eval_runtime': 2.8164, 'eval_samples_per_second': 151.257, 'eval_steps_per_second': 2.485, 'epoch': 9.0}
{'loss': 0.1291, 'learning_rate': 1.3477564710088097e-07, 'epoch': 9.26}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.96it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.64it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.21it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.47it/s][A                                                 
                                             [A100%|██████████| 270/270 [06:50<00:00,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.47it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-270
Configuration saved in training-output/checkpoint-270/config.json
Model weights saved in training-output/checkpoint-270/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-270/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-270/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-216 (score: 0.9552565643048029).
                                                 100%|██████████| 270/270 [07:04<00:00,  1.18it/s]100%|██████████| 270/270 [07:04<00:00,  1.57s/it]
{'eval_loss': 0.14658674597740173, 'eval_f1': 0.9521179331635835, 'eval_runtime': 2.6894, 'eval_samples_per_second': 158.399, 'eval_steps_per_second': 2.603, 'epoch': 10.0}
{'train_runtime': 424.6131, 'train_samples_per_second': 40.036, 'train_steps_per_second': 0.636, 'train_loss': 0.4490501880645752, 'epoch': 10.0}
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:06, 17.14it/s]  4%|▎         | 4/107 [00:00<00:05, 17.27it/s]  7%|▋         | 7/107 [00:00<00:04, 21.90it/s]  9%|▉         | 10/107 [00:00<00:03, 24.34it/s] 12%|█▏        | 13/107 [00:00<00:03, 25.67it/s] 15%|█▍        | 16/107 [00:00<00:03, 26.53it/s] 18%|█▊        | 19/107 [00:00<00:03, 27.02it/s] 21%|██        | 22/107 [00:00<00:03, 27.40it/s] 23%|██▎       | 25/107 [00:00<00:02, 27.67it/s] 26%|██▌       | 28/107 [00:01<00:02, 27.77it/s] 29%|██▉       | 31/107 [00:01<00:02, 27.44it/s] 32%|███▏      | 34/107 [00:01<00:02, 27.44it/s] 35%|███▍      | 37/107 [00:01<00:02, 27.52it/s] 37%|███▋      | 40/107 [00:01<00:02, 27.75it/s] 40%|████      | 43/107 [00:01<00:02, 27.83it/s] 43%|████▎     | 46/107 [00:01<00:02, 27.85it/s] 46%|████▌     | 49/107 [00:01<00:02, 27.89it/s] 49%|████▊     | 52/107 [00:01<00:01, 27.86it/s] 51%|█████▏    | 55/107 [00:02<00:01, 27.88it/s] 54%|█████▍    | 58/107 [00:02<00:01, 27.68it/s] 57%|█████▋    | 61/107 [00:02<00:01, 27.71it/s] 60%|█████▉    | 64/107 [00:02<00:01, 27.75it/s] 63%|██████▎   | 67/107 [00:02<00:01, 27.83it/s] 65%|██████▌   | 70/107 [00:02<00:01, 27.91it/s] 68%|██████▊   | 73/107 [00:02<00:01, 27.86it/s] 71%|███████   | 76/107 [00:02<00:01, 27.80it/s] 74%|███████▍  | 79/107 [00:02<00:01, 27.93it/s] 77%|███████▋  | 82/107 [00:03<00:00, 28.05it/s] 79%|███████▉  | 85/107 [00:03<00:00, 27.95it/s] 82%|████████▏ | 88/107 [00:03<00:00, 27.88it/s] 85%|████████▌ | 91/107 [00:03<00:00, 27.86it/s] 88%|████████▊ | 94/107 [00:03<00:00, 27.81it/s] 91%|█████████ | 97/107 [00:03<00:00, 27.72it/s] 93%|█████████▎| 100/107 [00:03<00:00, 27.82it/s] 96%|█████████▋| 103/107 [00:03<00:00, 27.78it/s] 99%|█████████▉| 106/107 [00:03<00:00, 27.82it/s]100%|██████████| 107/107 [00:03<00:00, 27.34it/s]
Model finished with accuracy: 0.9577464788732394, macro-f1: 0.9552565643048029
Confusion matrix:
[[ 76   0   0   0   4]
 [  1  77   1   2   1]
 [  0   0  52   3   0]
 [  0   0   5 112   0]
 [  1   0   0   0  91]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14_mx46s0fx52.csv, 58_lvcsk0yy8g.csv, 133_c0kk72b3fm.csv, 132_s5i7ngx7bu.csv, 112_7g35kmtkhr.csv, 101_9l21auewdi.csv, 69_4tlegp20n1.csv, 35_g0ldvn0rjm.csv, 143_w11kkydg7n.csv, 20_1zahygarmq.csv, 53_4t2mju8m3y.csv, 166_5lv4sjg1qv.csv, 105_3acmrslh5t.csv, 198_shm86f7xzz.csv, 155_97dannvd9q.csv, 189_4mh7pjb6hj.csv, 187_kp3lfo9rti.csv, 3_5uxjfwf4c9.csv, 76_f4jqlrtl0d.csv, 152_uhh1hem3ei.csv, 14_apv4u3qebs.csv, 127_q2ifamcjop.csv, 100_5397borrxc.csv, 129_h8c8ny4x7u.csv, 123_g66af1qsqo.csv, 95_6ikda3av09.csv, 42_kr1ailiclk.csv, 162_gzcq0eli5q.csv, 148_nekma0liim.csv, 50_tmt7xio73x.csv, 183_ex67vzlp8e.csv, 64_gjf3ihxyfj.csv, 182_icy94tmqbq.csv, 96_fe55r9bnfd.csv, 37_7swqi7vgnl.csv, 17_qvtzqld86i.csv, 185_9bjzxvbwca.csv, 4_vet6mgh5md.csv, 109_ehl2s3ah2w.csv, 36_4bg4d4bf4q.csv, 165_vcamdtt7pl.csv, 136_dd735psc8g.csv, 24_1y633kulxy.csv, 7_swfaqmk6y6.csv, 46_eg7yytwm57.csv, 12_i0qb55ygnr.csv, 8_v4bqc7mvzy.csv, 120_tfdaxlk9n2.csv, 103_v129kmq3c5.csv, 144_6g4qagf0aw.csv, 72_37k8qe6uf3.csv, 111_4dftcjzv4j.csv, 1_z949c1i541.csv, 178_7rflrddsnn.csv, 6_blz9uaf562.csv, 15_j7tl3bmirm.csv, 83_slgfcmc4xj.csv, 174_vw5a8s9vzj.csv, 9_m11ut0fzvo.csv, 199_hzu91su6jo.csv, 39_3064xz3lgl.csv, 62_6v7alh2xza.csv, 137_eusnnkienm.csv, 118_66k0pevslq.csv, 113_s8v38lnghb.csv, 90_60znzzhvt3.csv, 161_qu7m9mil3a.csv, 32_jitoqenvci.csv, 176_cl4orgir7l.csv, 65_5y9bhpar44.csv, 27_adgb2e3ry7.csv, 33_zybtjc12ef.csv, 140_rpfb54del9.csv, 70_0b98yn7zuy.csv, 158_lhrhfgkazf.csv, 10_ten3l2nuwn.csv, 48_u6mx1k6p64.csv, 29_juqgc9mrcx.csv, 195_nwk7f5mq3h.csv, 119_hdkgwx4tgr.csv, 163_g4ownm07q9.csv, 134_behtf6grhp.csv, 128_y3nqk9k0x4.csv, 93_7s7jo61qgr.csv, 125_onkofvwzgk.csv, 196_h8ebmn0b4i.csv, 186_77g5px55z1.csv, 80_zbkdixjx1u.csv, 154_i7n85wzpa3.csv, 59_tvvx1filo9.csv, 91_6nvvo85k2r.csv, 26_kcs08kmlzv.csv, 146_umxfdzx3sg.csv, 157_7onih683qj.csv, 122_hmocfdllhz.csv, 102_r9vbr9i6qn.csv, 25_zdmhvwm1pk.csv, 16_omnxwedc88.csv, 116_wf72qb1ctj.csv, 87_zzzdc841sd.csv, 168_39vz7oesxk.csv, 24_pa58yc2bsn.csv, 51_km3hwahw1m.csv, 19_sevtqojtws.csv, 21_x8u5ofrlzo.csv, 75_gginvgfabf.csv, 45_4g2awfbbtn.csv, 4_xu9afrhg0o.csv, 25_ceghr7ntgk.csv, 15_jzm9dvtw73.csv, 89_28gvnlaz8j.csv, 57_l8ux51os5l.csv, 153_mwu7z89cr2.csv, 147_qz39am2soc.csv, 67_ip2ys2xhib.csv, 78_tshg4zfas1.csv, 142_d8jaik03id.csv, 5_wa4f0flvdd.csv, 181_wk573tzjde.csv, 193_kjtylxs5x6.csv, 34_egy98kczu6.csv, 85_i7f88e7b7g.csv, 106_oa27xk4eqt.csv, 156_yk739g3286.csv, 11_0na6xsa4cd.csv, 31_jhmyyhmjh5.csv, 149_tvntt1n3zf.csv, 6_qyv4vhzoe.csv, 63_j79b3nmr83.csv, 104_jos9y36cxr.csv, 151_8k3mbw3phz.csv, 60_zry6xyk2v2.csv, 86_kvt6l8gw8d.csv, 131_bivn5or23h.csv, 175_954mvwcwet.csv, 110_zq7vq985bq.csv, 52_wfyl6gdoxn.csv, 92_u8wrf1zhh5.csv, 71_jjxizibivj.csv, 169_2y8d92hhek.csv, 114_fkhid5yyst.csv, 173_op7k51t6eh.csv, 145_1p84lqxs5t.csv, 171_evt2lbu5wz.csv, 2_wrrttduqx0.csv, 107_jfii3uf345.csv, 164_qcoq7yni4y.csv, 160_9xq61i8z5q.csv, 150_hvfxdsp6uq.csv, 16_j1uppkkwpc.csv, 26_2k5thw2ykv.csv, 172_gg34wiyfxn.csv, 27_r6anjd4ie1.csv, 11_7vk8oa1o24.csv, 138_yx53ai19ar.csv, 44_zsqcy1dp0n.csv, 68_4fxf0cl1sa.csv, 77_gup8yvrjg8.csv, 23_lce4ue1yit.csv, 197_ni25hikvxf.csv, 55_o97cc78tyd.csv, 13_xb7ph2k0m6.csv, 126_s6vu9wsf2o.csv, 124_4p9vzltfyn.csv, 135_7g73f18y8h.csv, 194_tnwwh3zbnj.csv, 99_7fcky2iv54.csv, 49_8fcz4c6oqy.csv, 84_q4rh6yklie.csv, 28_7svph4yobz.csv, 38_38doqz3unu.csv, 5_lcampqa5d8.csv, 82_3qr5zxdcxe.csv, 180_jm6cn0n08c.csv, 66_8gquz8hbpz.csv, 40_zh8fjx6tiy.csv, 141_tvldgnl6pt.csv, 30_rb578trmsj.csv, 184_hzlbex83q3.csv, 56_lxflttkcgn.csv, 22_udsmb087i5.csv, 8_hfxm8dkunn.csv, 61_l3jkp84irv.csv, 139_1qp7vpj70o.csv, 73_4s2wt6mgps.csv, 7_viucp080gt.csv, 108_qkxdz8djgt.csv, 54_07djfakfxh.csv, 98_8et5hi6nqf.csv, 74_08dh1ae49m.csv, 21_x90g7ie9t7.csv, 81_dbvr7ry56o.csv, 121_aobrexg5sv.csv, 191_yxm7fc16uc.csv, 94_gcso78g1mh.csv, 167_bk26q01ex6.csv, 41_rrwls6y9fl.csv, 170_szpuak9x3b.csv, 47_qrdef2nnjn.csv, 13_y1i6zh8y9w.csv, 188_5sjzeb4w03.csv, 130_7a6m1my07s.csv, 192_isa0drpmu4.csv, 79_wkvkhcnsn0.csv, 2_9actgs9mi7.csv, 18_o65lxoj85y.csv, 3_j0hn3a0xzt.csv, 43_1nemhlm74b.csv, 20_z3zjd5sgm4.csv, 117_mgq0ymf5hc.csv, 0_imfn3fsnqb.csv, 88_p169fqr8hl.csv, 1_ofoaso8yxd.csv, 97_ksnb8zg7e2.csv, 159_wmf9y5lb3d.csv, 179_enmmbxkd9b.csv, 18_omkydub7qy.csv, 115_iocn2dpj2o.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|██████████| 2/2 [00:00<00:00, 19.36ba/s]100%|██████████| 2/2 [00:00<00:00, 19.30ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 29.93ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:04<19:05,  4.26s/it]  1%|          | 2/270 [00:05<10:15,  2.30s/it]  1%|          | 3/270 [00:06<07:24,  1.67s/it]  1%|▏         | 4/270 [00:07<06:06,  1.38s/it]  2%|▏         | 5/270 [00:07<05:21,  1.21s/it]  2%|▏         | 6/270 [00:08<04:54,  1.11s/it]  3%|▎         | 7/270 [00:09<04:36,  1.05s/it]  3%|▎         | 8/270 [00:10<04:25,  1.02s/it]  3%|▎         | 9/270 [00:11<04:16,  1.02it/s]  4%|▎         | 10/270 [00:12<04:11,  1.03it/s]  4%|▍         | 11/270 [00:13<04:06,  1.05it/s]  4%|▍         | 12/270 [00:14<04:02,  1.06it/s]  5%|▍         | 13/270 [00:15<04:00,  1.07it/s]  5%|▌         | 14/270 [00:16<03:58,  1.07it/s]  6%|▌         | 15/270 [00:17<03:56,  1.08it/s]  6%|▌         | 16/270 [00:18<03:55,  1.08it/s]  6%|▋         | 17/270 [00:19<03:55,  1.08it/s]  7%|▋         | 18/270 [00:19<03:54,  1.07it/s]  7%|▋         | 19/270 [00:20<03:53,  1.07it/s]  7%|▋         | 20/270 [00:21<03:53,  1.07it/s]  8%|▊         | 21/270 [00:22<03:50,  1.08it/s]  8%|▊         | 22/270 [00:23<03:49,  1.08it/s]  9%|▊         | 23/270 [00:24<03:49,  1.08it/s]  9%|▉         | 24/270 [00:25<03:49,  1.07it/s]  9%|▉         | 25/270 [00:26<03:49,  1.07it/s] 10%|▉         | 26/270 [00:27<03:48,  1.07it/s] 10%|█         | 27/270 [00:28<03:23,  1.20it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.13it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.25it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.44it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.29it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                
                                             [A 10%|█         | 27/270 [00:30<03:23,  1.20it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|█         | 28/270 [00:45<22:57,  5.69s/it] 11%|█         | 29/270 [00:46<17:20,  4.32s/it] 11%|█         | 30/270 [00:47<13:12,  3.30s/it] 11%|█▏        | 31/270 [00:48<10:18,  2.59s/it] 12%|█▏        | 32/270 [00:48<08:17,  2.09s/it] 12%|█▏        | 33/270 [00:49<06:53,  1.75s/it] 13%|█▎        | 34/270 [00:50<05:54,  1.50s/it] 13%|█▎        | 35/270 [00:51<05:13,  1.33s/it] 13%|█▎        | 36/270 [00:52<04:43,  1.21s/it] 14%|█▎        | 37/270 [00:53<04:23,  1.13s/it] 14%|█▍        | 38/270 [00:54<04:08,  1.07s/it] 14%|█▍        | 39/270 [00:55<03:58,  1.03s/it] 15%|█▍        | 40/270 [00:56<03:51,  1.00s/it] 15%|█▌        | 41/270 [00:57<03:44,  1.02it/s] 16%|█▌        | 42/270 [00:58<03:40,  1.03it/s] 16%|█▌        | 43/270 [00:59<03:37,  1.04it/s] 16%|█▋        | 44/270 [01:00<03:34,  1.05it/s] 17%|█▋        | 45/270 [01:01<03:33,  1.05it/s] 17%|█▋        | 46/270 [01:02<03:31,  1.06it/s] 17%|█▋        | 47/270 [01:03<03:30,  1.06it/s] 18%|█▊        | 48/270 [01:03<03:28,  1.06it/s] 18%|█▊        | 49/270 [01:04<03:27,  1.07it/s] 19%|█▊        | 50/270 [01:05<03:26,  1.07it/s]                                                 19%|█▊        | 50/270 [01:05<03:26,  1.07it/s] 19%|█▉        | 51/270 [01:06<03:25,  1.07it/s] 19%|█▉        | 52/270 [01:07<03:24,  1.07it/s] 20%|█▉        | 53/270 [01:08<03:23,  1.07it/s] 20%|██        | 54/270 [01:09<03:01,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 1.4193480014801025, 'eval_f1': 0.25157439883717253, 'eval_runtime': 2.651, 'eval_samples_per_second': 160.692, 'eval_steps_per_second': 2.64, 'epoch': 1.0}
{'loss': 1.3837, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.14it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.28it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                
                                             [A 20%|██        | 54/270 [01:11<03:01,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██        | 55/270 [01:26<20:59,  5.86s/it] 21%|██        | 56/270 [01:27<15:37,  4.38s/it] 21%|██        | 57/270 [01:28<11:52,  3.35s/it] 21%|██▏       | 58/270 [01:29<09:16,  2.62s/it] 22%|██▏       | 59/270 [01:30<07:26,  2.12s/it] 22%|██▏       | 60/270 [01:31<06:10,  1.76s/it] 23%|██▎       | 61/270 [01:32<05:16,  1.52s/it] 23%|██▎       | 62/270 [01:33<04:39,  1.34s/it] 23%|██▎       | 63/270 [01:34<04:12,  1.22s/it] 24%|██▎       | 64/270 [01:35<03:54,  1.14s/it] 24%|██▍       | 65/270 [01:36<03:40,  1.08s/it] 24%|██▍       | 66/270 [01:37<03:31,  1.03s/it] 25%|██▍       | 67/270 [01:38<03:24,  1.01s/it] 25%|██▌       | 68/270 [01:38<03:19,  1.01it/s] 26%|██▌       | 69/270 [01:39<03:15,  1.03it/s] 26%|██▌       | 70/270 [01:40<03:13,  1.03it/s] 26%|██▋       | 71/270 [01:41<03:10,  1.04it/s] 27%|██▋       | 72/270 [01:42<03:09,  1.05it/s] 27%|██▋       | 73/270 [01:43<03:07,  1.05it/s] 27%|██▋       | 74/270 [01:44<03:06,  1.05it/s] 28%|██▊       | 75/270 [01:45<03:04,  1.06it/s] 28%|██▊       | 76/270 [01:46<03:03,  1.06it/s] 29%|██▊       | 77/270 [01:47<03:02,  1.06it/s] 29%|██▉       | 78/270 [01:48<03:01,  1.06it/s] 29%|██▉       | 79/270 [01:49<03:00,  1.06it/s] 30%|██▉       | 80/270 [01:50<02:59,  1.06it/s] 30%|███       | 81/270 [01:50<02:39,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.7315717935562134, 'eval_f1': 0.6243835599241768, 'eval_runtime': 2.6536, 'eval_samples_per_second': 160.535, 'eval_steps_per_second': 2.638, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.07it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.64it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.40it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                
                                             [A 30%|███       | 81/270 [01:53<02:39,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|███       | 82/270 [02:09<19:03,  6.08s/it] 31%|███       | 83/270 [02:10<14:08,  4.54s/it] 31%|███       | 84/270 [02:11<10:43,  3.46s/it] 31%|███▏      | 85/270 [02:12<08:19,  2.70s/it] 32%|███▏      | 86/270 [02:12<06:39,  2.17s/it] 32%|███▏      | 87/270 [02:13<05:29,  1.80s/it] 33%|███▎      | 88/270 [02:14<04:40,  1.54s/it] 33%|███▎      | 89/270 [02:15<04:06,  1.36s/it] 33%|███▎      | 90/270 [02:16<03:41,  1.23s/it] 34%|███▎      | 91/270 [02:17<03:25,  1.15s/it] 34%|███▍      | 92/270 [02:18<03:12,  1.08s/it] 34%|███▍      | 93/270 [02:19<03:04,  1.04s/it] 35%|███▍      | 94/270 [02:20<02:58,  1.01s/it] 35%|███▌      | 95/270 [02:21<02:53,  1.01it/s] 36%|███▌      | 96/270 [02:22<02:50,  1.02it/s] 36%|███▌      | 97/270 [02:23<02:47,  1.03it/s] 36%|███▋      | 98/270 [02:24<02:45,  1.04it/s] 37%|███▋      | 99/270 [02:25<02:43,  1.05it/s] 37%|███▋      | 100/270 [02:26<02:41,  1.05it/s]                                                  37%|███▋      | 100/270 [02:26<02:41,  1.05it/s] 37%|███▋      | 101/270 [02:27<02:40,  1.05it/s] 38%|███▊      | 102/270 [02:28<02:39,  1.06it/s] 38%|███▊      | 103/270 [02:28<02:37,  1.06it/s] 39%|███▊      | 104/270 [02:29<02:36,  1.06it/s] 39%|███▉      | 105/270 [02:30<02:36,  1.06it/s] 39%|███▉      | 106/270 [02:31<02:35,  1.06it/s] 40%|███▉      | 107/270 [02:32<02:34,  1.06it/s] 40%|████      | 108/270 [02:33<02:16,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.3993729054927826, 'eval_f1': 0.9036462098269851, 'eval_runtime': 2.6559, 'eval_samples_per_second': 160.396, 'eval_steps_per_second': 2.636, 'epoch': 3.0}
{'loss': 0.5909, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.03it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.25it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.42it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                 
                                             [A 40%|████      | 108/270 [02:36<02:16,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████      | 109/270 [02:50<15:44,  5.87s/it] 41%|████      | 110/270 [02:51<11:42,  4.39s/it] 41%|████      | 111/270 [02:52<08:52,  3.35s/it] 41%|████▏     | 112/270 [02:53<06:54,  2.63s/it] 42%|████▏     | 113/270 [02:54<05:32,  2.12s/it] 42%|████▏     | 114/270 [02:55<04:35,  1.77s/it] 43%|████▎     | 115/270 [02:56<03:54,  1.52s/it] 43%|████▎     | 116/270 [02:57<03:26,  1.34s/it] 43%|████▎     | 117/270 [02:58<03:06,  1.22s/it] 44%|████▎     | 118/270 [02:59<02:52,  1.14s/it] 44%|████▍     | 119/270 [03:00<02:42,  1.08s/it] 44%|████▍     | 120/270 [03:01<02:36,  1.04s/it] 45%|████▍     | 121/270 [03:02<02:30,  1.01s/it] 45%|████▌     | 122/270 [03:03<02:26,  1.01it/s] 46%|████▌     | 123/270 [03:04<02:23,  1.03it/s] 46%|████▌     | 124/270 [03:05<02:21,  1.03it/s] 46%|████▋     | 125/270 [03:05<02:19,  1.04it/s] 47%|████▋     | 126/270 [03:06<02:17,  1.05it/s] 47%|████▋     | 127/270 [03:07<02:16,  1.05it/s] 47%|████▋     | 128/270 [03:08<02:14,  1.05it/s] 48%|████▊     | 129/270 [03:09<02:13,  1.05it/s] 48%|████▊     | 130/270 [03:10<02:12,  1.05it/s] 49%|████▊     | 131/270 [03:11<02:12,  1.05it/s] 49%|████▉     | 132/270 [03:12<02:19,  1.01s/it] 49%|████▉     | 133/270 [03:13<02:15,  1.01it/s] 50%|████▉     | 134/270 [03:14<02:13,  1.02it/s] 50%|█████     | 135/270 [03:15<01:57,  1.15it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.26214155554771423, 'eval_f1': 0.918187304645844, 'eval_runtime': 2.6864, 'eval_samples_per_second': 158.574, 'eval_steps_per_second': 2.606, 'epoch': 4.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.02it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.63it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.38it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.29it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                 
                                             [A 50%|█████     | 135/270 [03:17<01:57,  1.15it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 136/270 [03:33<13:26,  6.02s/it] 51%|█████     | 137/270 [03:34<09:58,  4.50s/it] 51%|█████     | 138/270 [03:35<07:33,  3.43s/it] 51%|█████▏    | 139/270 [03:36<05:51,  2.68s/it] 52%|█████▏    | 140/270 [03:37<04:40,  2.16s/it] 52%|█████▏    | 141/270 [03:38<03:51,  1.79s/it] 53%|█████▎    | 142/270 [03:39<03:17,  1.54s/it] 53%|█████▎    | 143/270 [03:39<02:53,  1.36s/it] 53%|█████▎    | 144/270 [03:40<02:35,  1.24s/it] 54%|█████▎    | 145/270 [03:41<02:23,  1.15s/it] 54%|█████▍    | 146/270 [03:42<02:15,  1.09s/it] 54%|█████▍    | 147/270 [03:43<02:08,  1.04s/it] 55%|█████▍    | 148/270 [03:44<02:03,  1.02s/it] 55%|█████▌    | 149/270 [03:45<02:00,  1.01it/s] 56%|█████▌    | 150/270 [03:46<01:57,  1.02it/s]                                                  56%|█████▌    | 150/270 [03:46<01:57,  1.02it/s] 56%|█████▌    | 151/270 [03:47<01:55,  1.03it/s] 56%|█████▋    | 152/270 [03:48<01:54,  1.03it/s] 57%|█████▋    | 153/270 [03:49<01:52,  1.04it/s] 57%|█████▋    | 154/270 [03:50<01:51,  1.04it/s] 57%|█████▋    | 155/270 [03:51<01:50,  1.04it/s] 58%|█████▊    | 156/270 [03:52<01:48,  1.05it/s] 58%|█████▊    | 157/270 [03:53<01:47,  1.05it/s] 59%|█████▊    | 158/270 [03:54<01:46,  1.05it/s] 59%|█████▉    | 159/270 [03:55<01:45,  1.05it/s] 59%|█████▉    | 160/270 [03:56<01:45,  1.05it/s] 60%|█████▉    | 161/270 [03:57<01:44,  1.04it/s] 60%|██████    | 162/270 [03:57<01:32,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.2238810509443283, 'eval_f1': 0.9388406470768059, 'eval_runtime': 2.6487, 'eval_samples_per_second': 160.831, 'eval_steps_per_second': 2.643, 'epoch': 5.0}
{'loss': 0.2478, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.00it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.23it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.73it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.29it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.55it/s][A                                                 
                                             [A 60%|██████    | 162/270 [04:00<01:32,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.55it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████    | 163/270 [04:14<10:16,  5.76s/it] 61%|██████    | 164/270 [04:15<07:36,  4.31s/it] 61%|██████    | 165/270 [04:16<05:46,  3.30s/it] 61%|██████▏   | 166/270 [04:17<04:29,  2.59s/it] 62%|██████▏   | 167/270 [04:18<03:36,  2.10s/it] 62%|██████▏   | 168/270 [04:19<02:58,  1.75s/it] 63%|██████▎   | 169/270 [04:20<02:32,  1.51s/it] 63%|██████▎   | 170/270 [04:21<02:13,  1.34s/it] 63%|██████▎   | 171/270 [04:22<02:01,  1.22s/it] 64%|██████▎   | 172/270 [04:23<01:51,  1.14s/it] 64%|██████▍   | 173/270 [04:24<01:44,  1.08s/it] 64%|██████▍   | 174/270 [04:25<01:39,  1.04s/it] 65%|██████▍   | 175/270 [04:26<01:35,  1.01s/it] 65%|██████▌   | 176/270 [04:27<01:33,  1.01it/s] 66%|██████▌   | 177/270 [04:28<01:30,  1.02it/s] 66%|██████▌   | 178/270 [04:29<01:29,  1.03it/s] 66%|██████▋   | 179/270 [04:29<01:27,  1.04it/s] 67%|██████▋   | 180/270 [04:30<01:26,  1.05it/s] 67%|██████▋   | 181/270 [04:31<01:24,  1.05it/s] 67%|██████▋   | 182/270 [04:32<01:23,  1.05it/s] 68%|██████▊   | 183/270 [04:33<01:22,  1.05it/s] 68%|██████▊   | 184/270 [04:34<01:21,  1.06it/s] 69%|██████▊   | 185/270 [04:35<01:20,  1.06it/s] 69%|██████▉   | 186/270 [04:36<01:19,  1.06it/s] 69%|██████▉   | 187/270 [04:37<01:18,  1.06it/s] 70%|██████▉   | 188/270 [04:38<01:17,  1.05it/s] 70%|███████   | 189/270 [04:39<01:08,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.20113025605678558, 'eval_f1': 0.9433096891969066, 'eval_runtime': 2.8207, 'eval_samples_per_second': 151.026, 'eval_steps_per_second': 2.482, 'epoch': 6.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.04it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.64it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.37it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.24it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.51it/s][A                                                 
                                             [A 70%|███████   | 189/270 [04:41<01:08,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|███████   | 190/270 [04:56<07:53,  5.92s/it] 71%|███████   | 191/270 [04:57<05:49,  4.43s/it] 71%|███████   | 192/270 [04:58<04:23,  3.38s/it] 71%|███████▏  | 193/270 [04:59<03:23,  2.65s/it] 72%|███████▏  | 194/270 [05:00<02:42,  2.13s/it] 72%|███████▏  | 195/270 [05:01<02:13,  1.78s/it] 73%|███████▎  | 196/270 [05:02<01:52,  1.52s/it] 73%|███████▎  | 197/270 [05:03<01:38,  1.36s/it] 73%|███████▎  | 198/270 [05:04<01:28,  1.23s/it] 74%|███████▎  | 199/270 [05:05<01:21,  1.15s/it] 74%|███████▍  | 200/270 [05:06<01:15,  1.08s/it]                                                  74%|███████▍  | 200/270 [05:06<01:15,  1.08s/it] 74%|███████▍  | 201/270 [05:07<01:12,  1.04s/it] 75%|███████▍  | 202/270 [05:08<01:09,  1.02s/it] 75%|███████▌  | 203/270 [05:09<01:06,  1.01it/s] 76%|███████▌  | 204/270 [05:10<01:04,  1.02it/s] 76%|███████▌  | 205/270 [05:11<01:02,  1.03it/s] 76%|███████▋  | 206/270 [05:11<01:01,  1.04it/s] 77%|███████▋  | 207/270 [05:12<01:00,  1.05it/s] 77%|███████▋  | 208/270 [05:13<00:59,  1.05it/s] 77%|███████▋  | 209/270 [05:14<00:58,  1.05it/s] 78%|███████▊  | 210/270 [05:15<00:57,  1.05it/s] 78%|███████▊  | 211/270 [05:16<00:56,  1.05it/s] 79%|███████▊  | 212/270 [05:17<00:55,  1.05it/s] 79%|███████▉  | 213/270 [05:18<00:54,  1.05it/s] 79%|███████▉  | 214/270 [05:19<00:53,  1.05it/s] 80%|███████▉  | 215/270 [05:20<00:52,  1.05it/s] 80%|████████  | 216/270 [05:21<00:45,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.1921602338552475, 'eval_f1': 0.943087328502035, 'eval_runtime': 2.6763, 'eval_samples_per_second': 159.176, 'eval_steps_per_second': 2.616, 'epoch': 7.0}
{'loss': 0.1711, 'learning_rate': 1.5687918106563326e-06, 'epoch': 7.41}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.05it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.65it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.42it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.29it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.57it/s][A                                                 
                                             [A 80%|████████  | 216/270 [05:23<00:45,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.57it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-216
Configuration saved in training-output/checkpoint-216/config.json
Model weights saved in training-output/checkpoint-216/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-216/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-216/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████  | 217/270 [05:39<05:26,  6.16s/it] 81%|████████  | 218/270 [05:40<03:58,  4.59s/it] 81%|████████  | 219/270 [05:41<02:58,  3.50s/it] 81%|████████▏ | 220/270 [05:42<02:16,  2.72s/it] 82%|████████▏ | 221/270 [05:43<01:47,  2.19s/it] 82%|████████▏ | 222/270 [05:44<01:27,  1.81s/it] 83%|████████▎ | 223/270 [05:45<01:13,  1.55s/it] 83%|████████▎ | 224/270 [05:46<01:03,  1.37s/it] 83%|████████▎ | 225/270 [05:47<00:55,  1.24s/it] 84%|████████▎ | 226/270 [05:48<00:50,  1.15s/it] 84%|████████▍ | 227/270 [05:49<00:46,  1.09s/it] 84%|████████▍ | 228/270 [05:50<00:44,  1.05s/it] 85%|████████▍ | 229/270 [05:50<00:41,  1.01s/it] 85%|████████▌ | 230/270 [05:51<00:39,  1.01it/s] 86%|████████▌ | 231/270 [05:52<00:38,  1.02it/s] 86%|████████▌ | 232/270 [05:53<00:36,  1.03it/s] 86%|████████▋ | 233/270 [05:54<00:35,  1.04it/s] 87%|████████▋ | 234/270 [05:55<00:34,  1.04it/s] 87%|████████▋ | 235/270 [05:56<00:33,  1.05it/s] 87%|████████▋ | 236/270 [05:57<00:32,  1.05it/s] 88%|████████▊ | 237/270 [05:58<00:31,  1.05it/s] 88%|████████▊ | 238/270 [05:59<00:30,  1.05it/s] 89%|████████▊ | 239/270 [06:00<00:29,  1.05it/s] 89%|████████▉ | 240/270 [06:01<00:28,  1.05it/s] 89%|████████▉ | 241/270 [06:02<00:27,  1.05it/s] 90%|████████▉ | 242/270 [06:03<00:26,  1.05it/s] 90%|█████████ | 243/270 [06:03<00:23,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.18531358242034912, 'eval_f1': 0.943087328502035, 'eval_runtime': 2.6425, 'eval_samples_per_second': 161.208, 'eval_steps_per_second': 2.649, 'epoch': 8.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.08it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.66it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.36it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  2.66it/s][A
100%|██████████| 7/7 [00:02<00:00,  3.02it/s][A                                                 
                                             [A 90%|█████████ | 243/270 [06:06<00:23,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.02it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-243
Configuration saved in training-output/checkpoint-243/config.json
Model weights saved in training-output/checkpoint-243/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-243/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-243/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|█████████ | 244/270 [06:23<02:51,  6.60s/it] 91%|█████████ | 245/270 [06:24<02:02,  4.90s/it] 91%|█████████ | 246/270 [06:25<01:29,  3.71s/it] 91%|█████████▏| 247/270 [06:26<01:06,  2.88s/it] 92%|█████████▏| 248/270 [06:27<00:50,  2.30s/it] 92%|█████████▏| 249/270 [06:28<00:39,  1.89s/it] 93%|█████████▎| 250/270 [06:29<00:32,  1.61s/it]                                                  93%|█████████▎| 250/270 [06:29<00:32,  1.61s/it] 93%|█████████▎| 251/270 [06:30<00:26,  1.41s/it] 93%|█████████▎| 252/270 [06:31<00:22,  1.27s/it] 94%|█████████▎| 253/270 [06:32<00:19,  1.17s/it] 94%|█████████▍| 254/270 [06:33<00:17,  1.11s/it] 94%|█████████▍| 255/270 [06:34<00:15,  1.06s/it] 95%|█████████▍| 256/270 [06:35<00:14,  1.02s/it] 95%|█████████▌| 257/270 [06:36<00:13,  1.00s/it] 96%|█████████▌| 258/270 [06:37<00:11,  1.01it/s] 96%|█████████▌| 259/270 [06:38<00:10,  1.02it/s] 96%|█████████▋| 260/270 [06:39<00:09,  1.03it/s] 97%|█████████▋| 261/270 [06:40<00:08,  1.04it/s] 97%|█████████▋| 262/270 [06:41<00:07,  1.04it/s] 97%|█████████▋| 263/270 [06:41<00:06,  1.05it/s] 98%|█████████▊| 264/270 [06:42<00:05,  1.05it/s] 98%|█████████▊| 265/270 [06:43<00:04,  1.05it/s] 99%|█████████▊| 266/270 [06:44<00:03,  1.05it/s] 99%|█████████▉| 267/270 [06:45<00:02,  1.05it/s] 99%|█████████▉| 268/270 [06:46<00:01,  1.05it/s]100%|█████████▉| 269/270 [06:47<00:00,  1.05it/s]100%|██████████| 270/270 [06:48<00:00,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.18470464646816254, 'eval_f1': 0.943087328502035, 'eval_runtime': 2.8709, 'eval_samples_per_second': 148.385, 'eval_steps_per_second': 2.438, 'epoch': 9.0}
{'loss': 0.1407, 'learning_rate': 1.3477564710088097e-07, 'epoch': 9.26}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.12it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.60it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.34it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.21it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.49it/s][A                                                 
                                             [A100%|██████████| 270/270 [06:50<00:00,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.49it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-270
Configuration saved in training-output/checkpoint-270/config.json
Model weights saved in training-output/checkpoint-270/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-270/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-270/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-162 (score: 0.9433096891969066).
                                                 100%|██████████| 270/270 [07:05<00:00,  1.17it/s]100%|██████████| 270/270 [07:05<00:00,  1.58s/it]
{'eval_loss': 0.18391132354736328, 'eval_f1': 0.943087328502035, 'eval_runtime': 2.7004, 'eval_samples_per_second': 157.754, 'eval_steps_per_second': 2.592, 'epoch': 10.0}
{'train_runtime': 425.5759, 'train_samples_per_second': 39.946, 'train_steps_per_second': 0.634, 'train_loss': 0.4785304148991903, 'epoch': 10.0}
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:06, 16.59it/s]  4%|▎         | 4/107 [00:00<00:05, 17.73it/s]  7%|▋         | 7/107 [00:00<00:04, 22.23it/s]  9%|▉         | 10/107 [00:00<00:03, 24.46it/s] 12%|█▏        | 13/107 [00:00<00:03, 25.70it/s] 15%|█▍        | 16/107 [00:00<00:03, 26.36it/s] 18%|█▊        | 19/107 [00:00<00:03, 26.88it/s] 21%|██        | 22/107 [00:00<00:03, 27.16it/s] 23%|██▎       | 25/107 [00:00<00:03, 27.28it/s] 26%|██▌       | 28/107 [00:01<00:02, 27.51it/s] 29%|██▉       | 31/107 [00:01<00:02, 27.25it/s] 32%|███▏      | 34/107 [00:01<00:02, 27.33it/s] 35%|███▍      | 37/107 [00:01<00:02, 27.52it/s] 37%|███▋      | 40/107 [00:01<00:02, 27.56it/s] 40%|████      | 43/107 [00:01<00:02, 27.56it/s] 43%|████▎     | 46/107 [00:01<00:02, 27.70it/s] 46%|████▌     | 49/107 [00:01<00:02, 27.80it/s] 49%|████▊     | 52/107 [00:01<00:01, 27.78it/s] 51%|█████▏    | 55/107 [00:02<00:01, 27.85it/s] 54%|█████▍    | 58/107 [00:02<00:01, 27.77it/s] 57%|█████▋    | 61/107 [00:02<00:01, 27.55it/s] 60%|█████▉    | 64/107 [00:02<00:01, 27.71it/s] 63%|██████▎   | 67/107 [00:02<00:01, 27.77it/s] 65%|██████▌   | 70/107 [00:02<00:01, 27.56it/s] 68%|██████▊   | 73/107 [00:02<00:01, 27.78it/s] 71%|███████   | 76/107 [00:02<00:01, 27.80it/s] 74%|███████▍  | 79/107 [00:02<00:01, 27.80it/s] 77%|███████▋  | 82/107 [00:03<00:00, 27.81it/s] 79%|███████▉  | 85/107 [00:03<00:00, 27.72it/s] 82%|████████▏ | 88/107 [00:03<00:00, 27.52it/s] 85%|████████▌ | 91/107 [00:03<00:00, 27.62it/s] 88%|████████▊ | 94/107 [00:03<00:00, 27.64it/s] 91%|█████████ | 97/107 [00:03<00:00, 27.57it/s] 93%|█████████▎| 100/107 [00:03<00:00, 27.65it/s] 96%|█████████▋| 103/107 [00:03<00:00, 27.70it/s] 99%|█████████▉| 106/107 [00:03<00:00, 27.71it/s]100%|██████████| 107/107 [00:03<00:00, 27.22it/s]
Model finished with accuracy: 0.9436619718309859, macro-f1: 0.9433096891969066
Confusion matrix:
[[81  0  0  1  4]
 [ 1 73  2  3  4]
 [ 0  0 68  3  0]
 [ 0  0  3 95  0]
 [ 2  1  0  0 85]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14_mx46s0fx52.csv, 58_lvcsk0yy8g.csv, 133_c0kk72b3fm.csv, 132_s5i7ngx7bu.csv, 112_7g35kmtkhr.csv, 101_9l21auewdi.csv, 69_4tlegp20n1.csv, 35_g0ldvn0rjm.csv, 143_w11kkydg7n.csv, 20_1zahygarmq.csv, 53_4t2mju8m3y.csv, 166_5lv4sjg1qv.csv, 105_3acmrslh5t.csv, 198_shm86f7xzz.csv, 155_97dannvd9q.csv, 189_4mh7pjb6hj.csv, 187_kp3lfo9rti.csv, 3_5uxjfwf4c9.csv, 76_f4jqlrtl0d.csv, 152_uhh1hem3ei.csv, 14_apv4u3qebs.csv, 127_q2ifamcjop.csv, 100_5397borrxc.csv, 129_h8c8ny4x7u.csv, 123_g66af1qsqo.csv, 95_6ikda3av09.csv, 42_kr1ailiclk.csv, 162_gzcq0eli5q.csv, 148_nekma0liim.csv, 50_tmt7xio73x.csv, 183_ex67vzlp8e.csv, 64_gjf3ihxyfj.csv, 182_icy94tmqbq.csv, 96_fe55r9bnfd.csv, 37_7swqi7vgnl.csv, 17_qvtzqld86i.csv, 185_9bjzxvbwca.csv, 4_vet6mgh5md.csv, 109_ehl2s3ah2w.csv, 36_4bg4d4bf4q.csv, 165_vcamdtt7pl.csv, 136_dd735psc8g.csv, 24_1y633kulxy.csv, 7_swfaqmk6y6.csv, 46_eg7yytwm57.csv, 12_i0qb55ygnr.csv, 8_v4bqc7mvzy.csv, 120_tfdaxlk9n2.csv, 103_v129kmq3c5.csv, 144_6g4qagf0aw.csv, 72_37k8qe6uf3.csv, 111_4dftcjzv4j.csv, 1_z949c1i541.csv, 178_7rflrddsnn.csv, 6_blz9uaf562.csv, 15_j7tl3bmirm.csv, 83_slgfcmc4xj.csv, 174_vw5a8s9vzj.csv, 9_m11ut0fzvo.csv, 199_hzu91su6jo.csv, 39_3064xz3lgl.csv, 62_6v7alh2xza.csv, 137_eusnnkienm.csv, 118_66k0pevslq.csv, 113_s8v38lnghb.csv, 90_60znzzhvt3.csv, 161_qu7m9mil3a.csv, 32_jitoqenvci.csv, 176_cl4orgir7l.csv, 65_5y9bhpar44.csv, 27_adgb2e3ry7.csv, 33_zybtjc12ef.csv, 140_rpfb54del9.csv, 70_0b98yn7zuy.csv, 158_lhrhfgkazf.csv, 10_ten3l2nuwn.csv, 48_u6mx1k6p64.csv, 29_juqgc9mrcx.csv, 195_nwk7f5mq3h.csv, 119_hdkgwx4tgr.csv, 163_g4ownm07q9.csv, 134_behtf6grhp.csv, 128_y3nqk9k0x4.csv, 93_7s7jo61qgr.csv, 125_onkofvwzgk.csv, 196_h8ebmn0b4i.csv, 186_77g5px55z1.csv, 80_zbkdixjx1u.csv, 154_i7n85wzpa3.csv, 59_tvvx1filo9.csv, 91_6nvvo85k2r.csv, 26_kcs08kmlzv.csv, 146_umxfdzx3sg.csv, 157_7onih683qj.csv, 122_hmocfdllhz.csv, 102_r9vbr9i6qn.csv, 25_zdmhvwm1pk.csv, 16_omnxwedc88.csv, 116_wf72qb1ctj.csv, 87_zzzdc841sd.csv, 168_39vz7oesxk.csv, 24_pa58yc2bsn.csv, 51_km3hwahw1m.csv, 19_sevtqojtws.csv, 21_x8u5ofrlzo.csv, 75_gginvgfabf.csv, 45_4g2awfbbtn.csv, 4_xu9afrhg0o.csv, 25_ceghr7ntgk.csv, 15_jzm9dvtw73.csv, 89_28gvnlaz8j.csv, 57_l8ux51os5l.csv, 153_mwu7z89cr2.csv, 147_qz39am2soc.csv, 67_ip2ys2xhib.csv, 78_tshg4zfas1.csv, 142_d8jaik03id.csv, 5_wa4f0flvdd.csv, 181_wk573tzjde.csv, 193_kjtylxs5x6.csv, 34_egy98kczu6.csv, 85_i7f88e7b7g.csv, 106_oa27xk4eqt.csv, 156_yk739g3286.csv, 11_0na6xsa4cd.csv, 31_jhmyyhmjh5.csv, 149_tvntt1n3zf.csv, 6_qyv4vhzoe.csv, 63_j79b3nmr83.csv, 104_jos9y36cxr.csv, 151_8k3mbw3phz.csv, 60_zry6xyk2v2.csv, 86_kvt6l8gw8d.csv, 131_bivn5or23h.csv, 175_954mvwcwet.csv, 110_zq7vq985bq.csv, 52_wfyl6gdoxn.csv, 92_u8wrf1zhh5.csv, 71_jjxizibivj.csv, 169_2y8d92hhek.csv, 114_fkhid5yyst.csv, 173_op7k51t6eh.csv, 145_1p84lqxs5t.csv, 171_evt2lbu5wz.csv, 2_wrrttduqx0.csv, 107_jfii3uf345.csv, 164_qcoq7yni4y.csv, 160_9xq61i8z5q.csv, 150_hvfxdsp6uq.csv, 16_j1uppkkwpc.csv, 26_2k5thw2ykv.csv, 172_gg34wiyfxn.csv, 27_r6anjd4ie1.csv, 11_7vk8oa1o24.csv, 138_yx53ai19ar.csv, 44_zsqcy1dp0n.csv, 68_4fxf0cl1sa.csv, 77_gup8yvrjg8.csv, 23_lce4ue1yit.csv, 197_ni25hikvxf.csv, 55_o97cc78tyd.csv, 13_xb7ph2k0m6.csv, 126_s6vu9wsf2o.csv, 124_4p9vzltfyn.csv, 135_7g73f18y8h.csv, 194_tnwwh3zbnj.csv, 99_7fcky2iv54.csv, 49_8fcz4c6oqy.csv, 84_q4rh6yklie.csv, 28_7svph4yobz.csv, 38_38doqz3unu.csv, 5_lcampqa5d8.csv, 82_3qr5zxdcxe.csv, 180_jm6cn0n08c.csv, 66_8gquz8hbpz.csv, 40_zh8fjx6tiy.csv, 141_tvldgnl6pt.csv, 30_rb578trmsj.csv, 184_hzlbex83q3.csv, 56_lxflttkcgn.csv, 22_udsmb087i5.csv, 8_hfxm8dkunn.csv, 61_l3jkp84irv.csv, 139_1qp7vpj70o.csv, 73_4s2wt6mgps.csv, 7_viucp080gt.csv, 108_qkxdz8djgt.csv, 54_07djfakfxh.csv, 98_8et5hi6nqf.csv, 74_08dh1ae49m.csv, 21_x90g7ie9t7.csv, 81_dbvr7ry56o.csv, 121_aobrexg5sv.csv, 191_yxm7fc16uc.csv, 94_gcso78g1mh.csv, 167_bk26q01ex6.csv, 41_rrwls6y9fl.csv, 170_szpuak9x3b.csv, 47_qrdef2nnjn.csv, 13_y1i6zh8y9w.csv, 188_5sjzeb4w03.csv, 130_7a6m1my07s.csv, 192_isa0drpmu4.csv, 79_wkvkhcnsn0.csv, 2_9actgs9mi7.csv, 18_o65lxoj85y.csv, 3_j0hn3a0xzt.csv, 43_1nemhlm74b.csv, 20_z3zjd5sgm4.csv, 117_mgq0ymf5hc.csv, 0_imfn3fsnqb.csv, 88_p169fqr8hl.csv, 1_ofoaso8yxd.csv, 97_ksnb8zg7e2.csv, 159_wmf9y5lb3d.csv, 179_enmmbxkd9b.csv, 18_omkydub7qy.csv, 115_iocn2dpj2o.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|██████████| 2/2 [00:00<00:00, 17.75ba/s]100%|██████████| 2/2 [00:00<00:00, 17.69ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 37.87ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:04<20:20,  4.54s/it]  1%|          | 2/270 [00:05<10:46,  2.41s/it]  1%|          | 3/270 [00:06<07:41,  1.73s/it]  1%|▏         | 4/270 [00:07<06:15,  1.41s/it]  2%|▏         | 5/270 [00:08<05:26,  1.23s/it]  2%|▏         | 6/270 [00:09<04:57,  1.13s/it]  3%|▎         | 7/270 [00:10<04:38,  1.06s/it]  3%|▎         | 8/270 [00:10<04:26,  1.02s/it]  3%|▎         | 9/270 [00:11<04:17,  1.01it/s]  4%|▎         | 10/270 [00:12<04:11,  1.03it/s]  4%|▍         | 11/270 [00:13<04:06,  1.05it/s]  4%|▍         | 12/270 [00:14<04:03,  1.06it/s]  5%|▍         | 13/270 [00:15<04:01,  1.07it/s]  5%|▌         | 14/270 [00:16<04:00,  1.07it/s]  6%|▌         | 15/270 [00:17<03:58,  1.07it/s]  6%|▌         | 16/270 [00:18<03:57,  1.07it/s]  6%|▋         | 17/270 [00:19<03:56,  1.07it/s]  7%|▋         | 18/270 [00:20<03:56,  1.07it/s]  7%|▋         | 19/270 [00:21<03:54,  1.07it/s]  7%|▋         | 20/270 [00:22<03:53,  1.07it/s]  8%|▊         | 21/270 [00:23<03:51,  1.07it/s]  8%|▊         | 22/270 [00:23<03:50,  1.07it/s]  9%|▊         | 23/270 [00:24<03:49,  1.08it/s]  9%|▉         | 24/270 [00:25<03:48,  1.08it/s]  9%|▉         | 25/270 [00:26<03:47,  1.08it/s] 10%|▉         | 26/270 [00:27<03:46,  1.08it/s] 10%|█         | 27/270 [00:28<03:22,  1.20it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.07it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.30it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.46it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.33it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.57it/s][A                                                
                                             [A 10%|█         | 27/270 [00:30<03:22,  1.20it/s]
100%|██████████| 7/7 [00:02<00:00,  3.57it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|█         | 28/270 [00:46<24:26,  6.06s/it] 11%|█         | 29/270 [00:47<18:23,  4.58s/it] 11%|█         | 30/270 [00:48<13:56,  3.48s/it] 11%|█▏        | 31/270 [00:49<10:49,  2.72s/it] 12%|█▏        | 32/270 [00:50<08:39,  2.18s/it] 12%|█▏        | 33/270 [00:51<07:07,  1.81s/it] 13%|█▎        | 34/270 [00:52<06:04,  1.54s/it] 13%|█▎        | 35/270 [00:53<05:19,  1.36s/it] 13%|█▎        | 36/270 [00:54<04:48,  1.23s/it] 14%|█▎        | 37/270 [00:55<04:25,  1.14s/it] 14%|█▍        | 38/270 [00:56<04:10,  1.08s/it] 14%|█▍        | 39/270 [00:57<03:59,  1.04s/it] 15%|█▍        | 40/270 [00:57<03:51,  1.01s/it] 15%|█▌        | 41/270 [00:58<03:45,  1.02it/s] 16%|█▌        | 42/270 [00:59<03:41,  1.03it/s] 16%|█▌        | 43/270 [01:00<03:37,  1.04it/s] 16%|█▋        | 44/270 [01:01<03:34,  1.05it/s] 17%|█▋        | 45/270 [01:02<03:32,  1.06it/s] 17%|█▋        | 46/270 [01:03<03:31,  1.06it/s] 17%|█▋        | 47/270 [01:04<03:29,  1.06it/s] 18%|█▊        | 48/270 [01:05<03:29,  1.06it/s] 18%|█▊        | 49/270 [01:06<03:27,  1.06it/s] 19%|█▊        | 50/270 [01:07<03:26,  1.06it/s]                                                 19%|█▊        | 50/270 [01:07<03:26,  1.06it/s] 19%|█▉        | 51/270 [01:08<03:27,  1.06it/s] 19%|█▉        | 52/270 [01:09<03:25,  1.06it/s] 20%|█▉        | 53/270 [01:10<03:24,  1.06it/s] 20%|██        | 54/270 [01:10<03:01,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 1.3594824075698853, 'eval_f1': 0.3229272627695742, 'eval_runtime': 2.644, 'eval_samples_per_second': 161.117, 'eval_steps_per_second': 2.647, 'epoch': 1.0}
{'loss': 1.3469, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.99it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.25it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.71it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.46it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.29it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.55it/s][A                                                
                                             [A 20%|██        | 54/270 [01:13<03:01,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.55it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██        | 55/270 [01:29<22:37,  6.32s/it] 21%|██        | 56/270 [01:30<16:46,  4.70s/it] 21%|██        | 57/270 [01:31<12:40,  3.57s/it] 21%|██▏       | 58/270 [01:32<09:48,  2.78s/it] 22%|██▏       | 59/270 [01:33<07:49,  2.23s/it] 22%|██▏       | 60/270 [01:34<06:25,  1.84s/it] 23%|██▎       | 61/270 [01:35<05:28,  1.57s/it] 23%|██▎       | 62/270 [01:36<04:46,  1.38s/it] 23%|██▎       | 63/270 [01:37<04:18,  1.25s/it] 24%|██▎       | 64/270 [01:38<03:58,  1.16s/it] 24%|██▍       | 65/270 [01:39<03:43,  1.09s/it] 24%|██▍       | 66/270 [01:40<03:33,  1.05s/it] 25%|██▍       | 67/270 [01:41<03:25,  1.01s/it] 25%|██▌       | 68/270 [01:42<03:20,  1.01it/s] 26%|██▌       | 69/270 [01:42<03:16,  1.02it/s] 26%|██▌       | 70/270 [01:43<03:13,  1.03it/s] 26%|██▋       | 71/270 [01:44<03:10,  1.05it/s] 27%|██▋       | 72/270 [01:45<03:09,  1.05it/s] 27%|██▋       | 73/270 [01:46<03:07,  1.05it/s] 27%|██▋       | 74/270 [01:47<03:06,  1.05it/s] 28%|██▊       | 75/270 [01:48<03:04,  1.05it/s] 28%|██▊       | 76/270 [01:49<03:04,  1.05it/s] 29%|██▊       | 77/270 [01:50<03:02,  1.06it/s] 29%|██▉       | 78/270 [01:51<03:01,  1.06it/s] 29%|██▉       | 79/270 [01:52<03:01,  1.05it/s] 30%|██▉       | 80/270 [01:53<02:59,  1.06it/s] 30%|███       | 81/270 [01:53<02:39,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.6486768126487732, 'eval_f1': 0.7894031856260358, 'eval_runtime': 2.6509, 'eval_samples_per_second': 160.701, 'eval_steps_per_second': 2.641, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.12it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.33it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.70it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.26it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.49it/s][A                                                
                                             [A 30%|███       | 81/270 [01:56<02:39,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.49it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|███       | 82/270 [02:12<19:19,  6.17s/it] 31%|███       | 83/270 [02:13<14:19,  4.60s/it] 31%|███       | 84/270 [02:14<10:50,  3.50s/it] 31%|███▏      | 85/270 [02:15<08:26,  2.74s/it] 32%|███▏      | 86/270 [02:16<06:44,  2.20s/it] 32%|███▏      | 87/270 [02:17<05:33,  1.82s/it] 33%|███▎      | 88/270 [02:18<04:43,  1.56s/it] 33%|███▎      | 89/270 [02:19<04:07,  1.37s/it] 33%|███▎      | 90/270 [02:20<03:43,  1.24s/it] 34%|███▎      | 91/270 [02:21<03:26,  1.15s/it] 34%|███▍      | 92/270 [02:21<03:14,  1.09s/it] 34%|███▍      | 93/270 [02:22<03:05,  1.05s/it] 35%|███▍      | 94/270 [02:23<02:58,  1.01s/it] 35%|███▌      | 95/270 [02:24<02:53,  1.01it/s] 36%|███▌      | 96/270 [02:25<02:50,  1.02it/s] 36%|███▌      | 97/270 [02:26<02:47,  1.03it/s] 36%|███▋      | 98/270 [02:27<02:45,  1.04it/s] 37%|███▋      | 99/270 [02:28<02:44,  1.04it/s] 37%|███▋      | 100/270 [02:29<02:42,  1.05it/s]                                                  37%|███▋      | 100/270 [02:29<02:42,  1.05it/s] 37%|███▋      | 101/270 [02:30<02:41,  1.05it/s] 38%|███▊      | 102/270 [02:31<02:39,  1.05it/s] 38%|███▊      | 103/270 [02:32<02:38,  1.05it/s] 39%|███▊      | 104/270 [02:33<02:37,  1.05it/s] 39%|███▉      | 105/270 [02:34<02:36,  1.05it/s] 39%|███▉      | 106/270 [02:35<02:35,  1.06it/s] 40%|███▉      | 107/270 [02:36<02:34,  1.06it/s] 40%|████      | 108/270 [02:36<02:16,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.3539389967918396, 'eval_f1': 0.8948680684365922, 'eval_runtime': 2.8802, 'eval_samples_per_second': 147.907, 'eval_steps_per_second': 2.43, 'epoch': 3.0}
{'loss': 0.4995, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.04it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.27it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.71it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.42it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.52it/s][A                                                 
                                             [A 40%|████      | 108/270 [02:39<02:16,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.52it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████      | 109/270 [02:54<15:56,  5.94s/it] 41%|████      | 110/270 [02:55<11:50,  4.44s/it] 41%|████      | 111/270 [02:56<08:58,  3.39s/it] 41%|████▏     | 112/270 [02:57<06:59,  2.65s/it] 42%|████▏     | 113/270 [02:58<05:35,  2.14s/it] 42%|████▏     | 114/270 [02:59<04:37,  1.78s/it] 43%|████▎     | 115/270 [03:00<03:56,  1.53s/it] 43%|████▎     | 116/270 [03:01<03:28,  1.36s/it] 43%|████▎     | 117/270 [03:02<03:08,  1.23s/it] 44%|████▎     | 118/270 [03:03<02:53,  1.14s/it] 44%|████▍     | 119/270 [03:04<02:43,  1.09s/it] 44%|████▍     | 120/270 [03:04<02:36,  1.04s/it] 45%|████▍     | 121/270 [03:05<02:30,  1.01s/it] 45%|████▌     | 122/270 [03:06<02:26,  1.01it/s] 46%|████▌     | 123/270 [03:07<02:23,  1.02it/s] 46%|████▌     | 124/270 [03:08<02:21,  1.03it/s] 46%|████▋     | 125/270 [03:09<02:19,  1.04it/s] 47%|████▋     | 126/270 [03:10<02:18,  1.04it/s] 47%|████▋     | 127/270 [03:11<02:16,  1.05it/s] 47%|████▋     | 128/270 [03:12<02:15,  1.05it/s] 48%|████▊     | 129/270 [03:13<02:14,  1.05it/s] 48%|████▊     | 130/270 [03:14<02:13,  1.05it/s] 49%|████▊     | 131/270 [03:15<02:12,  1.05it/s] 49%|████▉     | 132/270 [03:16<02:10,  1.05it/s] 49%|████▉     | 133/270 [03:17<02:10,  1.05it/s] 50%|████▉     | 134/270 [03:18<02:09,  1.05it/s] 50%|█████     | 135/270 [03:18<01:54,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.24468331038951874, 'eval_f1': 0.9276674780218018, 'eval_runtime': 2.6699, 'eval_samples_per_second': 159.558, 'eval_steps_per_second': 2.622, 'epoch': 4.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  3.76it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.41it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.24it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.16it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.12it/s][A
100%|██████████| 7/7 [00:02<00:00,  3.37it/s][A                                                 
                                             [A 50%|█████     | 135/270 [03:21<01:54,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.37it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 136/270 [03:37<13:58,  6.26s/it] 51%|█████     | 137/270 [03:38<10:19,  4.66s/it] 51%|█████     | 138/270 [03:39<07:47,  3.54s/it] 51%|█████▏    | 139/270 [03:40<06:01,  2.76s/it] 52%|█████▏    | 140/270 [03:41<04:47,  2.22s/it] 52%|█████▏    | 141/270 [03:42<03:56,  1.83s/it] 53%|█████▎    | 142/270 [03:43<03:20,  1.56s/it] 53%|█████▎    | 143/270 [03:44<02:54,  1.38s/it] 53%|█████▎    | 144/270 [03:45<02:36,  1.24s/it] 54%|█████▎    | 145/270 [03:46<02:24,  1.15s/it] 54%|█████▍    | 146/270 [03:47<02:15,  1.09s/it] 54%|█████▍    | 147/270 [03:48<02:08,  1.05s/it] 55%|█████▍    | 148/270 [03:49<02:04,  1.02s/it] 55%|█████▌    | 149/270 [03:49<02:00,  1.00it/s] 56%|█████▌    | 150/270 [03:50<01:57,  1.02it/s]                                                  56%|█████▌    | 150/270 [03:50<01:57,  1.02it/s] 56%|█████▌    | 151/270 [03:51<01:55,  1.03it/s] 56%|█████▋    | 152/270 [03:52<01:53,  1.04it/s] 57%|█████▋    | 153/270 [03:53<01:52,  1.04it/s] 57%|█████▋    | 154/270 [03:54<01:50,  1.05it/s] 57%|█████▋    | 155/270 [03:55<01:49,  1.05it/s] 58%|█████▊    | 156/270 [03:56<01:48,  1.05it/s] 58%|█████▊    | 157/270 [03:57<01:47,  1.05it/s] 59%|█████▊    | 158/270 [03:58<01:46,  1.05it/s] 59%|█████▉    | 159/270 [03:59<01:45,  1.05it/s] 59%|█████▉    | 160/270 [04:00<01:44,  1.05it/s] 60%|█████▉    | 161/270 [04:01<01:43,  1.05it/s] 60%|██████    | 162/270 [04:01<01:31,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.21175691485404968, 'eval_f1': 0.9273381390488231, 'eval_runtime': 2.8721, 'eval_samples_per_second': 148.323, 'eval_steps_per_second': 2.437, 'epoch': 5.0}
{'loss': 0.2189, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.94it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.65it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.23it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.49it/s][A                                                 
                                             [A 60%|██████    | 162/270 [04:04<01:31,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.49it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████    | 163/270 [04:22<12:15,  6.87s/it] 61%|██████    | 164/270 [04:23<08:59,  5.09s/it] 61%|██████    | 165/270 [04:24<06:43,  3.84s/it] 61%|██████▏   | 166/270 [04:25<05:08,  2.97s/it] 62%|██████▏   | 167/270 [04:26<04:02,  2.36s/it] 62%|██████▏   | 168/270 [04:27<03:17,  1.93s/it] 63%|██████▎   | 169/270 [04:28<02:45,  1.64s/it] 63%|██████▎   | 170/270 [04:29<02:22,  1.43s/it] 63%|██████▎   | 171/270 [04:30<02:07,  1.28s/it] 64%|██████▎   | 172/270 [04:31<01:55,  1.18s/it] 64%|██████▍   | 173/270 [04:32<01:47,  1.11s/it] 64%|██████▍   | 174/270 [04:33<01:41,  1.06s/it] 65%|██████▍   | 175/270 [04:34<01:37,  1.02s/it] 65%|██████▌   | 176/270 [04:35<01:33,  1.00it/s] 66%|██████▌   | 177/270 [04:36<01:31,  1.02it/s] 66%|██████▌   | 178/270 [04:36<01:29,  1.03it/s] 66%|██████▋   | 179/270 [04:37<01:27,  1.04it/s] 67%|██████▋   | 180/270 [04:38<01:26,  1.04it/s] 67%|██████▋   | 181/270 [04:39<01:24,  1.05it/s] 67%|██████▋   | 182/270 [04:40<01:23,  1.05it/s] 68%|██████▊   | 183/270 [04:41<01:22,  1.05it/s] 68%|██████▊   | 184/270 [04:42<01:21,  1.05it/s] 69%|██████▊   | 185/270 [04:43<01:20,  1.05it/s] 69%|██████▉   | 186/270 [04:44<01:19,  1.06it/s] 69%|██████▉   | 187/270 [04:45<01:18,  1.06it/s] 70%|██████▉   | 188/270 [04:46<01:17,  1.05it/s] 70%|███████   | 189/270 [04:47<01:08,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.17590105533599854, 'eval_f1': 0.9438264663062095, 'eval_runtime': 2.6797, 'eval_samples_per_second': 158.975, 'eval_steps_per_second': 2.612, 'epoch': 6.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.99it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.24it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.42it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.26it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.51it/s][A                                                 
                                             [A 70%|███████   | 189/270 [04:49<01:08,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|███████   | 190/270 [05:04<07:51,  5.89s/it] 71%|███████   | 191/270 [05:05<05:48,  4.41s/it] 71%|███████   | 192/270 [05:06<04:23,  3.37s/it] 71%|███████▏  | 193/270 [05:07<03:23,  2.65s/it] 72%|███████▏  | 194/270 [05:08<02:42,  2.13s/it] 72%|███████▏  | 195/270 [05:09<02:13,  1.78s/it] 73%|███████▎  | 196/270 [05:10<01:52,  1.52s/it] 73%|███████▎  | 197/270 [05:11<01:38,  1.35s/it] 73%|███████▎  | 198/270 [05:12<01:28,  1.23s/it] 74%|███████▎  | 199/270 [05:13<01:21,  1.15s/it] 74%|███████▍  | 200/270 [05:14<01:16,  1.09s/it]                                                  74%|███████▍  | 200/270 [05:14<01:16,  1.09s/it] 74%|███████▍  | 201/270 [05:15<01:12,  1.05s/it] 75%|███████▍  | 202/270 [05:16<01:09,  1.02s/it] 75%|███████▌  | 203/270 [05:17<01:06,  1.01it/s] 76%|███████▌  | 204/270 [05:17<01:04,  1.02it/s] 76%|███████▌  | 205/270 [05:18<01:03,  1.03it/s] 76%|███████▋  | 206/270 [05:19<01:01,  1.04it/s] 77%|███████▋  | 207/270 [05:20<01:00,  1.05it/s] 77%|███████▋  | 208/270 [05:21<00:59,  1.05it/s] 77%|███████▋  | 209/270 [05:22<00:58,  1.05it/s] 78%|███████▊  | 210/270 [05:23<00:57,  1.05it/s] 78%|███████▊  | 211/270 [05:24<00:56,  1.05it/s] 79%|███████▊  | 212/270 [05:25<00:55,  1.04it/s] 79%|███████▉  | 213/270 [05:26<00:54,  1.04it/s] 79%|███████▉  | 214/270 [05:27<00:53,  1.04it/s] 80%|███████▉  | 215/270 [05:28<00:52,  1.04it/s] 80%|████████  | 216/270 [05:29<00:46,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.1717757135629654, 'eval_f1': 0.9438264663062095, 'eval_runtime': 2.6652, 'eval_samples_per_second': 159.836, 'eval_steps_per_second': 2.626, 'epoch': 7.0}
{'loss': 0.1545, 'learning_rate': 1.5687918106563326e-06, 'epoch': 7.41}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.92it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.62it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.37it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.23it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.50it/s][A                                                 
                                             [A 80%|████████  | 216/270 [05:31<00:46,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.50it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-216
Configuration saved in training-output/checkpoint-216/config.json
Model weights saved in training-output/checkpoint-216/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-216/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-216/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████  | 217/270 [05:47<05:21,  6.07s/it] 81%|████████  | 218/270 [05:48<03:55,  4.53s/it] 81%|████████  | 219/270 [05:49<02:56,  3.45s/it] 81%|████████▏ | 220/270 [05:50<02:14,  2.70s/it] 82%|████████▏ | 221/270 [05:51<01:46,  2.17s/it] 82%|████████▏ | 222/270 [05:51<01:26,  1.80s/it] 83%|████████▎ | 223/270 [05:52<01:12,  1.54s/it] 83%|████████▎ | 224/270 [05:53<01:02,  1.36s/it] 83%|████████▎ | 225/270 [05:54<00:55,  1.24s/it] 84%|████████▎ | 226/270 [05:55<00:50,  1.15s/it] 84%|████████▍ | 227/270 [05:56<00:46,  1.08s/it] 84%|████████▍ | 228/270 [05:57<00:43,  1.04s/it] 85%|████████▍ | 229/270 [05:58<00:41,  1.01s/it] 85%|████████▌ | 230/270 [05:59<00:39,  1.01it/s] 86%|████████▌ | 231/270 [06:00<00:38,  1.02it/s] 86%|████████▌ | 232/270 [06:01<00:36,  1.03it/s] 86%|████████▋ | 233/270 [06:02<00:35,  1.04it/s] 87%|████████▋ | 234/270 [06:03<00:36,  1.01s/it] 87%|████████▋ | 235/270 [06:04<00:34,  1.01it/s] 87%|████████▋ | 236/270 [06:05<00:33,  1.02it/s] 88%|████████▊ | 237/270 [06:06<00:32,  1.03it/s] 88%|████████▊ | 238/270 [06:07<00:30,  1.04it/s] 89%|████████▊ | 239/270 [06:08<00:29,  1.04it/s] 89%|████████▉ | 240/270 [06:09<00:28,  1.05it/s] 89%|████████▉ | 241/270 [06:10<00:27,  1.05it/s] 90%|████████▉ | 242/270 [06:11<00:26,  1.05it/s] 90%|█████████ | 243/270 [06:11<00:22,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.15686199069023132, 'eval_f1': 0.9485729519458129, 'eval_runtime': 2.6984, 'eval_samples_per_second': 157.87, 'eval_steps_per_second': 2.594, 'epoch': 8.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.11it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.65it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.38it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                 
                                             [A 90%|█████████ | 243/270 [06:14<00:22,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-243
Configuration saved in training-output/checkpoint-243/config.json
Model weights saved in training-output/checkpoint-243/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-243/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-243/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|█████████ | 244/270 [06:29<02:36,  6.03s/it] 91%|█████████ | 245/270 [06:30<01:52,  4.50s/it] 91%|█████████ | 246/270 [06:31<01:22,  3.43s/it] 91%|█████████▏| 247/270 [06:32<01:01,  2.69s/it] 92%|█████████▏| 248/270 [06:33<00:47,  2.16s/it] 92%|█████████▏| 249/270 [06:34<00:37,  1.80s/it] 93%|█████████▎| 250/270 [06:35<00:30,  1.54s/it]                                                  93%|█████████▎| 250/270 [06:35<00:30,  1.54s/it] 93%|█████████▎| 251/270 [06:36<00:25,  1.36s/it] 93%|█████████▎| 252/270 [06:37<00:22,  1.24s/it] 94%|█████████▎| 253/270 [06:38<00:19,  1.15s/it] 94%|█████████▍| 254/270 [06:39<00:17,  1.09s/it] 94%|█████████▍| 255/270 [06:40<00:15,  1.04s/it] 95%|█████████▍| 256/270 [06:41<00:14,  1.02s/it] 95%|█████████▌| 257/270 [06:42<00:12,  1.01it/s] 96%|█████████▌| 258/270 [06:43<00:11,  1.02it/s] 96%|█████████▌| 259/270 [06:43<00:10,  1.03it/s] 96%|█████████▋| 260/270 [06:44<00:09,  1.04it/s] 97%|█████████▋| 261/270 [06:45<00:08,  1.04it/s] 97%|█████████▋| 262/270 [06:46<00:07,  1.05it/s] 97%|█████████▋| 263/270 [06:47<00:06,  1.05it/s] 98%|█████████▊| 264/270 [06:48<00:05,  1.05it/s] 98%|█████████▊| 265/270 [06:49<00:04,  1.05it/s] 99%|█████████▊| 266/270 [06:50<00:03,  1.05it/s] 99%|█████████▉| 267/270 [06:51<00:02,  1.05it/s] 99%|█████████▉| 268/270 [06:52<00:01,  1.05it/s]100%|█████████▉| 269/270 [06:53<00:00,  1.05it/s]100%|██████████| 270/270 [06:54<00:00,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.15610221028327942, 'eval_f1': 0.947821433887329, 'eval_runtime': 2.6767, 'eval_samples_per_second': 159.151, 'eval_steps_per_second': 2.615, 'epoch': 9.0}
{'loss': 0.1189, 'learning_rate': 1.3477564710088097e-07, 'epoch': 9.26}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.10it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.25it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.64it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.40it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.24it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.51it/s][A                                                 
                                             [A100%|██████████| 270/270 [06:56<00:00,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-270
Configuration saved in training-output/checkpoint-270/config.json
Model weights saved in training-output/checkpoint-270/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-270/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-270/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-270 (score: 0.9500806605363863).
                                                 100%|██████████| 270/270 [07:11<00:00,  1.18it/s]100%|██████████| 270/270 [07:11<00:00,  1.60s/it]
{'eval_loss': 0.15615861117839813, 'eval_f1': 0.9500806605363863, 'eval_runtime': 2.6614, 'eval_samples_per_second': 160.067, 'eval_steps_per_second': 2.63, 'epoch': 10.0}
{'train_runtime': 431.2725, 'train_samples_per_second': 39.418, 'train_steps_per_second': 0.626, 'train_loss': 0.44189692426610877, 'epoch': 10.0}
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:05, 17.89it/s]  5%|▍         | 5/107 [00:00<00:04, 21.09it/s]  7%|▋         | 8/107 [00:00<00:04, 23.27it/s] 10%|█         | 11/107 [00:00<00:03, 25.02it/s] 13%|█▎        | 14/107 [00:00<00:03, 26.08it/s] 16%|█▌        | 17/107 [00:00<00:03, 26.74it/s] 19%|█▊        | 20/107 [00:00<00:03, 27.09it/s] 21%|██▏       | 23/107 [00:00<00:03, 27.42it/s] 24%|██▍       | 26/107 [00:00<00:02, 27.59it/s] 27%|██▋       | 29/107 [00:01<00:02, 27.74it/s] 30%|██▉       | 32/107 [00:01<00:02, 27.51it/s] 33%|███▎      | 35/107 [00:01<00:02, 27.35it/s] 36%|███▌      | 38/107 [00:01<00:02, 27.41it/s] 38%|███▊      | 41/107 [00:01<00:02, 27.60it/s] 41%|████      | 44/107 [00:01<00:02, 27.53it/s] 44%|████▍     | 47/107 [00:01<00:02, 27.70it/s] 47%|████▋     | 50/107 [00:01<00:02, 27.81it/s] 50%|████▉     | 53/107 [00:01<00:01, 27.85it/s] 52%|█████▏    | 56/107 [00:02<00:01, 27.81it/s] 55%|█████▌    | 59/107 [00:02<00:01, 27.74it/s] 58%|█████▊    | 62/107 [00:02<00:01, 27.60it/s] 61%|██████    | 65/107 [00:02<00:01, 27.62it/s] 64%|██████▎   | 68/107 [00:02<00:01, 27.75it/s] 66%|██████▋   | 71/107 [00:02<00:01, 27.75it/s] 69%|██████▉   | 74/107 [00:02<00:01, 27.79it/s] 72%|███████▏  | 77/107 [00:02<00:01, 27.89it/s] 75%|███████▍  | 80/107 [00:02<00:00, 27.92it/s] 78%|███████▊  | 83/107 [00:03<00:00, 27.85it/s] 80%|████████  | 86/107 [00:03<00:00, 27.85it/s] 83%|████████▎ | 89/107 [00:03<00:00, 27.82it/s] 86%|████████▌ | 92/107 [00:03<00:00, 27.80it/s] 89%|████████▉ | 95/107 [00:03<00:00, 27.76it/s] 92%|█████████▏| 98/107 [00:03<00:00, 27.72it/s] 94%|█████████▍| 101/107 [00:03<00:00, 27.81it/s] 97%|█████████▋| 104/107 [00:03<00:00, 27.80it/s]100%|██████████| 107/107 [00:03<00:00, 27.41it/s]
Model finished with accuracy: 0.9530516431924883, macro-f1: 0.9500806605363863
Confusion matrix:
[[ 83   0   2   1   2]
 [  0  77   1   0   0]
 [  0   0  52   2   0]
 [  0   1   6 109   0]
 [  4   1   0   0  85]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14_mx46s0fx52.csv, 58_lvcsk0yy8g.csv, 133_c0kk72b3fm.csv, 132_s5i7ngx7bu.csv, 112_7g35kmtkhr.csv, 101_9l21auewdi.csv, 69_4tlegp20n1.csv, 35_g0ldvn0rjm.csv, 143_w11kkydg7n.csv, 20_1zahygarmq.csv, 53_4t2mju8m3y.csv, 166_5lv4sjg1qv.csv, 105_3acmrslh5t.csv, 198_shm86f7xzz.csv, 155_97dannvd9q.csv, 189_4mh7pjb6hj.csv, 187_kp3lfo9rti.csv, 3_5uxjfwf4c9.csv, 76_f4jqlrtl0d.csv, 152_uhh1hem3ei.csv, 14_apv4u3qebs.csv, 127_q2ifamcjop.csv, 100_5397borrxc.csv, 129_h8c8ny4x7u.csv, 123_g66af1qsqo.csv, 95_6ikda3av09.csv, 42_kr1ailiclk.csv, 162_gzcq0eli5q.csv, 148_nekma0liim.csv, 50_tmt7xio73x.csv, 183_ex67vzlp8e.csv, 64_gjf3ihxyfj.csv, 182_icy94tmqbq.csv, 96_fe55r9bnfd.csv, 37_7swqi7vgnl.csv, 17_qvtzqld86i.csv, 185_9bjzxvbwca.csv, 4_vet6mgh5md.csv, 109_ehl2s3ah2w.csv, 36_4bg4d4bf4q.csv, 165_vcamdtt7pl.csv, 136_dd735psc8g.csv, 24_1y633kulxy.csv, 7_swfaqmk6y6.csv, 46_eg7yytwm57.csv, 12_i0qb55ygnr.csv, 8_v4bqc7mvzy.csv, 120_tfdaxlk9n2.csv, 103_v129kmq3c5.csv, 144_6g4qagf0aw.csv, 72_37k8qe6uf3.csv, 111_4dftcjzv4j.csv, 1_z949c1i541.csv, 178_7rflrddsnn.csv, 6_blz9uaf562.csv, 15_j7tl3bmirm.csv, 83_slgfcmc4xj.csv, 174_vw5a8s9vzj.csv, 9_m11ut0fzvo.csv, 199_hzu91su6jo.csv, 39_3064xz3lgl.csv, 62_6v7alh2xza.csv, 137_eusnnkienm.csv, 118_66k0pevslq.csv, 113_s8v38lnghb.csv, 90_60znzzhvt3.csv, 161_qu7m9mil3a.csv, 32_jitoqenvci.csv, 176_cl4orgir7l.csv, 65_5y9bhpar44.csv, 27_adgb2e3ry7.csv, 33_zybtjc12ef.csv, 140_rpfb54del9.csv, 70_0b98yn7zuy.csv, 158_lhrhfgkazf.csv, 10_ten3l2nuwn.csv, 48_u6mx1k6p64.csv, 29_juqgc9mrcx.csv, 195_nwk7f5mq3h.csv, 119_hdkgwx4tgr.csv, 163_g4ownm07q9.csv, 134_behtf6grhp.csv, 128_y3nqk9k0x4.csv, 93_7s7jo61qgr.csv, 125_onkofvwzgk.csv, 196_h8ebmn0b4i.csv, 186_77g5px55z1.csv, 80_zbkdixjx1u.csv, 154_i7n85wzpa3.csv, 59_tvvx1filo9.csv, 91_6nvvo85k2r.csv, 26_kcs08kmlzv.csv, 146_umxfdzx3sg.csv, 157_7onih683qj.csv, 122_hmocfdllhz.csv, 102_r9vbr9i6qn.csv, 25_zdmhvwm1pk.csv, 16_omnxwedc88.csv, 116_wf72qb1ctj.csv, 87_zzzdc841sd.csv, 168_39vz7oesxk.csv, 24_pa58yc2bsn.csv, 51_km3hwahw1m.csv, 19_sevtqojtws.csv, 21_x8u5ofrlzo.csv, 75_gginvgfabf.csv, 45_4g2awfbbtn.csv, 4_xu9afrhg0o.csv, 25_ceghr7ntgk.csv, 15_jzm9dvtw73.csv, 89_28gvnlaz8j.csv, 57_l8ux51os5l.csv, 153_mwu7z89cr2.csv, 147_qz39am2soc.csv, 67_ip2ys2xhib.csv, 78_tshg4zfas1.csv, 142_d8jaik03id.csv, 5_wa4f0flvdd.csv, 181_wk573tzjde.csv, 193_kjtylxs5x6.csv, 34_egy98kczu6.csv, 85_i7f88e7b7g.csv, 106_oa27xk4eqt.csv, 156_yk739g3286.csv, 11_0na6xsa4cd.csv, 31_jhmyyhmjh5.csv, 149_tvntt1n3zf.csv, 6_qyv4vhzoe.csv, 63_j79b3nmr83.csv, 104_jos9y36cxr.csv, 151_8k3mbw3phz.csv, 60_zry6xyk2v2.csv, 86_kvt6l8gw8d.csv, 131_bivn5or23h.csv, 175_954mvwcwet.csv, 110_zq7vq985bq.csv, 52_wfyl6gdoxn.csv, 92_u8wrf1zhh5.csv, 71_jjxizibivj.csv, 169_2y8d92hhek.csv, 114_fkhid5yyst.csv, 173_op7k51t6eh.csv, 145_1p84lqxs5t.csv, 171_evt2lbu5wz.csv, 2_wrrttduqx0.csv, 107_jfii3uf345.csv, 164_qcoq7yni4y.csv, 160_9xq61i8z5q.csv, 150_hvfxdsp6uq.csv, 16_j1uppkkwpc.csv, 26_2k5thw2ykv.csv, 172_gg34wiyfxn.csv, 27_r6anjd4ie1.csv, 11_7vk8oa1o24.csv, 138_yx53ai19ar.csv, 44_zsqcy1dp0n.csv, 68_4fxf0cl1sa.csv, 77_gup8yvrjg8.csv, 23_lce4ue1yit.csv, 197_ni25hikvxf.csv, 55_o97cc78tyd.csv, 13_xb7ph2k0m6.csv, 126_s6vu9wsf2o.csv, 124_4p9vzltfyn.csv, 135_7g73f18y8h.csv, 194_tnwwh3zbnj.csv, 99_7fcky2iv54.csv, 49_8fcz4c6oqy.csv, 84_q4rh6yklie.csv, 28_7svph4yobz.csv, 38_38doqz3unu.csv, 5_lcampqa5d8.csv, 82_3qr5zxdcxe.csv, 180_jm6cn0n08c.csv, 66_8gquz8hbpz.csv, 40_zh8fjx6tiy.csv, 141_tvldgnl6pt.csv, 30_rb578trmsj.csv, 184_hzlbex83q3.csv, 56_lxflttkcgn.csv, 22_udsmb087i5.csv, 8_hfxm8dkunn.csv, 61_l3jkp84irv.csv, 139_1qp7vpj70o.csv, 73_4s2wt6mgps.csv, 7_viucp080gt.csv, 108_qkxdz8djgt.csv, 54_07djfakfxh.csv, 98_8et5hi6nqf.csv, 74_08dh1ae49m.csv, 21_x90g7ie9t7.csv, 81_dbvr7ry56o.csv, 121_aobrexg5sv.csv, 191_yxm7fc16uc.csv, 94_gcso78g1mh.csv, 167_bk26q01ex6.csv, 41_rrwls6y9fl.csv, 170_szpuak9x3b.csv, 47_qrdef2nnjn.csv, 13_y1i6zh8y9w.csv, 188_5sjzeb4w03.csv, 130_7a6m1my07s.csv, 192_isa0drpmu4.csv, 79_wkvkhcnsn0.csv, 2_9actgs9mi7.csv, 18_o65lxoj85y.csv, 3_j0hn3a0xzt.csv, 43_1nemhlm74b.csv, 20_z3zjd5sgm4.csv, 117_mgq0ymf5hc.csv, 0_imfn3fsnqb.csv, 88_p169fqr8hl.csv, 1_ofoaso8yxd.csv, 97_ksnb8zg7e2.csv, 159_wmf9y5lb3d.csv, 179_enmmbxkd9b.csv, 18_omkydub7qy.csv, 115_iocn2dpj2o.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|██████████| 2/2 [00:00<00:00, 20.03ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 43.84ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:04<19:25,  4.33s/it]  1%|          | 2/270 [00:05<10:23,  2.33s/it]  1%|          | 3/270 [00:06<07:28,  1.68s/it]  1%|▏         | 4/270 [00:07<06:07,  1.38s/it]  2%|▏         | 5/270 [00:08<05:22,  1.22s/it]  2%|▏         | 6/270 [00:08<04:53,  1.11s/it]  3%|▎         | 7/270 [00:09<04:36,  1.05s/it]  3%|▎         | 8/270 [00:10<04:24,  1.01s/it]  3%|▎         | 9/270 [00:11<04:17,  1.01it/s]  4%|▎         | 10/270 [00:12<04:11,  1.03it/s]  4%|▍         | 11/270 [00:13<04:07,  1.05it/s]  4%|▍         | 12/270 [00:14<04:03,  1.06it/s]  5%|▍         | 13/270 [00:15<04:01,  1.06it/s]  5%|▌         | 14/270 [00:16<03:59,  1.07it/s]  6%|▌         | 15/270 [00:17<04:00,  1.06it/s]  6%|▌         | 16/270 [00:18<03:58,  1.06it/s]  6%|▋         | 17/270 [00:19<03:56,  1.07it/s]  7%|▋         | 18/270 [00:20<03:55,  1.07it/s]  7%|▋         | 19/270 [00:21<03:54,  1.07it/s]  7%|▋         | 20/270 [00:21<03:53,  1.07it/s]  8%|▊         | 21/270 [00:22<03:51,  1.07it/s]  8%|▊         | 22/270 [00:23<03:50,  1.07it/s]  9%|▊         | 23/270 [00:24<03:49,  1.07it/s]  9%|▉         | 24/270 [00:25<03:49,  1.07it/s]  9%|▉         | 25/270 [00:26<03:48,  1.07it/s] 10%|▉         | 26/270 [00:27<03:45,  1.08it/s] 10%|█         | 27/270 [00:28<03:21,  1.20it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.01it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.31it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.45it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.30it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                
                                             [A 10%|█         | 27/270 [00:30<03:21,  1.20it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|█         | 28/270 [00:45<23:38,  5.86s/it] 11%|█         | 29/270 [00:46<17:50,  4.44s/it] 11%|█         | 30/270 [00:47<13:33,  3.39s/it] 11%|█▏        | 31/270 [00:48<10:33,  2.65s/it] 12%|█▏        | 32/270 [00:49<08:27,  2.13s/it] 12%|█▏        | 33/270 [00:50<07:00,  1.77s/it] 13%|█▎        | 34/270 [00:51<05:59,  1.52s/it] 13%|█▎        | 35/270 [00:52<05:17,  1.35s/it] 13%|█▎        | 36/270 [00:53<04:47,  1.23s/it] 14%|█▎        | 37/270 [00:54<04:25,  1.14s/it] 14%|█▍        | 38/270 [00:55<04:09,  1.08s/it] 14%|█▍        | 39/270 [00:56<03:59,  1.04s/it] 15%|█▍        | 40/270 [00:57<03:52,  1.01s/it] 15%|█▌        | 41/270 [00:58<03:46,  1.01it/s] 16%|█▌        | 42/270 [00:59<03:41,  1.03it/s] 16%|█▌        | 43/270 [00:59<03:38,  1.04it/s] 16%|█▋        | 44/270 [01:00<03:35,  1.05it/s] 17%|█▋        | 45/270 [01:01<03:33,  1.05it/s] 17%|█▋        | 46/270 [01:02<03:32,  1.06it/s] 17%|█▋        | 47/270 [01:03<03:30,  1.06it/s] 18%|█▊        | 48/270 [01:04<03:29,  1.06it/s] 18%|█▊        | 49/270 [01:05<03:28,  1.06it/s] 19%|█▊        | 50/270 [01:06<03:27,  1.06it/s]                                                 19%|█▊        | 50/270 [01:06<03:27,  1.06it/s] 19%|█▉        | 51/270 [01:07<03:26,  1.06it/s] 19%|█▉        | 52/270 [01:08<03:25,  1.06it/s] 20%|█▉        | 53/270 [01:09<03:24,  1.06it/s] 20%|██        | 54/270 [01:09<03:01,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 1.2813993692398071, 'eval_f1': 0.41217225930639617, 'eval_runtime': 2.8536, 'eval_samples_per_second': 149.285, 'eval_steps_per_second': 2.453, 'epoch': 1.0}
{'loss': 1.3173, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.15it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.25it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.68it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.44it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.28it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                
                                             [A 20%|██        | 54/270 [01:12<03:01,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██        | 55/270 [01:28<21:44,  6.07s/it] 21%|██        | 56/270 [01:29<16:08,  4.53s/it] 21%|██        | 57/270 [01:30<12:15,  3.45s/it] 21%|██▏       | 58/270 [01:31<09:31,  2.70s/it] 22%|██▏       | 59/270 [01:31<07:37,  2.17s/it] 22%|██▏       | 60/270 [01:32<06:17,  1.80s/it] 23%|██▎       | 61/270 [01:33<05:21,  1.54s/it] 23%|██▎       | 62/270 [01:34<04:42,  1.36s/it] 23%|██▎       | 63/270 [01:35<04:14,  1.23s/it] 24%|██▎       | 64/270 [01:36<03:55,  1.14s/it] 24%|██▍       | 65/270 [01:37<03:41,  1.08s/it] 24%|██▍       | 66/270 [01:38<03:31,  1.04s/it] 25%|██▍       | 67/270 [01:39<03:24,  1.01s/it] 25%|██▌       | 68/270 [01:40<03:19,  1.01it/s] 26%|██▌       | 69/270 [01:41<03:16,  1.02it/s] 26%|██▌       | 70/270 [01:42<03:13,  1.03it/s] 26%|██▋       | 71/270 [01:43<03:11,  1.04it/s] 27%|██▋       | 72/270 [01:44<03:09,  1.05it/s] 27%|██▋       | 73/270 [01:45<03:07,  1.05it/s] 27%|██▋       | 74/270 [01:46<03:06,  1.05it/s] 28%|██▊       | 75/270 [01:47<03:04,  1.05it/s] 28%|██▊       | 76/270 [01:47<03:03,  1.06it/s] 29%|██▊       | 77/270 [01:48<03:02,  1.06it/s] 29%|██▉       | 78/270 [01:49<03:02,  1.05it/s] 29%|██▉       | 79/270 [01:50<03:01,  1.05it/s] 30%|██▉       | 80/270 [01:51<02:59,  1.06it/s] 30%|███       | 81/270 [01:52<02:39,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.6171854734420776, 'eval_f1': 0.8033107376637011, 'eval_runtime': 2.8531, 'eval_samples_per_second': 149.312, 'eval_steps_per_second': 2.453, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.17it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.31it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.44it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.28it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                
                                             [A 30%|███       | 81/270 [01:55<02:39,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|███       | 82/270 [02:09<18:22,  5.86s/it] 31%|███       | 83/270 [02:10<13:39,  4.38s/it] 31%|███       | 84/270 [02:11<10:22,  3.35s/it] 31%|███▏      | 85/270 [02:12<08:05,  2.62s/it] 32%|███▏      | 86/270 [02:13<06:29,  2.12s/it] 32%|███▏      | 87/270 [02:14<05:23,  1.77s/it] 33%|███▎      | 88/270 [02:15<04:36,  1.52s/it] 33%|███▎      | 89/270 [02:16<04:03,  1.35s/it] 33%|███▎      | 90/270 [02:17<03:40,  1.22s/it] 34%|███▎      | 91/270 [02:18<03:24,  1.14s/it] 34%|███▍      | 92/270 [02:19<03:12,  1.08s/it] 34%|███▍      | 93/270 [02:20<03:04,  1.04s/it] 35%|███▍      | 94/270 [02:21<02:57,  1.01s/it] 35%|███▌      | 95/270 [02:22<02:53,  1.01it/s] 36%|███▌      | 96/270 [02:23<02:50,  1.02it/s] 36%|███▌      | 97/270 [02:24<02:47,  1.03it/s] 36%|███▋      | 98/270 [02:25<02:45,  1.04it/s] 37%|███▋      | 99/270 [02:25<02:43,  1.04it/s] 37%|███▋      | 100/270 [02:26<02:42,  1.05it/s]                                                  37%|███▋      | 100/270 [02:26<02:42,  1.05it/s] 37%|███▋      | 101/270 [02:27<02:40,  1.05it/s] 38%|███▊      | 102/270 [02:28<02:39,  1.05it/s] 38%|███▊      | 103/270 [02:29<02:38,  1.05it/s] 39%|███▊      | 104/270 [02:30<02:37,  1.05it/s] 39%|███▉      | 105/270 [02:31<02:36,  1.06it/s] 39%|███▉      | 106/270 [02:32<02:35,  1.06it/s] 40%|███▉      | 107/270 [02:33<02:34,  1.06it/s] 40%|████      | 108/270 [02:34<02:16,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.3187181055545807, 'eval_f1': 0.9074239781469933, 'eval_runtime': 2.6501, 'eval_samples_per_second': 160.746, 'eval_steps_per_second': 2.641, 'epoch': 3.0}
{'loss': 0.5007, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.95it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.63it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.52it/s][A                                                 
                                             [A 40%|████      | 108/270 [02:36<02:16,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.52it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████      | 109/270 [02:51<15:50,  5.90s/it] 41%|████      | 110/270 [02:52<11:46,  4.41s/it] 41%|████      | 111/270 [02:53<08:55,  3.37s/it] 41%|████▏     | 112/270 [02:54<06:57,  2.64s/it] 42%|████▏     | 113/270 [02:55<05:33,  2.13s/it] 42%|████▏     | 114/270 [02:56<04:36,  1.77s/it] 43%|████▎     | 115/270 [02:57<03:55,  1.52s/it] 43%|████▎     | 116/270 [02:58<03:27,  1.35s/it] 43%|████▎     | 117/270 [02:59<03:07,  1.22s/it] 44%|████▎     | 118/270 [03:00<02:53,  1.14s/it] 44%|████▍     | 119/270 [03:01<02:43,  1.08s/it] 44%|████▍     | 120/270 [03:02<02:36,  1.04s/it] 45%|████▍     | 121/270 [03:03<02:30,  1.01s/it] 45%|████▌     | 122/270 [03:04<02:26,  1.01it/s] 46%|████▌     | 123/270 [03:04<02:23,  1.02it/s] 46%|████▌     | 124/270 [03:05<02:20,  1.04it/s] 46%|████▋     | 125/270 [03:06<02:19,  1.04it/s] 47%|████▋     | 126/270 [03:07<02:17,  1.05it/s] 47%|████▋     | 127/270 [03:08<02:16,  1.05it/s] 47%|████▋     | 128/270 [03:09<02:15,  1.05it/s] 48%|████▊     | 129/270 [03:10<02:13,  1.05it/s] 48%|████▊     | 130/270 [03:11<02:13,  1.05it/s] 49%|████▊     | 131/270 [03:12<02:10,  1.06it/s] 49%|████▉     | 132/270 [03:13<02:18,  1.00s/it] 49%|████▉     | 133/270 [03:14<02:14,  1.02it/s] 50%|████▉     | 134/270 [03:15<02:12,  1.03it/s] 50%|█████     | 135/270 [03:16<01:57,  1.15it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.22940291464328766, 'eval_f1': 0.9191179049045075, 'eval_runtime': 2.6602, 'eval_samples_per_second': 160.141, 'eval_steps_per_second': 2.631, 'epoch': 4.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.09it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.23it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.68it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.40it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                 
                                             [A 50%|█████     | 135/270 [03:18<01:57,  1.15it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 136/270 [03:34<13:28,  6.03s/it] 51%|█████     | 137/270 [03:35<10:03,  4.54s/it] 51%|█████     | 138/270 [03:36<07:37,  3.46s/it] 51%|█████▏    | 139/270 [03:37<05:54,  2.71s/it] 52%|█████▏    | 140/270 [03:38<04:43,  2.18s/it] 52%|█████▏    | 141/270 [03:39<03:52,  1.80s/it] 53%|█████▎    | 142/270 [03:40<03:17,  1.54s/it] 53%|█████▎    | 143/270 [03:40<02:53,  1.36s/it] 53%|█████▎    | 144/270 [03:41<02:36,  1.24s/it] 54%|█████▎    | 145/270 [03:42<02:24,  1.15s/it] 54%|█████▍    | 146/270 [03:43<02:15,  1.09s/it] 54%|█████▍    | 147/270 [03:44<02:09,  1.05s/it] 55%|█████▍    | 148/270 [03:45<02:04,  1.02s/it] 55%|█████▌    | 149/270 [03:46<02:00,  1.00it/s] 56%|█████▌    | 150/270 [03:47<01:57,  1.02it/s]                                                  56%|█████▌    | 150/270 [03:47<01:57,  1.02it/s] 56%|█████▌    | 151/270 [03:48<01:56,  1.02it/s] 56%|█████▋    | 152/270 [03:49<01:54,  1.03it/s] 57%|█████▋    | 153/270 [03:50<01:53,  1.03it/s] 57%|█████▋    | 154/270 [03:51<01:51,  1.04it/s] 57%|█████▋    | 155/270 [03:52<01:50,  1.04it/s] 58%|█████▊    | 156/270 [03:53<01:49,  1.04it/s] 58%|█████▊    | 157/270 [03:54<01:48,  1.04it/s] 59%|█████▊    | 158/270 [03:55<01:47,  1.05it/s] 59%|█████▉    | 159/270 [03:56<01:45,  1.05it/s] 59%|█████▉    | 160/270 [03:57<01:44,  1.05it/s] 60%|█████▉    | 161/270 [03:58<01:43,  1.05it/s] 60%|██████    | 162/270 [03:58<01:31,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.18471232056617737, 'eval_f1': 0.9334155587090786, 'eval_runtime': 2.6649, 'eval_samples_per_second': 159.853, 'eval_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 0.244, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.87it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.66it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.38it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.51it/s][A                                                 
                                             [A 60%|██████    | 162/270 [04:01<01:31,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████    | 163/270 [04:16<10:27,  5.86s/it] 61%|██████    | 164/270 [04:17<07:44,  4.38s/it] 61%|██████    | 165/270 [04:18<05:51,  3.35s/it] 61%|██████▏   | 166/270 [04:19<04:33,  2.63s/it] 62%|██████▏   | 167/270 [04:20<03:38,  2.12s/it] 62%|██████▏   | 168/270 [04:21<03:00,  1.77s/it] 63%|██████▎   | 169/270 [04:21<02:33,  1.52s/it] 63%|██████▎   | 170/270 [04:22<02:14,  1.35s/it] 63%|██████▎   | 171/270 [04:23<02:01,  1.22s/it] 64%|██████▎   | 172/270 [04:24<01:51,  1.14s/it] 64%|██████▍   | 173/270 [04:25<01:44,  1.08s/it] 64%|██████▍   | 174/270 [04:26<01:39,  1.04s/it] 65%|██████▍   | 175/270 [04:27<01:35,  1.01s/it] 65%|██████▌   | 176/270 [04:28<01:33,  1.01it/s] 66%|██████▌   | 177/270 [04:29<01:31,  1.02it/s] 66%|██████▌   | 178/270 [04:30<01:29,  1.03it/s] 66%|██████▋   | 179/270 [04:31<01:27,  1.04it/s] 67%|██████▋   | 180/270 [04:32<01:26,  1.04it/s] 67%|██████▋   | 181/270 [04:33<01:25,  1.05it/s] 67%|██████▋   | 182/270 [04:34<01:23,  1.05it/s] 68%|██████▊   | 183/270 [04:35<01:22,  1.05it/s] 68%|██████▊   | 184/270 [04:36<01:21,  1.05it/s] 69%|██████▊   | 185/270 [04:37<01:20,  1.05it/s] 69%|██████▉   | 186/270 [04:38<01:20,  1.05it/s] 69%|██████▉   | 187/270 [04:38<01:18,  1.05it/s] 70%|██████▉   | 188/270 [04:39<01:17,  1.05it/s] 70%|███████   | 189/270 [04:40<01:08,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.16980163753032684, 'eval_f1': 0.9397963276901933, 'eval_runtime': 2.6872, 'eval_samples_per_second': 158.53, 'eval_steps_per_second': 2.605, 'epoch': 6.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.09it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.23it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.66it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.41it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.24it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                 
                                             [A 70%|███████   | 189/270 [04:43<01:08,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|███████   | 190/270 [04:58<07:50,  5.89s/it] 71%|███████   | 191/270 [04:59<05:47,  4.40s/it] 71%|███████   | 192/270 [05:00<04:22,  3.36s/it] 71%|███████▏  | 193/270 [05:01<03:22,  2.64s/it] 72%|███████▏  | 194/270 [05:01<02:41,  2.13s/it] 72%|███████▏  | 195/270 [05:02<02:13,  1.77s/it] 73%|███████▎  | 196/270 [05:03<01:52,  1.52s/it] 73%|███████▎  | 197/270 [05:04<01:38,  1.35s/it] 73%|███████▎  | 198/270 [05:05<01:28,  1.23s/it] 74%|███████▎  | 199/270 [05:06<01:21,  1.14s/it] 74%|███████▍  | 200/270 [05:07<01:15,  1.09s/it]                                                  74%|███████▍  | 200/270 [05:07<01:15,  1.09s/it] 74%|███████▍  | 201/270 [05:08<01:12,  1.04s/it] 75%|███████▍  | 202/270 [05:09<01:09,  1.02s/it] 75%|███████▌  | 203/270 [05:10<01:06,  1.01it/s] 76%|███████▌  | 204/270 [05:11<01:04,  1.02it/s] 76%|███████▌  | 205/270 [05:12<01:03,  1.03it/s] 76%|███████▋  | 206/270 [05:13<01:01,  1.03it/s] 77%|███████▋  | 207/270 [05:14<01:00,  1.04it/s] 77%|███████▋  | 208/270 [05:15<00:59,  1.04it/s] 77%|███████▋  | 209/270 [05:16<00:58,  1.04it/s] 78%|███████▊  | 210/270 [05:17<00:57,  1.05it/s] 78%|███████▊  | 211/270 [05:18<00:56,  1.05it/s] 79%|███████▊  | 212/270 [05:19<00:55,  1.05it/s] 79%|███████▉  | 213/270 [05:19<00:54,  1.05it/s] 79%|███████▉  | 214/270 [05:20<00:53,  1.05it/s] 80%|███████▉  | 215/270 [05:21<00:52,  1.05it/s] 80%|████████  | 216/270 [05:22<00:45,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.1724846363067627, 'eval_f1': 0.9448729617598814, 'eval_runtime': 2.6892, 'eval_samples_per_second': 158.409, 'eval_steps_per_second': 2.603, 'epoch': 7.0}
{'loss': 0.1592, 'learning_rate': 1.5687918106563326e-06, 'epoch': 7.41}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.92it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.63it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.44it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.51it/s][A                                                 
                                             [A 80%|████████  | 216/270 [05:25<00:45,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-216
Configuration saved in training-output/checkpoint-216/config.json
Model weights saved in training-output/checkpoint-216/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-216/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-216/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████  | 217/270 [05:40<05:12,  5.90s/it] 81%|████████  | 218/270 [05:41<03:49,  4.41s/it] 81%|████████  | 219/270 [05:42<02:51,  3.37s/it] 81%|████████▏ | 220/270 [05:42<02:12,  2.64s/it] 82%|████████▏ | 221/270 [05:43<01:44,  2.13s/it] 82%|████████▏ | 222/270 [05:44<01:25,  1.78s/it] 83%|████████▎ | 223/270 [05:45<01:11,  1.53s/it] 83%|████████▎ | 224/270 [05:46<01:02,  1.35s/it] 83%|████████▎ | 225/270 [05:47<00:55,  1.23s/it] 84%|████████▎ | 226/270 [05:48<00:50,  1.14s/it] 84%|████████▍ | 227/270 [05:49<00:46,  1.08s/it] 84%|████████▍ | 228/270 [05:50<00:43,  1.04s/it] 85%|████████▍ | 229/270 [05:51<00:41,  1.01s/it] 85%|████████▌ | 230/270 [05:52<00:39,  1.01it/s] 86%|████████▌ | 231/270 [05:53<00:38,  1.02it/s] 86%|████████▌ | 232/270 [05:54<00:36,  1.03it/s] 86%|████████▋ | 233/270 [05:55<00:35,  1.04it/s] 87%|████████▋ | 234/270 [05:56<00:34,  1.04it/s] 87%|████████▋ | 235/270 [05:57<00:33,  1.04it/s] 87%|████████▋ | 236/270 [05:58<00:32,  1.05it/s] 88%|████████▊ | 237/270 [05:59<00:31,  1.05it/s] 88%|████████▊ | 238/270 [06:00<00:30,  1.05it/s] 89%|████████▊ | 239/270 [06:01<00:29,  1.05it/s] 89%|████████▉ | 240/270 [06:01<00:28,  1.05it/s] 89%|████████▉ | 241/270 [06:02<00:27,  1.05it/s] 90%|████████▉ | 242/270 [06:03<00:26,  1.05it/s] 90%|█████████ | 243/270 [06:04<00:22,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.16001160442829132, 'eval_f1': 0.9397448760454248, 'eval_runtime': 2.6706, 'eval_samples_per_second': 159.516, 'eval_steps_per_second': 2.621, 'epoch': 8.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.13it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.29it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.93it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.96it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  2.97it/s][A
100%|██████████| 7/7 [00:02<00:00,  3.30it/s][A                                                 
                                             [A 90%|█████████ | 243/270 [06:07<00:22,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.30it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-243
Configuration saved in training-output/checkpoint-243/config.json
Model weights saved in training-output/checkpoint-243/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-243/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-243/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|█████████ | 244/270 [06:25<02:57,  6.82s/it] 91%|█████████ | 245/270 [06:26<02:06,  5.05s/it] 91%|█████████ | 246/270 [06:27<01:31,  3.82s/it] 91%|█████████▏| 247/270 [06:28<01:07,  2.96s/it] 92%|█████████▏| 248/270 [06:28<00:51,  2.35s/it] 92%|█████████▏| 249/270 [06:29<00:40,  1.93s/it] 93%|█████████▎| 250/270 [06:30<00:32,  1.64s/it]                                                  93%|█████████▎| 250/270 [06:30<00:32,  1.64s/it] 93%|█████████▎| 251/270 [06:31<00:27,  1.43s/it] 93%|█████████▎| 252/270 [06:32<00:23,  1.28s/it] 94%|█████████▎| 253/270 [06:33<00:20,  1.18s/it] 94%|█████████▍| 254/270 [06:34<00:17,  1.11s/it] 94%|█████████▍| 255/270 [06:35<00:15,  1.06s/it] 95%|█████████▍| 256/270 [06:36<00:14,  1.03s/it] 95%|█████████▌| 257/270 [06:37<00:13,  1.01s/it] 96%|█████████▌| 258/270 [06:38<00:11,  1.01it/s] 96%|█████████▌| 259/270 [06:39<00:10,  1.02it/s] 96%|█████████▋| 260/270 [06:40<00:09,  1.03it/s] 97%|█████████▋| 261/270 [06:41<00:08,  1.04it/s] 97%|█████████▋| 262/270 [06:42<00:07,  1.04it/s] 97%|█████████▋| 263/270 [06:43<00:06,  1.05it/s] 98%|█████████▊| 264/270 [06:44<00:05,  1.05it/s] 98%|█████████▊| 265/270 [06:45<00:04,  1.05it/s] 99%|█████████▊| 266/270 [06:46<00:03,  1.05it/s] 99%|█████████▉| 267/270 [06:47<00:02,  1.05it/s] 99%|█████████▉| 268/270 [06:47<00:01,  1.05it/s]100%|█████████▉| 269/270 [06:48<00:00,  1.05it/s]100%|██████████| 270/270 [06:49<00:00,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.16554872691631317, 'eval_f1': 0.9429484308825133, 'eval_runtime': 2.98, 'eval_samples_per_second': 142.953, 'eval_steps_per_second': 2.349, 'epoch': 9.0}
{'loss': 0.1294, 'learning_rate': 1.3477564710088097e-07, 'epoch': 9.26}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.06it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.17it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.60it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.35it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.22it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.49it/s][A                                                 
                                             [A100%|██████████| 270/270 [06:52<00:00,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.49it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-270
Configuration saved in training-output/checkpoint-270/config.json
Model weights saved in training-output/checkpoint-270/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-270/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-270/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-189 (score: 0.9448729617598814).
                                                 100%|██████████| 270/270 [07:06<00:00,  1.17it/s]100%|██████████| 270/270 [07:06<00:00,  1.58s/it]
{'eval_loss': 0.16331598162651062, 'eval_f1': 0.9402613488525802, 'eval_runtime': 2.67, 'eval_samples_per_second': 159.552, 'eval_steps_per_second': 2.622, 'epoch': 10.0}
{'train_runtime': 426.7622, 'train_samples_per_second': 39.835, 'train_steps_per_second': 0.633, 'train_loss': 0.4451165517171224, 'epoch': 10.0}
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:06, 16.65it/s]  5%|▍         | 5/107 [00:00<00:04, 21.21it/s]  7%|▋         | 8/107 [00:00<00:04, 23.16it/s] 10%|█         | 11/107 [00:00<00:03, 24.98it/s] 13%|█▎        | 14/107 [00:00<00:03, 26.06it/s] 16%|█▌        | 17/107 [00:00<00:03, 26.85it/s] 19%|█▊        | 20/107 [00:00<00:03, 27.23it/s] 21%|██▏       | 23/107 [00:00<00:03, 27.54it/s] 24%|██▍       | 26/107 [00:01<00:02, 27.63it/s] 27%|██▋       | 29/107 [00:01<00:02, 27.79it/s] 30%|██▉       | 32/107 [00:01<00:02, 27.64it/s] 33%|███▎      | 35/107 [00:01<00:02, 27.47it/s] 36%|███▌      | 38/107 [00:01<00:02, 27.56it/s] 38%|███▊      | 41/107 [00:01<00:02, 27.60it/s] 41%|████      | 44/107 [00:01<00:02, 27.74it/s] 44%|████▍     | 47/107 [00:01<00:02, 27.83it/s] 47%|████▋     | 50/107 [00:01<00:02, 27.87it/s] 50%|████▉     | 53/107 [00:01<00:01, 27.92it/s] 52%|█████▏    | 56/107 [00:02<00:01, 27.90it/s] 55%|█████▌    | 59/107 [00:02<00:01, 27.93it/s] 58%|█████▊    | 62/107 [00:02<00:01, 27.62it/s] 61%|██████    | 65/107 [00:02<00:01, 27.67it/s] 64%|██████▎   | 68/107 [00:02<00:01, 27.63it/s] 66%|██████▋   | 71/107 [00:02<00:01, 27.15it/s] 69%|██████▉   | 74/107 [00:02<00:01, 27.43it/s] 72%|███████▏  | 77/107 [00:02<00:01, 27.66it/s] 75%|███████▍  | 80/107 [00:02<00:00, 27.75it/s] 78%|███████▊  | 83/107 [00:03<00:00, 27.81it/s] 80%|████████  | 86/107 [00:03<00:00, 27.92it/s] 83%|████████▎ | 89/107 [00:03<00:00, 27.76it/s] 86%|████████▌ | 92/107 [00:03<00:00, 27.74it/s] 89%|████████▉ | 95/107 [00:03<00:00, 27.61it/s] 92%|█████████▏| 98/107 [00:03<00:00, 27.57it/s] 94%|█████████▍| 101/107 [00:03<00:00, 27.74it/s] 97%|█████████▋| 104/107 [00:03<00:00, 27.73it/s]100%|██████████| 107/107 [00:03<00:00, 23.65it/s]100%|██████████| 107/107 [00:03<00:00, 26.83it/s]
Model finished with accuracy: 0.9460093896713615, macro-f1: 0.9448729617598814
Confusion matrix:
[[88  1  0  0  6]
 [ 0 78  0  0  1]
 [ 1  0 57  1  0]
 [ 0  1  8 97  0]
 [ 3  1  0  0 83]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14_mx46s0fx52.csv, 58_lvcsk0yy8g.csv, 133_c0kk72b3fm.csv, 132_s5i7ngx7bu.csv, 112_7g35kmtkhr.csv, 101_9l21auewdi.csv, 69_4tlegp20n1.csv, 35_g0ldvn0rjm.csv, 143_w11kkydg7n.csv, 20_1zahygarmq.csv, 53_4t2mju8m3y.csv, 166_5lv4sjg1qv.csv, 105_3acmrslh5t.csv, 198_shm86f7xzz.csv, 155_97dannvd9q.csv, 189_4mh7pjb6hj.csv, 187_kp3lfo9rti.csv, 3_5uxjfwf4c9.csv, 76_f4jqlrtl0d.csv, 152_uhh1hem3ei.csv, 14_apv4u3qebs.csv, 127_q2ifamcjop.csv, 100_5397borrxc.csv, 129_h8c8ny4x7u.csv, 123_g66af1qsqo.csv, 95_6ikda3av09.csv, 42_kr1ailiclk.csv, 162_gzcq0eli5q.csv, 148_nekma0liim.csv, 50_tmt7xio73x.csv, 183_ex67vzlp8e.csv, 64_gjf3ihxyfj.csv, 182_icy94tmqbq.csv, 96_fe55r9bnfd.csv, 37_7swqi7vgnl.csv, 17_qvtzqld86i.csv, 185_9bjzxvbwca.csv, 4_vet6mgh5md.csv, 109_ehl2s3ah2w.csv, 36_4bg4d4bf4q.csv, 165_vcamdtt7pl.csv, 136_dd735psc8g.csv, 24_1y633kulxy.csv, 7_swfaqmk6y6.csv, 46_eg7yytwm57.csv, 12_i0qb55ygnr.csv, 8_v4bqc7mvzy.csv, 120_tfdaxlk9n2.csv, 103_v129kmq3c5.csv, 144_6g4qagf0aw.csv, 72_37k8qe6uf3.csv, 111_4dftcjzv4j.csv, 1_z949c1i541.csv, 178_7rflrddsnn.csv, 6_blz9uaf562.csv, 15_j7tl3bmirm.csv, 83_slgfcmc4xj.csv, 174_vw5a8s9vzj.csv, 9_m11ut0fzvo.csv, 199_hzu91su6jo.csv, 39_3064xz3lgl.csv, 62_6v7alh2xza.csv, 137_eusnnkienm.csv, 118_66k0pevslq.csv, 113_s8v38lnghb.csv, 90_60znzzhvt3.csv, 161_qu7m9mil3a.csv, 32_jitoqenvci.csv, 176_cl4orgir7l.csv, 65_5y9bhpar44.csv, 27_adgb2e3ry7.csv, 33_zybtjc12ef.csv, 140_rpfb54del9.csv, 70_0b98yn7zuy.csv, 158_lhrhfgkazf.csv, 10_ten3l2nuwn.csv, 48_u6mx1k6p64.csv, 29_juqgc9mrcx.csv, 195_nwk7f5mq3h.csv, 119_hdkgwx4tgr.csv, 163_g4ownm07q9.csv, 134_behtf6grhp.csv, 128_y3nqk9k0x4.csv, 93_7s7jo61qgr.csv, 125_onkofvwzgk.csv, 196_h8ebmn0b4i.csv, 186_77g5px55z1.csv, 80_zbkdixjx1u.csv, 154_i7n85wzpa3.csv, 59_tvvx1filo9.csv, 91_6nvvo85k2r.csv, 26_kcs08kmlzv.csv, 146_umxfdzx3sg.csv, 157_7onih683qj.csv, 122_hmocfdllhz.csv, 102_r9vbr9i6qn.csv, 25_zdmhvwm1pk.csv, 16_omnxwedc88.csv, 116_wf72qb1ctj.csv, 87_zzzdc841sd.csv, 168_39vz7oesxk.csv, 24_pa58yc2bsn.csv, 51_km3hwahw1m.csv, 19_sevtqojtws.csv, 21_x8u5ofrlzo.csv, 75_gginvgfabf.csv, 45_4g2awfbbtn.csv, 4_xu9afrhg0o.csv, 25_ceghr7ntgk.csv, 15_jzm9dvtw73.csv, 89_28gvnlaz8j.csv, 57_l8ux51os5l.csv, 153_mwu7z89cr2.csv, 147_qz39am2soc.csv, 67_ip2ys2xhib.csv, 78_tshg4zfas1.csv, 142_d8jaik03id.csv, 5_wa4f0flvdd.csv, 181_wk573tzjde.csv, 193_kjtylxs5x6.csv, 34_egy98kczu6.csv, 85_i7f88e7b7g.csv, 106_oa27xk4eqt.csv, 156_yk739g3286.csv, 11_0na6xsa4cd.csv, 31_jhmyyhmjh5.csv, 149_tvntt1n3zf.csv, 6_qyv4vhzoe.csv, 63_j79b3nmr83.csv, 104_jos9y36cxr.csv, 151_8k3mbw3phz.csv, 60_zry6xyk2v2.csv, 86_kvt6l8gw8d.csv, 131_bivn5or23h.csv, 175_954mvwcwet.csv, 110_zq7vq985bq.csv, 52_wfyl6gdoxn.csv, 92_u8wrf1zhh5.csv, 71_jjxizibivj.csv, 169_2y8d92hhek.csv, 114_fkhid5yyst.csv, 173_op7k51t6eh.csv, 145_1p84lqxs5t.csv, 171_evt2lbu5wz.csv, 2_wrrttduqx0.csv, 107_jfii3uf345.csv, 164_qcoq7yni4y.csv, 160_9xq61i8z5q.csv, 150_hvfxdsp6uq.csv, 16_j1uppkkwpc.csv, 26_2k5thw2ykv.csv, 172_gg34wiyfxn.csv, 27_r6anjd4ie1.csv, 11_7vk8oa1o24.csv, 138_yx53ai19ar.csv, 44_zsqcy1dp0n.csv, 68_4fxf0cl1sa.csv, 77_gup8yvrjg8.csv, 23_lce4ue1yit.csv, 197_ni25hikvxf.csv, 55_o97cc78tyd.csv, 13_xb7ph2k0m6.csv, 126_s6vu9wsf2o.csv, 124_4p9vzltfyn.csv, 135_7g73f18y8h.csv, 194_tnwwh3zbnj.csv, 99_7fcky2iv54.csv, 49_8fcz4c6oqy.csv, 84_q4rh6yklie.csv, 28_7svph4yobz.csv, 38_38doqz3unu.csv, 5_lcampqa5d8.csv, 82_3qr5zxdcxe.csv, 180_jm6cn0n08c.csv, 66_8gquz8hbpz.csv, 40_zh8fjx6tiy.csv, 141_tvldgnl6pt.csv, 30_rb578trmsj.csv, 184_hzlbex83q3.csv, 56_lxflttkcgn.csv, 22_udsmb087i5.csv, 8_hfxm8dkunn.csv, 61_l3jkp84irv.csv, 139_1qp7vpj70o.csv, 73_4s2wt6mgps.csv, 7_viucp080gt.csv, 108_qkxdz8djgt.csv, 54_07djfakfxh.csv, 98_8et5hi6nqf.csv, 74_08dh1ae49m.csv, 21_x90g7ie9t7.csv, 81_dbvr7ry56o.csv, 121_aobrexg5sv.csv, 191_yxm7fc16uc.csv, 94_gcso78g1mh.csv, 167_bk26q01ex6.csv, 41_rrwls6y9fl.csv, 170_szpuak9x3b.csv, 47_qrdef2nnjn.csv, 13_y1i6zh8y9w.csv, 188_5sjzeb4w03.csv, 130_7a6m1my07s.csv, 192_isa0drpmu4.csv, 79_wkvkhcnsn0.csv, 2_9actgs9mi7.csv, 18_o65lxoj85y.csv, 3_j0hn3a0xzt.csv, 43_1nemhlm74b.csv, 20_z3zjd5sgm4.csv, 117_mgq0ymf5hc.csv, 0_imfn3fsnqb.csv, 88_p169fqr8hl.csv, 1_ofoaso8yxd.csv, 97_ksnb8zg7e2.csv, 159_wmf9y5lb3d.csv, 179_enmmbxkd9b.csv, 18_omkydub7qy.csv, 115_iocn2dpj2o.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|██████████| 2/2 [00:00<00:00, 19.04ba/s]100%|██████████| 2/2 [00:00<00:00, 18.98ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 44.65ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:04<19:28,  4.34s/it]  1%|          | 2/270 [00:05<10:24,  2.33s/it]  1%|          | 3/270 [00:06<07:29,  1.68s/it]  1%|▏         | 4/270 [00:07<06:08,  1.38s/it]  2%|▏         | 5/270 [00:08<05:21,  1.22s/it]  2%|▏         | 6/270 [00:08<04:54,  1.12s/it]  3%|▎         | 7/270 [00:09<04:36,  1.05s/it]  3%|▎         | 8/270 [00:10<04:25,  1.01s/it]  3%|▎         | 9/270 [00:11<04:16,  1.02it/s]  4%|▎         | 10/270 [00:12<04:11,  1.04it/s]  4%|▍         | 11/270 [00:13<04:07,  1.05it/s]  4%|▍         | 12/270 [00:14<04:04,  1.05it/s]  5%|▍         | 13/270 [00:15<04:02,  1.06it/s]  5%|▌         | 14/270 [00:16<04:00,  1.06it/s]  6%|▌         | 15/270 [00:17<03:58,  1.07it/s]  6%|▌         | 16/270 [00:18<03:57,  1.07it/s]  6%|▋         | 17/270 [00:19<03:55,  1.07it/s]  7%|▋         | 18/270 [00:20<03:54,  1.07it/s]  7%|▋         | 19/270 [00:21<03:53,  1.07it/s]  7%|▋         | 20/270 [00:21<03:52,  1.08it/s]  8%|▊         | 21/270 [00:22<03:51,  1.07it/s]  8%|▊         | 22/270 [00:23<03:50,  1.08it/s]  9%|▊         | 23/270 [00:24<03:50,  1.07it/s]  9%|▉         | 24/270 [00:25<03:49,  1.07it/s]  9%|▉         | 25/270 [00:26<03:49,  1.07it/s] 10%|▉         | 26/270 [00:27<03:47,  1.07it/s] 10%|█         | 27/270 [00:28<03:22,  1.20it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.09it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.26it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.71it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.47it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.30it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                
                                             [A 10%|█         | 27/270 [00:30<03:22,  1.20it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|█         | 28/270 [00:45<23:38,  5.86s/it] 11%|█         | 29/270 [00:46<17:49,  4.44s/it] 11%|█         | 30/270 [00:47<13:32,  3.38s/it] 11%|█▏        | 31/270 [00:48<10:32,  2.64s/it] 12%|█▏        | 32/270 [00:49<08:27,  2.13s/it] 12%|█▏        | 33/270 [00:50<07:00,  1.77s/it] 13%|█▎        | 34/270 [00:51<05:59,  1.52s/it] 13%|█▎        | 35/270 [00:52<05:16,  1.35s/it] 13%|█▎        | 36/270 [00:53<04:46,  1.22s/it] 14%|█▎        | 37/270 [00:54<04:24,  1.14s/it] 14%|█▍        | 38/270 [00:55<04:10,  1.08s/it] 14%|█▍        | 39/270 [00:56<03:59,  1.04s/it] 15%|█▍        | 40/270 [00:57<03:52,  1.01s/it] 15%|█▌        | 41/270 [00:58<03:46,  1.01it/s] 16%|█▌        | 42/270 [00:59<03:41,  1.03it/s] 16%|█▌        | 43/270 [00:59<03:38,  1.04it/s] 16%|█▋        | 44/270 [01:00<03:36,  1.05it/s] 17%|█▋        | 45/270 [01:01<03:34,  1.05it/s] 17%|█▋        | 46/270 [01:02<03:32,  1.05it/s] 17%|█▋        | 47/270 [01:03<03:30,  1.06it/s] 18%|█▊        | 48/270 [01:04<03:28,  1.06it/s] 18%|█▊        | 49/270 [01:05<03:28,  1.06it/s] 19%|█▊        | 50/270 [01:06<03:27,  1.06it/s]                                                 19%|█▊        | 50/270 [01:06<03:27,  1.06it/s] 19%|█▉        | 51/270 [01:07<03:26,  1.06it/s] 19%|█▉        | 52/270 [01:08<03:25,  1.06it/s] 20%|█▉        | 53/270 [01:09<03:24,  1.06it/s] 20%|██        | 54/270 [01:09<03:02,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 1.436572790145874, 'eval_f1': 0.34578177901913565, 'eval_runtime': 2.7721, 'eval_samples_per_second': 153.674, 'eval_steps_per_second': 2.525, 'epoch': 1.0}
{'loss': 1.397, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.06it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.26it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.74it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.44it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                
                                             [A 20%|██        | 54/270 [01:12<03:02,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██        | 55/270 [01:27<20:35,  5.75s/it] 21%|██        | 56/270 [01:28<15:20,  4.30s/it] 21%|██        | 57/270 [01:29<11:41,  3.29s/it] 21%|██▏       | 58/270 [01:29<09:07,  2.58s/it] 22%|██▏       | 59/270 [01:30<07:21,  2.09s/it] 22%|██▏       | 60/270 [01:31<06:06,  1.75s/it] 23%|██▎       | 61/270 [01:32<05:14,  1.50s/it] 23%|██▎       | 62/270 [01:33<04:37,  1.33s/it] 23%|██▎       | 63/270 [01:34<04:11,  1.22s/it] 24%|██▎       | 64/270 [01:35<03:53,  1.13s/it] 24%|██▍       | 65/270 [01:36<03:40,  1.08s/it] 24%|██▍       | 66/270 [01:37<03:31,  1.04s/it] 25%|██▍       | 67/270 [01:38<03:24,  1.01s/it] 25%|██▌       | 68/270 [01:39<03:19,  1.01it/s] 26%|██▌       | 69/270 [01:40<03:15,  1.03it/s] 26%|██▌       | 70/270 [01:41<03:13,  1.03it/s] 26%|██▋       | 71/270 [01:42<03:10,  1.04it/s] 27%|██▋       | 72/270 [01:43<03:09,  1.05it/s] 27%|██▋       | 73/270 [01:44<03:07,  1.05it/s] 27%|██▋       | 74/270 [01:45<03:05,  1.05it/s] 28%|██▊       | 75/270 [01:45<03:04,  1.06it/s] 28%|██▊       | 76/270 [01:46<03:03,  1.06it/s] 29%|██▊       | 77/270 [01:47<03:02,  1.06it/s] 29%|██▉       | 78/270 [01:48<03:00,  1.06it/s] 29%|██▉       | 79/270 [01:49<03:00,  1.06it/s] 30%|██▉       | 80/270 [01:50<02:59,  1.06it/s] 30%|███       | 81/270 [01:51<02:40,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.756691038608551, 'eval_f1': 0.6075279219929478, 'eval_runtime': 2.6433, 'eval_samples_per_second': 161.16, 'eval_steps_per_second': 2.648, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.26it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.25it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.68it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.41it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.26it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.52it/s][A                                                
                                             [A 30%|███       | 81/270 [01:53<02:40,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.52it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|███       | 82/270 [02:09<18:35,  5.94s/it] 31%|███       | 83/270 [02:10<13:49,  4.44s/it] 31%|███       | 84/270 [02:11<10:30,  3.39s/it] 31%|███▏      | 85/270 [02:11<08:10,  2.65s/it] 32%|███▏      | 86/270 [02:12<06:33,  2.14s/it] 32%|███▏      | 87/270 [02:13<05:25,  1.78s/it] 33%|███▎      | 88/270 [02:14<04:38,  1.53s/it] 33%|███▎      | 89/270 [02:15<04:05,  1.36s/it] 33%|███▎      | 90/270 [02:16<03:42,  1.24s/it] 34%|███▎      | 91/270 [02:17<03:25,  1.15s/it] 34%|███▍      | 92/270 [02:18<03:13,  1.09s/it] 34%|███▍      | 93/270 [02:19<03:04,  1.04s/it] 35%|███▍      | 94/270 [02:20<02:58,  1.01s/it] 35%|███▌      | 95/270 [02:21<02:53,  1.01it/s] 36%|███▌      | 96/270 [02:22<02:50,  1.02it/s] 36%|███▌      | 97/270 [02:23<02:48,  1.03it/s] 36%|███▋      | 98/270 [02:24<02:46,  1.03it/s] 37%|███▋      | 99/270 [02:25<02:44,  1.04it/s] 37%|███▋      | 100/270 [02:26<02:43,  1.04it/s]                                                  37%|███▋      | 100/270 [02:26<02:43,  1.04it/s] 37%|███▋      | 101/270 [02:27<02:41,  1.04it/s] 38%|███▊      | 102/270 [02:28<02:40,  1.05it/s] 38%|███▊      | 103/270 [02:29<02:38,  1.05it/s] 39%|███▊      | 104/270 [02:29<02:37,  1.05it/s] 39%|███▉      | 105/270 [02:30<02:36,  1.05it/s] 39%|███▉      | 106/270 [02:31<02:35,  1.05it/s] 40%|███▉      | 107/270 [02:32<02:34,  1.06it/s] 40%|████      | 108/270 [02:33<02:16,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.4193759560585022, 'eval_f1': 0.8788913421814304, 'eval_runtime': 2.6558, 'eval_samples_per_second': 160.405, 'eval_steps_per_second': 2.636, 'epoch': 3.0}
{'loss': 0.558, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.97it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.16it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.60it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.51it/s][A                                                 
                                             [A 40%|████      | 108/270 [02:36<02:16,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████      | 109/270 [02:51<16:10,  6.03s/it] 41%|████      | 110/270 [02:52<11:59,  4.50s/it] 41%|████      | 111/270 [02:53<09:05,  3.43s/it] 41%|████▏     | 112/270 [02:54<07:04,  2.69s/it] 42%|████▏     | 113/270 [02:55<05:39,  2.16s/it] 42%|████▏     | 114/270 [02:56<04:40,  1.80s/it] 43%|████▎     | 115/270 [02:57<03:58,  1.54s/it] 43%|████▎     | 116/270 [02:58<03:29,  1.36s/it] 43%|████▎     | 117/270 [02:59<03:08,  1.23s/it] 44%|████▎     | 118/270 [02:59<02:54,  1.15s/it] 44%|████▍     | 119/270 [03:00<02:44,  1.09s/it] 44%|████▍     | 120/270 [03:01<02:37,  1.05s/it] 45%|████▍     | 121/270 [03:02<02:31,  1.01s/it] 45%|████▌     | 122/270 [03:03<02:26,  1.01it/s] 46%|████▌     | 123/270 [03:04<02:24,  1.02it/s] 46%|████▌     | 124/270 [03:05<02:21,  1.03it/s] 46%|████▋     | 125/270 [03:06<02:19,  1.04it/s] 47%|████▋     | 126/270 [03:07<02:18,  1.04it/s] 47%|████▋     | 127/270 [03:08<02:16,  1.05it/s] 47%|████▋     | 128/270 [03:09<02:15,  1.05it/s] 48%|████▊     | 129/270 [03:10<02:14,  1.05it/s] 48%|████▊     | 130/270 [03:11<02:13,  1.05it/s] 49%|████▊     | 131/270 [03:12<02:12,  1.05it/s] 49%|████▉     | 132/270 [03:13<02:18,  1.01s/it] 49%|████▉     | 133/270 [03:14<02:15,  1.01it/s] 50%|████▉     | 134/270 [03:15<02:12,  1.02it/s] 50%|█████     | 135/270 [03:15<01:57,  1.15it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.2599692940711975, 'eval_f1': 0.939192704790171, 'eval_runtime': 2.6858, 'eval_samples_per_second': 158.611, 'eval_steps_per_second': 2.606, 'epoch': 4.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.08it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.66it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.38it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.23it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.51it/s][A                                                 
                                             [A 50%|█████     | 135/270 [03:18<01:57,  1.15it/s]
100%|██████████| 7/7 [00:02<00:00,  3.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 136/270 [03:33<13:10,  5.90s/it] 51%|█████     | 137/270 [03:34<09:46,  4.41s/it] 51%|█████     | 138/270 [03:35<07:24,  3.37s/it] 51%|█████▏    | 139/270 [03:36<05:46,  2.64s/it] 52%|█████▏    | 140/270 [03:37<04:37,  2.14s/it] 52%|█████▏    | 141/270 [03:38<03:49,  1.78s/it] 53%|█████▎    | 142/270 [03:39<03:15,  1.53s/it] 53%|█████▎    | 143/270 [03:40<02:51,  1.35s/it] 53%|█████▎    | 144/270 [03:41<02:35,  1.23s/it] 54%|█████▎    | 145/270 [03:42<02:23,  1.14s/it] 54%|█████▍    | 146/270 [03:43<02:14,  1.08s/it] 54%|█████▍    | 147/270 [03:43<02:08,  1.04s/it] 55%|█████▍    | 148/270 [03:44<02:03,  1.02s/it] 55%|█████▌    | 149/270 [03:45<02:00,  1.00it/s] 56%|█████▌    | 150/270 [03:46<01:57,  1.02it/s]                                                  56%|█████▌    | 150/270 [03:46<01:57,  1.02it/s] 56%|█████▌    | 151/270 [03:47<01:56,  1.03it/s] 56%|█████▋    | 152/270 [03:48<01:53,  1.04it/s] 57%|█████▋    | 153/270 [03:49<01:52,  1.04it/s] 57%|█████▋    | 154/270 [03:50<01:50,  1.05it/s] 57%|█████▋    | 155/270 [03:51<01:49,  1.05it/s] 58%|█████▊    | 156/270 [03:52<01:48,  1.05it/s] 58%|█████▊    | 157/270 [03:53<01:47,  1.05it/s] 59%|█████▊    | 158/270 [03:54<01:46,  1.05it/s] 59%|█████▉    | 159/270 [03:55<01:45,  1.05it/s] 59%|█████▉    | 160/270 [03:56<01:44,  1.05it/s] 60%|█████▉    | 161/270 [03:57<01:43,  1.05it/s] 60%|██████    | 162/270 [03:57<01:32,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.21422812342643738, 'eval_f1': 0.9418003722854694, 'eval_runtime': 2.656, 'eval_samples_per_second': 160.391, 'eval_steps_per_second': 2.636, 'epoch': 5.0}
{'loss': 0.2421, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.95it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.66it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.38it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.23it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.49it/s][A                                                 
                                             [A 60%|██████    | 162/270 [04:00<01:32,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.49it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████    | 163/270 [04:15<10:25,  5.84s/it] 61%|██████    | 164/270 [04:16<07:43,  4.37s/it] 61%|██████    | 165/270 [04:17<05:50,  3.34s/it] 61%|██████▏   | 166/270 [04:18<04:32,  2.62s/it] 62%|██████▏   | 167/270 [04:19<03:38,  2.12s/it] 62%|██████▏   | 168/270 [04:20<03:00,  1.77s/it] 63%|██████▎   | 169/270 [04:21<02:33,  1.52s/it] 63%|██████▎   | 170/270 [04:22<02:14,  1.35s/it] 63%|██████▎   | 171/270 [04:22<02:01,  1.23s/it] 64%|██████▎   | 172/270 [04:23<01:51,  1.14s/it] 64%|██████▍   | 173/270 [04:24<01:45,  1.08s/it] 64%|██████▍   | 174/270 [04:25<01:40,  1.04s/it] 65%|██████▍   | 175/270 [04:26<01:36,  1.02s/it] 65%|██████▌   | 176/270 [04:27<01:33,  1.01it/s] 66%|██████▌   | 177/270 [04:28<01:31,  1.01it/s] 66%|██████▌   | 178/270 [04:29<01:29,  1.02it/s] 66%|██████▋   | 179/270 [04:30<01:27,  1.03it/s] 67%|██████▋   | 180/270 [04:31<01:26,  1.04it/s] 67%|██████▋   | 181/270 [04:32<01:25,  1.04it/s] 67%|██████▋   | 182/270 [04:33<01:24,  1.04it/s] 68%|██████▊   | 183/270 [04:34<01:23,  1.05it/s] 68%|██████▊   | 184/270 [04:35<01:22,  1.05it/s] 69%|██████▊   | 185/270 [04:36<01:21,  1.05it/s] 69%|██████▉   | 186/270 [04:37<01:20,  1.05it/s] 69%|██████▉   | 187/270 [04:38<01:19,  1.05it/s] 70%|██████▉   | 188/270 [04:39<01:18,  1.05it/s] 70%|███████   | 189/270 [04:39<01:09,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.19456268846988678, 'eval_f1': 0.9538384563904423, 'eval_runtime': 2.8422, 'eval_samples_per_second': 149.883, 'eval_steps_per_second': 2.463, 'epoch': 6.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.94it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.64it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.36it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.22it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.49it/s][A                                                 
                                             [A 70%|███████   | 189/270 [04:42<01:09,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.49it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|███████   | 190/270 [04:57<07:48,  5.86s/it] 71%|███████   | 191/270 [04:58<05:46,  4.38s/it] 71%|███████   | 192/270 [04:59<04:21,  3.35s/it] 71%|███████▏  | 193/270 [05:00<03:22,  2.63s/it] 72%|███████▏  | 194/270 [05:01<02:41,  2.12s/it] 72%|███████▏  | 195/270 [05:02<02:12,  1.77s/it] 73%|███████▎  | 196/270 [05:02<01:53,  1.53s/it] 73%|███████▎  | 197/270 [05:03<01:38,  1.36s/it] 73%|███████▎  | 198/270 [05:04<01:28,  1.23s/it] 74%|███████▎  | 199/270 [05:05<01:21,  1.15s/it] 74%|███████▍  | 200/270 [05:06<01:15,  1.09s/it]                                                  74%|███████▍  | 200/270 [05:06<01:15,  1.09s/it] 74%|███████▍  | 201/270 [05:07<01:12,  1.04s/it] 75%|███████▍  | 202/270 [05:08<01:09,  1.02s/it] 75%|███████▌  | 203/270 [05:09<01:06,  1.00it/s] 76%|███████▌  | 204/270 [05:10<01:05,  1.01it/s] 76%|███████▌  | 205/270 [05:11<01:03,  1.02it/s] 76%|███████▋  | 206/270 [05:12<01:02,  1.03it/s] 77%|███████▋  | 207/270 [05:13<01:00,  1.03it/s] 77%|███████▋  | 208/270 [05:14<00:59,  1.04it/s] 77%|███████▋  | 209/270 [05:15<00:58,  1.05it/s] 78%|███████▊  | 210/270 [05:16<00:57,  1.05it/s] 78%|███████▊  | 211/270 [05:17<00:56,  1.05it/s] 79%|███████▊  | 212/270 [05:18<00:55,  1.05it/s] 79%|███████▉  | 213/270 [05:19<00:54,  1.04it/s] 79%|███████▉  | 214/270 [05:20<00:53,  1.04it/s] 80%|███████▉  | 215/270 [05:21<00:52,  1.04it/s] 80%|████████  | 216/270 [05:21<00:46,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.17730627954006195, 'eval_f1': 0.953673156455557, 'eval_runtime': 2.6859, 'eval_samples_per_second': 158.603, 'eval_steps_per_second': 2.606, 'epoch': 7.0}
{'loss': 0.163, 'learning_rate': 1.5687918106563326e-06, 'epoch': 7.41}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.00it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.61it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.37it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.21it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.45it/s][A                                                 
                                             [A 80%|████████  | 216/270 [05:24<00:46,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.45it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-216
Configuration saved in training-output/checkpoint-216/config.json
Model weights saved in training-output/checkpoint-216/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-216/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-216/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████  | 217/270 [05:38<05:04,  5.75s/it] 81%|████████  | 218/270 [05:39<03:44,  4.31s/it] 81%|████████  | 219/270 [05:40<02:48,  3.30s/it] 81%|████████▏ | 220/270 [05:41<02:09,  2.59s/it] 82%|████████▏ | 221/270 [05:42<01:42,  2.10s/it] 82%|████████▏ | 222/270 [05:43<01:24,  1.75s/it] 83%|████████▎ | 223/270 [05:44<01:11,  1.51s/it] 83%|████████▎ | 224/270 [05:45<01:01,  1.34s/it] 83%|████████▎ | 225/270 [05:46<00:55,  1.22s/it] 84%|████████▎ | 226/270 [05:47<00:50,  1.14s/it] 84%|████████▍ | 227/270 [05:48<00:46,  1.08s/it] 84%|████████▍ | 228/270 [05:49<00:43,  1.05s/it] 85%|████████▍ | 229/270 [05:50<00:41,  1.02s/it] 85%|████████▌ | 230/270 [05:51<00:39,  1.00it/s] 86%|████████▌ | 231/270 [05:52<00:38,  1.02it/s] 86%|████████▌ | 232/270 [05:53<00:36,  1.03it/s] 86%|████████▋ | 233/270 [05:54<00:35,  1.04it/s] 87%|████████▋ | 234/270 [05:55<00:34,  1.04it/s] 87%|████████▋ | 235/270 [05:55<00:33,  1.04it/s] 87%|████████▋ | 236/270 [05:56<00:32,  1.04it/s] 88%|████████▊ | 237/270 [05:57<00:31,  1.05it/s] 88%|████████▊ | 238/270 [05:58<00:30,  1.04it/s] 89%|████████▊ | 239/270 [05:59<00:29,  1.04it/s] 89%|████████▉ | 240/270 [06:00<00:28,  1.04it/s] 89%|████████▉ | 241/270 [06:01<00:27,  1.05it/s] 90%|████████▉ | 242/270 [06:02<00:26,  1.05it/s] 90%|█████████ | 243/270 [06:03<00:23,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.17371098697185516, 'eval_f1': 0.9557597831676545, 'eval_runtime': 2.6923, 'eval_samples_per_second': 158.232, 'eval_steps_per_second': 2.6, 'epoch': 8.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.13it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.17it/s][A
 57%|█████▋    | 4/7 [00:01<00:01,  2.88it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  2.95it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  2.94it/s][A
100%|██████████| 7/7 [00:02<00:00,  3.26it/s][A                                                 
                                             [A 90%|█████████ | 243/270 [06:06<00:23,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.26it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-243
Configuration saved in training-output/checkpoint-243/config.json
Model weights saved in training-output/checkpoint-243/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-243/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-243/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|█████████ | 244/270 [06:21<02:34,  5.93s/it] 91%|█████████ | 245/270 [06:21<01:50,  4.43s/it] 91%|█████████ | 246/270 [06:22<01:21,  3.38s/it] 91%|█████████▏| 247/270 [06:23<01:00,  2.65s/it] 92%|█████████▏| 248/270 [06:24<00:47,  2.14s/it] 92%|█████████▏| 249/270 [06:25<00:37,  1.78s/it] 93%|█████████▎| 250/270 [06:26<00:30,  1.53s/it]                                                  93%|█████████▎| 250/270 [06:26<00:30,  1.53s/it] 93%|█████████▎| 251/270 [06:27<00:25,  1.36s/it] 93%|█████████▎| 252/270 [06:28<00:22,  1.23s/it] 94%|█████████▎| 253/270 [06:29<00:19,  1.15s/it] 94%|█████████▍| 254/270 [06:30<00:17,  1.09s/it] 94%|█████████▍| 255/270 [06:31<00:15,  1.05s/it] 95%|█████████▍| 256/270 [06:32<00:14,  1.01s/it] 95%|█████████▌| 257/270 [06:33<00:12,  1.01it/s] 96%|█████████▌| 258/270 [06:34<00:11,  1.02it/s] 96%|█████████▌| 259/270 [06:35<00:10,  1.03it/s] 96%|█████████▋| 260/270 [06:36<00:09,  1.04it/s] 97%|█████████▋| 261/270 [06:37<00:08,  1.04it/s] 97%|█████████▋| 262/270 [06:38<00:07,  1.04it/s] 97%|█████████▋| 263/270 [06:39<00:06,  1.05it/s] 98%|█████████▊| 264/270 [06:39<00:05,  1.04it/s] 98%|█████████▊| 265/270 [06:40<00:04,  1.05it/s] 99%|█████████▊| 266/270 [06:41<00:03,  1.05it/s] 99%|█████████▉| 267/270 [06:42<00:02,  1.05it/s] 99%|█████████▉| 268/270 [06:43<00:01,  1.05it/s]100%|█████████▉| 269/270 [06:44<00:00,  1.04it/s]100%|██████████| 270/270 [06:45<00:00,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.1681687980890274, 'eval_f1': 0.9534148247247106, 'eval_runtime': 2.8661, 'eval_samples_per_second': 148.632, 'eval_steps_per_second': 2.442, 'epoch': 9.0}
{'loss': 0.1387, 'learning_rate': 1.3477564710088097e-07, 'epoch': 9.26}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.98it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.14it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.62it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.37it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.22it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.47it/s][A                                                 
                                             [A100%|██████████| 270/270 [06:48<00:00,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.47it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-270
Configuration saved in training-output/checkpoint-270/config.json
Model weights saved in training-output/checkpoint-270/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-270/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-270/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-216 (score: 0.9557597831676545).
                                                 100%|██████████| 270/270 [07:02<00:00,  1.17it/s]100%|██████████| 270/270 [07:02<00:00,  1.56s/it]
{'eval_loss': 0.16397523880004883, 'eval_f1': 0.9534697724548128, 'eval_runtime': 2.6962, 'eval_samples_per_second': 157.999, 'eval_steps_per_second': 2.596, 'epoch': 10.0}
{'train_runtime': 422.4789, 'train_samples_per_second': 40.239, 'train_steps_per_second': 0.639, 'train_loss': 0.4730338750062165, 'epoch': 10.0}
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:06, 16.53it/s]  4%|▎         | 4/107 [00:00<00:06, 16.91it/s]  7%|▋         | 7/107 [00:00<00:04, 21.73it/s]  9%|▉         | 10/107 [00:00<00:04, 24.18it/s] 12%|█▏        | 13/107 [00:00<00:03, 25.47it/s] 15%|█▍        | 16/107 [00:00<00:03, 26.30it/s] 18%|█▊        | 19/107 [00:00<00:03, 26.90it/s] 21%|██        | 22/107 [00:00<00:03, 27.23it/s] 23%|██▎       | 25/107 [00:00<00:02, 27.39it/s] 26%|██▌       | 28/107 [00:01<00:02, 27.60it/s] 29%|██▉       | 31/107 [00:01<00:02, 27.35it/s] 32%|███▏      | 34/107 [00:01<00:02, 27.38it/s] 35%|███▍      | 37/107 [00:01<00:02, 27.53it/s] 37%|███▋      | 40/107 [00:01<00:02, 27.61it/s] 40%|████      | 43/107 [00:01<00:02, 27.67it/s] 43%|████▎     | 46/107 [00:01<00:02, 27.81it/s] 46%|████▌     | 49/107 [00:01<00:02, 27.84it/s] 49%|████▊     | 52/107 [00:01<00:01, 27.79it/s] 51%|█████▏    | 55/107 [00:02<00:01, 27.80it/s] 54%|█████▍    | 58/107 [00:02<00:01, 27.64it/s] 57%|█████▋    | 61/107 [00:02<00:01, 27.55it/s] 60%|█████▉    | 64/107 [00:02<00:01, 27.68it/s] 63%|██████▎   | 67/107 [00:02<00:01, 27.64it/s] 65%|██████▌   | 70/107 [00:02<00:01, 27.71it/s] 68%|██████▊   | 73/107 [00:02<00:01, 27.81it/s] 71%|███████   | 76/107 [00:02<00:01, 27.88it/s] 74%|███████▍  | 79/107 [00:02<00:01, 27.88it/s] 77%|███████▋  | 82/107 [00:03<00:00, 27.84it/s] 79%|███████▉  | 85/107 [00:03<00:00, 27.83it/s] 82%|████████▏ | 88/107 [00:03<00:00, 27.66it/s] 85%|████████▌ | 91/107 [00:03<00:00, 27.70it/s] 88%|████████▊ | 94/107 [00:03<00:00, 27.58it/s] 91%|█████████ | 97/107 [00:03<00:00, 27.69it/s] 93%|█████████▎| 100/107 [00:03<00:00, 27.80it/s] 96%|█████████▋| 103/107 [00:03<00:00, 27.70it/s] 99%|█████████▉| 106/107 [00:03<00:00, 27.71it/s]100%|██████████| 107/107 [00:03<00:00, 27.20it/s]
Model finished with accuracy: 0.9553990610328639, macro-f1: 0.9557597831676545
Confusion matrix:
[[ 74   0   0   4   1]
 [  1  79   0   1   0]
 [  0   0  67   0   0]
 [  0   1   6  87   1]
 [  1   1   0   2 100]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14_mx46s0fx52.csv, 58_lvcsk0yy8g.csv, 133_c0kk72b3fm.csv, 132_s5i7ngx7bu.csv, 112_7g35kmtkhr.csv, 101_9l21auewdi.csv, 69_4tlegp20n1.csv, 35_g0ldvn0rjm.csv, 143_w11kkydg7n.csv, 20_1zahygarmq.csv, 53_4t2mju8m3y.csv, 166_5lv4sjg1qv.csv, 105_3acmrslh5t.csv, 198_shm86f7xzz.csv, 155_97dannvd9q.csv, 189_4mh7pjb6hj.csv, 187_kp3lfo9rti.csv, 3_5uxjfwf4c9.csv, 76_f4jqlrtl0d.csv, 152_uhh1hem3ei.csv, 14_apv4u3qebs.csv, 127_q2ifamcjop.csv, 100_5397borrxc.csv, 129_h8c8ny4x7u.csv, 123_g66af1qsqo.csv, 95_6ikda3av09.csv, 42_kr1ailiclk.csv, 162_gzcq0eli5q.csv, 148_nekma0liim.csv, 50_tmt7xio73x.csv, 183_ex67vzlp8e.csv, 64_gjf3ihxyfj.csv, 182_icy94tmqbq.csv, 96_fe55r9bnfd.csv, 37_7swqi7vgnl.csv, 17_qvtzqld86i.csv, 185_9bjzxvbwca.csv, 4_vet6mgh5md.csv, 109_ehl2s3ah2w.csv, 36_4bg4d4bf4q.csv, 165_vcamdtt7pl.csv, 136_dd735psc8g.csv, 24_1y633kulxy.csv, 7_swfaqmk6y6.csv, 46_eg7yytwm57.csv, 12_i0qb55ygnr.csv, 8_v4bqc7mvzy.csv, 120_tfdaxlk9n2.csv, 103_v129kmq3c5.csv, 144_6g4qagf0aw.csv, 72_37k8qe6uf3.csv, 111_4dftcjzv4j.csv, 1_z949c1i541.csv, 178_7rflrddsnn.csv, 6_blz9uaf562.csv, 15_j7tl3bmirm.csv, 83_slgfcmc4xj.csv, 174_vw5a8s9vzj.csv, 9_m11ut0fzvo.csv, 199_hzu91su6jo.csv, 39_3064xz3lgl.csv, 62_6v7alh2xza.csv, 137_eusnnkienm.csv, 118_66k0pevslq.csv, 113_s8v38lnghb.csv, 90_60znzzhvt3.csv, 161_qu7m9mil3a.csv, 32_jitoqenvci.csv, 176_cl4orgir7l.csv, 65_5y9bhpar44.csv, 27_adgb2e3ry7.csv, 33_zybtjc12ef.csv, 140_rpfb54del9.csv, 70_0b98yn7zuy.csv, 158_lhrhfgkazf.csv, 10_ten3l2nuwn.csv, 48_u6mx1k6p64.csv, 29_juqgc9mrcx.csv, 195_nwk7f5mq3h.csv, 119_hdkgwx4tgr.csv, 163_g4ownm07q9.csv, 134_behtf6grhp.csv, 128_y3nqk9k0x4.csv, 93_7s7jo61qgr.csv, 125_onkofvwzgk.csv, 196_h8ebmn0b4i.csv, 186_77g5px55z1.csv, 80_zbkdixjx1u.csv, 154_i7n85wzpa3.csv, 59_tvvx1filo9.csv, 91_6nvvo85k2r.csv, 26_kcs08kmlzv.csv, 146_umxfdzx3sg.csv, 157_7onih683qj.csv, 122_hmocfdllhz.csv, 102_r9vbr9i6qn.csv, 25_zdmhvwm1pk.csv, 16_omnxwedc88.csv, 116_wf72qb1ctj.csv, 87_zzzdc841sd.csv, 168_39vz7oesxk.csv, 24_pa58yc2bsn.csv, 51_km3hwahw1m.csv, 19_sevtqojtws.csv, 21_x8u5ofrlzo.csv, 75_gginvgfabf.csv, 45_4g2awfbbtn.csv, 4_xu9afrhg0o.csv, 25_ceghr7ntgk.csv, 15_jzm9dvtw73.csv, 89_28gvnlaz8j.csv, 57_l8ux51os5l.csv, 153_mwu7z89cr2.csv, 147_qz39am2soc.csv, 67_ip2ys2xhib.csv, 78_tshg4zfas1.csv, 142_d8jaik03id.csv, 5_wa4f0flvdd.csv, 181_wk573tzjde.csv, 193_kjtylxs5x6.csv, 34_egy98kczu6.csv, 85_i7f88e7b7g.csv, 106_oa27xk4eqt.csv, 156_yk739g3286.csv, 11_0na6xsa4cd.csv, 31_jhmyyhmjh5.csv, 149_tvntt1n3zf.csv, 6_qyv4vhzoe.csv, 63_j79b3nmr83.csv, 104_jos9y36cxr.csv, 151_8k3mbw3phz.csv, 60_zry6xyk2v2.csv, 86_kvt6l8gw8d.csv, 131_bivn5or23h.csv, 175_954mvwcwet.csv, 110_zq7vq985bq.csv, 52_wfyl6gdoxn.csv, 92_u8wrf1zhh5.csv, 71_jjxizibivj.csv, 169_2y8d92hhek.csv, 114_fkhid5yyst.csv, 173_op7k51t6eh.csv, 145_1p84lqxs5t.csv, 171_evt2lbu5wz.csv, 2_wrrttduqx0.csv, 107_jfii3uf345.csv, 164_qcoq7yni4y.csv, 160_9xq61i8z5q.csv, 150_hvfxdsp6uq.csv, 16_j1uppkkwpc.csv, 26_2k5thw2ykv.csv, 172_gg34wiyfxn.csv, 27_r6anjd4ie1.csv, 11_7vk8oa1o24.csv, 138_yx53ai19ar.csv, 44_zsqcy1dp0n.csv, 68_4fxf0cl1sa.csv, 77_gup8yvrjg8.csv, 23_lce4ue1yit.csv, 197_ni25hikvxf.csv, 55_o97cc78tyd.csv, 13_xb7ph2k0m6.csv, 126_s6vu9wsf2o.csv, 124_4p9vzltfyn.csv, 135_7g73f18y8h.csv, 194_tnwwh3zbnj.csv, 99_7fcky2iv54.csv, 49_8fcz4c6oqy.csv, 84_q4rh6yklie.csv, 28_7svph4yobz.csv, 38_38doqz3unu.csv, 5_lcampqa5d8.csv, 82_3qr5zxdcxe.csv, 180_jm6cn0n08c.csv, 66_8gquz8hbpz.csv, 40_zh8fjx6tiy.csv, 141_tvldgnl6pt.csv, 30_rb578trmsj.csv, 184_hzlbex83q3.csv, 56_lxflttkcgn.csv, 22_udsmb087i5.csv, 8_hfxm8dkunn.csv, 61_l3jkp84irv.csv, 139_1qp7vpj70o.csv, 73_4s2wt6mgps.csv, 7_viucp080gt.csv, 108_qkxdz8djgt.csv, 54_07djfakfxh.csv, 98_8et5hi6nqf.csv, 74_08dh1ae49m.csv, 21_x90g7ie9t7.csv, 81_dbvr7ry56o.csv, 121_aobrexg5sv.csv, 191_yxm7fc16uc.csv, 94_gcso78g1mh.csv, 167_bk26q01ex6.csv, 41_rrwls6y9fl.csv, 170_szpuak9x3b.csv, 47_qrdef2nnjn.csv, 13_y1i6zh8y9w.csv, 188_5sjzeb4w03.csv, 130_7a6m1my07s.csv, 192_isa0drpmu4.csv, 79_wkvkhcnsn0.csv, 2_9actgs9mi7.csv, 18_o65lxoj85y.csv, 3_j0hn3a0xzt.csv, 43_1nemhlm74b.csv, 20_z3zjd5sgm4.csv, 117_mgq0ymf5hc.csv, 0_imfn3fsnqb.csv, 88_p169fqr8hl.csv, 1_ofoaso8yxd.csv, 97_ksnb8zg7e2.csv, 159_wmf9y5lb3d.csv, 179_enmmbxkd9b.csv, 18_omkydub7qy.csv, 115_iocn2dpj2o.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|██████████| 2/2 [00:00<00:00, 18.73ba/s]100%|██████████| 2/2 [00:00<00:00, 18.67ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 43.81ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:04<19:17,  4.30s/it]  1%|          | 2/270 [00:05<10:25,  2.34s/it]  1%|          | 3/270 [00:06<07:30,  1.69s/it]  1%|▏         | 4/270 [00:07<06:07,  1.38s/it]  2%|▏         | 5/270 [00:08<05:21,  1.21s/it]  2%|▏         | 6/270 [00:08<04:53,  1.11s/it]  3%|▎         | 7/270 [00:09<04:36,  1.05s/it]  3%|▎         | 8/270 [00:10<04:25,  1.01s/it]  3%|▎         | 9/270 [00:11<04:16,  1.02it/s]  4%|▎         | 10/270 [00:12<04:10,  1.04it/s]  4%|▍         | 11/270 [00:13<04:06,  1.05it/s]  4%|▍         | 12/270 [00:14<04:03,  1.06it/s]  5%|▍         | 13/270 [00:15<04:00,  1.07it/s]  5%|▌         | 14/270 [00:16<03:58,  1.07it/s]  6%|▌         | 15/270 [00:17<03:56,  1.08it/s]  6%|▌         | 16/270 [00:18<03:55,  1.08it/s]  6%|▋         | 17/270 [00:19<03:54,  1.08it/s]  7%|▋         | 18/270 [00:20<03:53,  1.08it/s]  7%|▋         | 19/270 [00:20<03:52,  1.08it/s]  7%|▋         | 20/270 [00:21<03:51,  1.08it/s]  8%|▊         | 21/270 [00:22<03:50,  1.08it/s]  8%|▊         | 22/270 [00:23<03:49,  1.08it/s]  9%|▊         | 23/270 [00:24<03:47,  1.09it/s]  9%|▉         | 24/270 [00:25<03:46,  1.08it/s]  9%|▉         | 25/270 [00:26<03:44,  1.09it/s] 10%|▉         | 26/270 [00:27<03:45,  1.08it/s] 10%|█         | 27/270 [00:27<03:20,  1.21it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.13it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.30it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.49it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.36it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.60it/s][A                                                
                                             [A 10%|█         | 27/270 [00:30<03:20,  1.21it/s]
100%|██████████| 7/7 [00:02<00:00,  3.60it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|█         | 28/270 [00:45<23:57,  5.94s/it] 11%|█         | 29/270 [00:46<18:02,  4.49s/it] 11%|█         | 30/270 [00:47<13:42,  3.43s/it] 11%|█▏        | 31/270 [00:48<10:39,  2.68s/it] 12%|█▏        | 32/270 [00:49<08:32,  2.15s/it] 12%|█▏        | 33/270 [00:50<07:03,  1.79s/it] 13%|█▎        | 34/270 [00:51<06:02,  1.53s/it] 13%|█▎        | 35/270 [00:52<05:18,  1.35s/it] 13%|█▎        | 36/270 [00:53<04:46,  1.22s/it] 14%|█▎        | 37/270 [00:54<04:25,  1.14s/it] 14%|█▍        | 38/270 [00:55<04:10,  1.08s/it] 14%|█▍        | 39/270 [00:56<03:58,  1.03s/it] 15%|█▍        | 40/270 [00:57<03:50,  1.00s/it] 15%|█▌        | 41/270 [00:58<03:45,  1.02it/s] 16%|█▌        | 42/270 [00:59<03:41,  1.03it/s] 16%|█▌        | 43/270 [01:00<03:37,  1.04it/s] 16%|█▋        | 44/270 [01:00<03:34,  1.05it/s] 17%|█▋        | 45/270 [01:01<03:33,  1.05it/s] 17%|█▋        | 46/270 [01:02<03:32,  1.06it/s] 17%|█▋        | 47/270 [01:03<03:30,  1.06it/s] 18%|█▊        | 48/270 [01:04<03:28,  1.06it/s] 18%|█▊        | 49/270 [01:05<03:28,  1.06it/s] 19%|█▊        | 50/270 [01:06<03:26,  1.06it/s]                                                 19%|█▊        | 50/270 [01:06<03:26,  1.06it/s] 19%|█▉        | 51/270 [01:07<03:26,  1.06it/s] 19%|█▉        | 52/270 [01:08<03:24,  1.06it/s] 20%|█▉        | 53/270 [01:09<03:23,  1.06it/s] 20%|██        | 54/270 [01:10<03:00,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 1.2868832349777222, 'eval_f1': 0.34153133088368404, 'eval_runtime': 2.7162, 'eval_samples_per_second': 156.836, 'eval_steps_per_second': 2.577, 'epoch': 1.0}
{'loss': 1.3244, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.05it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.27it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.73it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.45it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.31it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                
                                             [A 20%|██        | 54/270 [01:12<03:00,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██        | 55/270 [01:28<21:51,  6.10s/it] 21%|██        | 56/270 [01:29<16:12,  4.55s/it] 21%|██        | 57/270 [01:30<12:17,  3.46s/it] 21%|██▏       | 58/270 [01:31<09:32,  2.70s/it] 22%|██▏       | 59/270 [01:32<07:38,  2.17s/it] 22%|██▏       | 60/270 [01:33<06:17,  1.80s/it] 23%|██▎       | 61/270 [01:33<05:21,  1.54s/it] 23%|██▎       | 62/270 [01:34<04:42,  1.36s/it] 23%|██▎       | 63/270 [01:35<04:14,  1.23s/it] 24%|██▎       | 64/270 [01:36<03:54,  1.14s/it] 24%|██▍       | 65/270 [01:37<03:41,  1.08s/it] 24%|██▍       | 66/270 [01:38<03:30,  1.03s/it] 25%|██▍       | 67/270 [01:39<03:23,  1.00s/it] 25%|██▌       | 68/270 [01:40<03:18,  1.02it/s] 26%|██▌       | 69/270 [01:41<03:14,  1.03it/s] 26%|██▌       | 70/270 [01:42<03:11,  1.04it/s] 26%|██▋       | 71/270 [01:43<03:09,  1.05it/s] 27%|██▋       | 72/270 [01:44<03:07,  1.06it/s] 27%|██▋       | 73/270 [01:45<03:05,  1.06it/s] 27%|██▋       | 74/270 [01:46<03:04,  1.06it/s] 28%|██▊       | 75/270 [01:47<03:03,  1.06it/s] 28%|██▊       | 76/270 [01:48<03:02,  1.06it/s] 29%|██▊       | 77/270 [01:48<03:01,  1.07it/s] 29%|██▉       | 78/270 [01:49<03:00,  1.06it/s] 29%|██▉       | 79/270 [01:50<02:59,  1.07it/s] 30%|██▉       | 80/270 [01:51<02:58,  1.06it/s] 30%|███       | 81/270 [01:52<02:38,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.7136620283126831, 'eval_f1': 0.6067675445976066, 'eval_runtime': 2.6376, 'eval_samples_per_second': 161.508, 'eval_steps_per_second': 2.654, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.08it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.24it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.69it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.31it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                
                                             [A 30%|███       | 81/270 [01:55<02:38,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|███       | 82/270 [02:10<18:39,  5.95s/it] 31%|███       | 83/270 [02:11<13:51,  4.45s/it] 31%|███       | 84/270 [02:12<10:30,  3.39s/it] 31%|███▏      | 85/270 [02:13<08:11,  2.65s/it] 32%|███▏      | 86/270 [02:13<06:33,  2.14s/it] 32%|███▏      | 87/270 [02:14<05:24,  1.78s/it] 33%|███▎      | 88/270 [02:15<04:37,  1.52s/it] 33%|███▎      | 89/270 [02:16<04:03,  1.35s/it] 33%|███▎      | 90/270 [02:17<03:40,  1.22s/it] 34%|███▎      | 91/270 [02:18<03:23,  1.13s/it] 34%|███▍      | 92/270 [02:19<03:12,  1.08s/it] 34%|███▍      | 93/270 [02:20<03:03,  1.04s/it] 35%|███▍      | 94/270 [02:21<02:57,  1.01s/it] 35%|███▌      | 95/270 [02:22<02:52,  1.01it/s] 36%|███▌      | 96/270 [02:23<02:49,  1.03it/s] 36%|███▌      | 97/270 [02:24<02:46,  1.04it/s] 36%|███▋      | 98/270 [02:25<02:44,  1.04it/s] 37%|███▋      | 99/270 [02:26<02:42,  1.05it/s] 37%|███▋      | 100/270 [02:27<02:41,  1.05it/s]                                                  37%|███▋      | 100/270 [02:27<02:41,  1.05it/s] 37%|███▋      | 101/270 [02:28<02:39,  1.06it/s] 38%|███▊      | 102/270 [02:29<02:38,  1.06it/s] 38%|███▊      | 103/270 [02:29<02:37,  1.06it/s] 39%|███▊      | 104/270 [02:30<02:36,  1.06it/s] 39%|███▉      | 105/270 [02:31<02:34,  1.07it/s] 39%|███▉      | 106/270 [02:32<02:34,  1.06it/s] 40%|███▉      | 107/270 [02:33<02:33,  1.06it/s] 40%|████      | 108/270 [02:34<02:15,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.434349000453949, 'eval_f1': 0.8766580443149617, 'eval_runtime': 2.7641, 'eval_samples_per_second': 154.116, 'eval_steps_per_second': 2.532, 'epoch': 3.0}
{'loss': 0.5451, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.04it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.25it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.71it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.26it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.52it/s][A                                                 
                                             [A 40%|████      | 108/270 [02:36<02:15,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.52it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████      | 109/270 [02:52<16:19,  6.08s/it] 41%|████      | 110/270 [02:53<12:05,  4.54s/it] 41%|████      | 111/270 [02:54<09:09,  3.46s/it] 41%|████▏     | 112/270 [02:55<07:06,  2.70s/it] 42%|████▏     | 113/270 [02:56<05:41,  2.17s/it] 42%|████▏     | 114/270 [02:57<04:40,  1.80s/it] 43%|████▎     | 115/270 [02:58<03:58,  1.54s/it] 43%|████▎     | 116/270 [02:59<03:29,  1.36s/it] 43%|████▎     | 117/270 [03:00<03:08,  1.23s/it] 44%|████▎     | 118/270 [03:01<02:53,  1.14s/it] 44%|████▍     | 119/270 [03:01<02:42,  1.08s/it] 44%|████▍     | 120/270 [03:02<02:35,  1.04s/it] 45%|████▍     | 121/270 [03:03<02:29,  1.01s/it] 45%|████▌     | 122/270 [03:04<02:25,  1.02it/s] 46%|████▌     | 123/270 [03:05<02:22,  1.03it/s] 46%|████▌     | 124/270 [03:06<02:20,  1.04it/s] 46%|████▋     | 125/270 [03:07<02:18,  1.04it/s] 47%|████▋     | 126/270 [03:08<02:17,  1.05it/s] 47%|████▋     | 127/270 [03:09<02:15,  1.05it/s] 47%|████▋     | 128/270 [03:10<02:14,  1.06it/s] 48%|████▊     | 129/270 [03:11<02:13,  1.06it/s] 48%|████▊     | 130/270 [03:12<02:12,  1.06it/s] 49%|████▊     | 131/270 [03:13<02:11,  1.06it/s] 49%|████▉     | 132/270 [03:14<02:10,  1.06it/s] 49%|████▉     | 133/270 [03:15<02:09,  1.06it/s] 50%|████▉     | 134/270 [03:16<02:08,  1.06it/s] 50%|█████     | 135/270 [03:16<01:53,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.27095097303390503, 'eval_f1': 0.9143782833519879, 'eval_runtime': 2.6652, 'eval_samples_per_second': 159.835, 'eval_steps_per_second': 2.626, 'epoch': 4.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.13it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.66it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.41it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                 
                                             [A 50%|█████     | 135/270 [03:19<01:53,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 136/270 [03:34<13:05,  5.87s/it] 51%|█████     | 137/270 [03:35<09:43,  4.39s/it] 51%|█████     | 138/270 [03:36<07:22,  3.35s/it] 51%|█████▏    | 139/270 [03:37<05:44,  2.63s/it] 52%|█████▏    | 140/270 [03:38<04:36,  2.12s/it] 52%|█████▏    | 141/270 [03:38<03:47,  1.77s/it] 53%|█████▎    | 142/270 [03:39<03:14,  1.52s/it] 53%|█████▎    | 143/270 [03:40<02:50,  1.35s/it] 53%|█████▎    | 144/270 [03:41<02:34,  1.23s/it] 54%|█████▎    | 145/270 [03:42<02:22,  1.14s/it] 54%|█████▍    | 146/270 [03:43<02:13,  1.08s/it] 54%|█████▍    | 147/270 [03:44<02:07,  1.04s/it] 55%|█████▍    | 148/270 [03:45<02:03,  1.01s/it] 55%|█████▌    | 149/270 [03:46<02:00,  1.01it/s] 56%|█████▌    | 150/270 [03:47<01:57,  1.02it/s]                                                  56%|█████▌    | 150/270 [03:47<01:57,  1.02it/s] 56%|█████▌    | 151/270 [03:48<01:55,  1.03it/s] 56%|█████▋    | 152/270 [03:49<01:53,  1.04it/s] 57%|█████▋    | 153/270 [03:50<01:51,  1.05it/s] 57%|█████▋    | 154/270 [03:51<01:50,  1.05it/s] 57%|█████▋    | 155/270 [03:52<01:49,  1.05it/s] 58%|█████▊    | 156/270 [03:53<01:48,  1.05it/s] 58%|█████▊    | 157/270 [03:54<01:47,  1.05it/s] 59%|█████▊    | 158/270 [03:55<01:46,  1.05it/s] 59%|█████▉    | 159/270 [03:56<01:51,  1.01s/it] 59%|█████▉    | 160/270 [03:57<01:48,  1.01it/s] 60%|█████▉    | 161/270 [03:58<01:46,  1.03it/s] 60%|██████    | 162/270 [03:58<01:33,  1.16it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.2071136087179184, 'eval_f1': 0.9474209952090608, 'eval_runtime': 2.6683, 'eval_samples_per_second': 159.651, 'eval_steps_per_second': 2.623, 'epoch': 5.0}
{'loss': 0.2469, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.93it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.23it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.67it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.36it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.21it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.49it/s][A                                                 
                                             [A 60%|██████    | 162/270 [04:01<01:33,  1.16it/s]
100%|██████████| 7/7 [00:02<00:00,  3.49it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████    | 163/270 [04:17<11:14,  6.30s/it] 61%|██████    | 164/270 [04:18<08:16,  4.69s/it] 61%|██████    | 165/270 [04:19<06:14,  3.56s/it] 61%|██████▏   | 166/270 [04:20<04:48,  2.77s/it] 62%|██████▏   | 167/270 [04:21<03:49,  2.22s/it] 62%|██████▏   | 168/270 [04:22<03:07,  1.83s/it] 63%|██████▎   | 169/270 [04:23<02:38,  1.57s/it] 63%|██████▎   | 170/270 [04:24<02:17,  1.38s/it] 63%|██████▎   | 171/270 [04:25<02:03,  1.25s/it] 64%|██████▎   | 172/270 [04:26<01:53,  1.15s/it] 64%|██████▍   | 173/270 [04:27<01:45,  1.09s/it] 64%|██████▍   | 174/270 [04:27<01:40,  1.04s/it] 65%|██████▍   | 175/270 [04:28<01:36,  1.01s/it] 65%|██████▌   | 176/270 [04:29<01:33,  1.01it/s] 66%|██████▌   | 177/270 [04:30<01:30,  1.02it/s] 66%|██████▌   | 178/270 [04:31<01:28,  1.04it/s] 66%|██████▋   | 179/270 [04:32<01:27,  1.04it/s] 67%|██████▋   | 180/270 [04:33<01:26,  1.05it/s] 67%|██████▋   | 181/270 [04:34<01:24,  1.05it/s] 67%|██████▋   | 182/270 [04:35<01:23,  1.05it/s] 68%|██████▊   | 183/270 [04:36<01:22,  1.06it/s] 68%|██████▊   | 184/270 [04:37<01:21,  1.06it/s] 69%|██████▊   | 185/270 [04:38<01:20,  1.05it/s] 69%|██████▉   | 186/270 [04:39<01:19,  1.05it/s] 69%|██████▉   | 187/270 [04:40<01:19,  1.05it/s] 70%|██████▉   | 188/270 [04:41<01:18,  1.05it/s] 70%|███████   | 189/270 [04:41<01:08,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.18874303996562958, 'eval_f1': 0.951832603351851, 'eval_runtime': 2.6884, 'eval_samples_per_second': 158.457, 'eval_steps_per_second': 2.604, 'epoch': 6.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.98it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.22it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.66it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.40it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.51it/s][A                                                 
                                             [A 70%|███████   | 189/270 [04:44<01:08,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|███████   | 190/270 [05:00<08:19,  6.25s/it] 71%|███████   | 191/270 [05:01<06:07,  4.65s/it] 71%|███████   | 192/270 [05:02<04:35,  3.54s/it] 71%|███████▏  | 193/270 [05:03<03:32,  2.76s/it] 72%|███████▏  | 194/270 [05:04<02:47,  2.21s/it] 72%|███████▏  | 195/270 [05:05<02:17,  1.83s/it] 73%|███████▎  | 196/270 [05:06<01:55,  1.56s/it] 73%|███████▎  | 197/270 [05:07<01:40,  1.38s/it] 73%|███████▎  | 198/270 [05:08<01:29,  1.24s/it] 74%|███████▎  | 199/270 [05:09<01:21,  1.15s/it] 74%|███████▍  | 200/270 [05:10<01:16,  1.09s/it]                                                  74%|███████▍  | 200/270 [05:10<01:16,  1.09s/it] 74%|███████▍  | 201/270 [05:10<01:12,  1.04s/it] 75%|███████▍  | 202/270 [05:11<01:08,  1.01s/it] 75%|███████▌  | 203/270 [05:12<01:06,  1.01it/s] 76%|███████▌  | 204/270 [05:13<01:04,  1.02it/s] 76%|███████▌  | 205/270 [05:14<01:02,  1.04it/s] 76%|███████▋  | 206/270 [05:15<01:01,  1.04it/s] 77%|███████▋  | 207/270 [05:16<01:00,  1.05it/s] 77%|███████▋  | 208/270 [05:17<00:59,  1.05it/s] 77%|███████▋  | 209/270 [05:18<00:57,  1.05it/s] 78%|███████▊  | 210/270 [05:19<00:56,  1.05it/s] 78%|███████▊  | 211/270 [05:20<00:55,  1.05it/s] 79%|███████▊  | 212/270 [05:21<00:54,  1.05it/s] 79%|███████▉  | 213/270 [05:22<00:53,  1.06it/s] 79%|███████▉  | 214/270 [05:23<00:52,  1.06it/s] 80%|███████▉  | 215/270 [05:24<00:51,  1.06it/s] 80%|████████  | 216/270 [05:24<00:45,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.17790089547634125, 'eval_f1': 0.95907458777475, 'eval_runtime': 2.7125, 'eval_samples_per_second': 157.049, 'eval_steps_per_second': 2.581, 'epoch': 7.0}
{'loss': 0.1697, 'learning_rate': 1.5687918106563326e-06, 'epoch': 7.41}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.97it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.23it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.65it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.40it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.51it/s][A                                                 
                                             [A 80%|████████  | 216/270 [05:27<00:45,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-216
Configuration saved in training-output/checkpoint-216/config.json
Model weights saved in training-output/checkpoint-216/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-216/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-216/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████  | 217/270 [05:42<05:18,  6.02s/it] 81%|████████  | 218/270 [05:43<03:53,  4.49s/it] 81%|████████  | 219/270 [05:44<02:54,  3.43s/it] 81%|████████▏ | 220/270 [05:45<02:13,  2.68s/it] 82%|████████▏ | 221/270 [05:46<01:45,  2.16s/it] 82%|████████▏ | 222/270 [05:47<01:26,  1.79s/it] 83%|████████▎ | 223/270 [05:48<01:12,  1.54s/it] 83%|████████▎ | 224/270 [05:49<01:02,  1.36s/it] 83%|████████▎ | 225/270 [05:50<00:55,  1.23s/it] 84%|████████▎ | 226/270 [05:51<00:50,  1.14s/it] 84%|████████▍ | 227/270 [05:52<00:46,  1.08s/it] 84%|████████▍ | 228/270 [05:53<00:43,  1.04s/it] 85%|████████▍ | 229/270 [05:54<00:41,  1.01s/it] 85%|████████▌ | 230/270 [05:55<00:39,  1.01it/s] 86%|████████▌ | 231/270 [05:56<00:38,  1.02it/s] 86%|████████▌ | 232/270 [05:56<00:36,  1.03it/s] 86%|████████▋ | 233/270 [05:57<00:35,  1.04it/s] 87%|████████▋ | 234/270 [05:58<00:34,  1.05it/s] 87%|████████▋ | 235/270 [05:59<00:33,  1.05it/s] 87%|████████▋ | 236/270 [06:00<00:32,  1.05it/s] 88%|████████▊ | 237/270 [06:01<00:31,  1.05it/s] 88%|████████▊ | 238/270 [06:02<00:30,  1.05it/s] 89%|████████▊ | 239/270 [06:03<00:29,  1.06it/s] 89%|████████▉ | 240/270 [06:04<00:28,  1.05it/s] 89%|████████▉ | 241/270 [06:05<00:27,  1.05it/s] 90%|████████▉ | 242/270 [06:06<00:26,  1.05it/s] 90%|█████████ | 243/270 [06:07<00:22,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.16602608561515808, 'eval_f1': 0.956739192602921, 'eval_runtime': 2.6628, 'eval_samples_per_second': 159.984, 'eval_steps_per_second': 2.629, 'epoch': 8.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.93it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.16it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.64it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.40it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.49it/s][A                                                 
                                             [A 90%|█████████ | 243/270 [06:10<00:22,  1.18it/s]
100%|██████████| 7/7 [00:03<00:00,  3.49it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-243
Configuration saved in training-output/checkpoint-243/config.json
Model weights saved in training-output/checkpoint-243/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-243/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-243/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|█████████ | 244/270 [06:27<02:52,  6.64s/it] 91%|█████████ | 245/270 [06:28<02:03,  4.93s/it] 91%|█████████ | 246/270 [06:29<01:29,  3.73s/it] 91%|█████████▏| 247/270 [06:29<01:06,  2.89s/it] 92%|█████████▏| 248/270 [06:30<00:50,  2.30s/it] 92%|█████████▏| 249/270 [06:31<00:39,  1.89s/it] 93%|█████████▎| 250/270 [06:32<00:32,  1.61s/it]                                                  93%|█████████▎| 250/270 [06:32<00:32,  1.61s/it] 93%|█████████▎| 251/270 [06:33<00:26,  1.41s/it] 93%|█████████▎| 252/270 [06:34<00:22,  1.27s/it] 94%|█████████▎| 253/270 [06:35<00:19,  1.17s/it] 94%|█████████▍| 254/270 [06:36<00:17,  1.10s/it] 94%|█████████▍| 255/270 [06:37<00:15,  1.05s/it] 95%|█████████▍| 256/270 [06:38<00:14,  1.02s/it] 95%|█████████▌| 257/270 [06:39<00:13,  1.00s/it] 96%|█████████▌| 258/270 [06:40<00:11,  1.02it/s] 96%|█████████▌| 259/270 [06:41<00:10,  1.03it/s] 96%|█████████▋| 260/270 [06:42<00:09,  1.04it/s] 97%|█████████▋| 261/270 [06:43<00:08,  1.04it/s] 97%|█████████▋| 262/270 [06:44<00:07,  1.05it/s] 97%|█████████▋| 263/270 [06:45<00:06,  1.05it/s] 98%|█████████▊| 264/270 [06:46<00:05,  1.05it/s] 98%|█████████▊| 265/270 [06:46<00:04,  1.05it/s] 99%|█████████▊| 266/270 [06:47<00:03,  1.05it/s] 99%|█████████▉| 267/270 [06:48<00:02,  1.05it/s] 99%|█████████▉| 268/270 [06:49<00:01,  1.05it/s]100%|█████████▉| 269/270 [06:50<00:00,  1.05it/s]100%|██████████| 270/270 [06:51<00:00,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.1678241640329361, 'eval_f1': 0.9567790309289377, 'eval_runtime': 3.4193, 'eval_samples_per_second': 124.586, 'eval_steps_per_second': 2.047, 'epoch': 9.0}
{'loss': 0.143, 'learning_rate': 1.3477564710088097e-07, 'epoch': 9.26}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.90it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.15it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.61it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.24it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.49it/s][A                                                 
                                             [A100%|██████████| 270/270 [06:54<00:00,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.49it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-270
Configuration saved in training-output/checkpoint-270/config.json
Model weights saved in training-output/checkpoint-270/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-270/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-270/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-189 (score: 0.95907458777475).
                                                 100%|██████████| 270/270 [07:11<00:00,  1.18it/s]100%|██████████| 270/270 [07:11<00:00,  1.60s/it]
{'eval_loss': 0.16828931868076324, 'eval_f1': 0.9567790309289377, 'eval_runtime': 2.6851, 'eval_samples_per_second': 158.655, 'eval_steps_per_second': 2.607, 'epoch': 10.0}
{'train_runtime': 431.5663, 'train_samples_per_second': 39.391, 'train_steps_per_second': 0.626, 'train_loss': 0.45944395860036213, 'epoch': 10.0}
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:06, 16.40it/s]  4%|▎         | 4/107 [00:00<00:05, 18.31it/s]  7%|▋         | 7/107 [00:00<00:04, 22.76it/s]  9%|▉         | 10/107 [00:00<00:03, 24.91it/s] 12%|█▏        | 13/107 [00:00<00:03, 26.02it/s] 15%|█▍        | 16/107 [00:00<00:03, 26.74it/s] 18%|█▊        | 19/107 [00:00<00:03, 27.25it/s] 21%|██        | 22/107 [00:00<00:03, 27.53it/s] 23%|██▎       | 25/107 [00:00<00:02, 27.68it/s] 26%|██▌       | 28/107 [00:01<00:02, 27.74it/s] 29%|██▉       | 31/107 [00:01<00:02, 27.60it/s] 32%|███▏      | 34/107 [00:01<00:02, 27.44it/s] 35%|███▍      | 37/107 [00:01<00:02, 27.58it/s] 37%|███▋      | 40/107 [00:01<00:02, 27.63it/s] 40%|████      | 43/107 [00:01<00:02, 27.59it/s] 43%|████▎     | 46/107 [00:01<00:02, 27.72it/s] 46%|████▌     | 49/107 [00:01<00:02, 27.85it/s] 49%|████▊     | 52/107 [00:01<00:01, 27.84it/s] 51%|█████▏    | 55/107 [00:02<00:01, 27.85it/s] 54%|█████▍    | 58/107 [00:02<00:01, 27.71it/s] 57%|█████▋    | 61/107 [00:02<00:01, 27.58it/s] 60%|█████▉    | 64/107 [00:02<00:01, 27.63it/s] 63%|██████▎   | 67/107 [00:02<00:01, 27.73it/s] 65%|██████▌   | 70/107 [00:02<00:01, 27.74it/s] 68%|██████▊   | 73/107 [00:02<00:01, 27.77it/s] 71%|███████   | 76/107 [00:02<00:01, 27.87it/s] 74%|███████▍  | 79/107 [00:02<00:01, 27.86it/s] 77%|███████▋  | 82/107 [00:03<00:00, 27.87it/s] 79%|███████▉  | 85/107 [00:03<00:00, 27.89it/s] 82%|████████▏ | 88/107 [00:03<00:00, 27.60it/s] 85%|████████▌ | 91/107 [00:03<00:00, 27.71it/s] 88%|████████▊ | 94/107 [00:03<00:00, 27.71it/s] 91%|█████████ | 97/107 [00:03<00:00, 27.66it/s] 93%|█████████▎| 100/107 [00:03<00:00, 27.74it/s] 96%|█████████▋| 103/107 [00:03<00:00, 27.76it/s] 99%|█████████▉| 106/107 [00:03<00:00, 27.71it/s]100%|██████████| 107/107 [00:03<00:00, 27.35it/s]
Model finished with accuracy: 0.960093896713615, macro-f1: 0.95907458777475
Confusion matrix:
[[ 87   0   0   1   3]
 [  0  80   0   1   0]
 [  0   1  58   3   0]
 [  0   0   4 104   0]
 [  1   2   0   1  80]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14_mx46s0fx52.csv, 58_lvcsk0yy8g.csv, 133_c0kk72b3fm.csv, 132_s5i7ngx7bu.csv, 112_7g35kmtkhr.csv, 101_9l21auewdi.csv, 69_4tlegp20n1.csv, 35_g0ldvn0rjm.csv, 143_w11kkydg7n.csv, 20_1zahygarmq.csv, 53_4t2mju8m3y.csv, 166_5lv4sjg1qv.csv, 105_3acmrslh5t.csv, 198_shm86f7xzz.csv, 155_97dannvd9q.csv, 189_4mh7pjb6hj.csv, 187_kp3lfo9rti.csv, 3_5uxjfwf4c9.csv, 76_f4jqlrtl0d.csv, 152_uhh1hem3ei.csv, 14_apv4u3qebs.csv, 127_q2ifamcjop.csv, 100_5397borrxc.csv, 129_h8c8ny4x7u.csv, 123_g66af1qsqo.csv, 95_6ikda3av09.csv, 42_kr1ailiclk.csv, 162_gzcq0eli5q.csv, 148_nekma0liim.csv, 50_tmt7xio73x.csv, 183_ex67vzlp8e.csv, 64_gjf3ihxyfj.csv, 182_icy94tmqbq.csv, 96_fe55r9bnfd.csv, 37_7swqi7vgnl.csv, 17_qvtzqld86i.csv, 185_9bjzxvbwca.csv, 4_vet6mgh5md.csv, 109_ehl2s3ah2w.csv, 36_4bg4d4bf4q.csv, 165_vcamdtt7pl.csv, 136_dd735psc8g.csv, 24_1y633kulxy.csv, 7_swfaqmk6y6.csv, 46_eg7yytwm57.csv, 12_i0qb55ygnr.csv, 8_v4bqc7mvzy.csv, 120_tfdaxlk9n2.csv, 103_v129kmq3c5.csv, 144_6g4qagf0aw.csv, 72_37k8qe6uf3.csv, 111_4dftcjzv4j.csv, 1_z949c1i541.csv, 178_7rflrddsnn.csv, 6_blz9uaf562.csv, 15_j7tl3bmirm.csv, 83_slgfcmc4xj.csv, 174_vw5a8s9vzj.csv, 9_m11ut0fzvo.csv, 199_hzu91su6jo.csv, 39_3064xz3lgl.csv, 62_6v7alh2xza.csv, 137_eusnnkienm.csv, 118_66k0pevslq.csv, 113_s8v38lnghb.csv, 90_60znzzhvt3.csv, 161_qu7m9mil3a.csv, 32_jitoqenvci.csv, 176_cl4orgir7l.csv, 65_5y9bhpar44.csv, 27_adgb2e3ry7.csv, 33_zybtjc12ef.csv, 140_rpfb54del9.csv, 70_0b98yn7zuy.csv, 158_lhrhfgkazf.csv, 10_ten3l2nuwn.csv, 48_u6mx1k6p64.csv, 29_juqgc9mrcx.csv, 195_nwk7f5mq3h.csv, 119_hdkgwx4tgr.csv, 163_g4ownm07q9.csv, 134_behtf6grhp.csv, 128_y3nqk9k0x4.csv, 93_7s7jo61qgr.csv, 125_onkofvwzgk.csv, 196_h8ebmn0b4i.csv, 186_77g5px55z1.csv, 80_zbkdixjx1u.csv, 154_i7n85wzpa3.csv, 59_tvvx1filo9.csv, 91_6nvvo85k2r.csv, 26_kcs08kmlzv.csv, 146_umxfdzx3sg.csv, 157_7onih683qj.csv, 122_hmocfdllhz.csv, 102_r9vbr9i6qn.csv, 25_zdmhvwm1pk.csv, 16_omnxwedc88.csv, 116_wf72qb1ctj.csv, 87_zzzdc841sd.csv, 168_39vz7oesxk.csv, 24_pa58yc2bsn.csv, 51_km3hwahw1m.csv, 19_sevtqojtws.csv, 21_x8u5ofrlzo.csv, 75_gginvgfabf.csv, 45_4g2awfbbtn.csv, 4_xu9afrhg0o.csv, 25_ceghr7ntgk.csv, 15_jzm9dvtw73.csv, 89_28gvnlaz8j.csv, 57_l8ux51os5l.csv, 153_mwu7z89cr2.csv, 147_qz39am2soc.csv, 67_ip2ys2xhib.csv, 78_tshg4zfas1.csv, 142_d8jaik03id.csv, 5_wa4f0flvdd.csv, 181_wk573tzjde.csv, 193_kjtylxs5x6.csv, 34_egy98kczu6.csv, 85_i7f88e7b7g.csv, 106_oa27xk4eqt.csv, 156_yk739g3286.csv, 11_0na6xsa4cd.csv, 31_jhmyyhmjh5.csv, 149_tvntt1n3zf.csv, 6_qyv4vhzoe.csv, 63_j79b3nmr83.csv, 104_jos9y36cxr.csv, 151_8k3mbw3phz.csv, 60_zry6xyk2v2.csv, 86_kvt6l8gw8d.csv, 131_bivn5or23h.csv, 175_954mvwcwet.csv, 110_zq7vq985bq.csv, 52_wfyl6gdoxn.csv, 92_u8wrf1zhh5.csv, 71_jjxizibivj.csv, 169_2y8d92hhek.csv, 114_fkhid5yyst.csv, 173_op7k51t6eh.csv, 145_1p84lqxs5t.csv, 171_evt2lbu5wz.csv, 2_wrrttduqx0.csv, 107_jfii3uf345.csv, 164_qcoq7yni4y.csv, 160_9xq61i8z5q.csv, 150_hvfxdsp6uq.csv, 16_j1uppkkwpc.csv, 26_2k5thw2ykv.csv, 172_gg34wiyfxn.csv, 27_r6anjd4ie1.csv, 11_7vk8oa1o24.csv, 138_yx53ai19ar.csv, 44_zsqcy1dp0n.csv, 68_4fxf0cl1sa.csv, 77_gup8yvrjg8.csv, 23_lce4ue1yit.csv, 197_ni25hikvxf.csv, 55_o97cc78tyd.csv, 13_xb7ph2k0m6.csv, 126_s6vu9wsf2o.csv, 124_4p9vzltfyn.csv, 135_7g73f18y8h.csv, 194_tnwwh3zbnj.csv, 99_7fcky2iv54.csv, 49_8fcz4c6oqy.csv, 84_q4rh6yklie.csv, 28_7svph4yobz.csv, 38_38doqz3unu.csv, 5_lcampqa5d8.csv, 82_3qr5zxdcxe.csv, 180_jm6cn0n08c.csv, 66_8gquz8hbpz.csv, 40_zh8fjx6tiy.csv, 141_tvldgnl6pt.csv, 30_rb578trmsj.csv, 184_hzlbex83q3.csv, 56_lxflttkcgn.csv, 22_udsmb087i5.csv, 8_hfxm8dkunn.csv, 61_l3jkp84irv.csv, 139_1qp7vpj70o.csv, 73_4s2wt6mgps.csv, 7_viucp080gt.csv, 108_qkxdz8djgt.csv, 54_07djfakfxh.csv, 98_8et5hi6nqf.csv, 74_08dh1ae49m.csv, 21_x90g7ie9t7.csv, 81_dbvr7ry56o.csv, 121_aobrexg5sv.csv, 191_yxm7fc16uc.csv, 94_gcso78g1mh.csv, 167_bk26q01ex6.csv, 41_rrwls6y9fl.csv, 170_szpuak9x3b.csv, 47_qrdef2nnjn.csv, 13_y1i6zh8y9w.csv, 188_5sjzeb4w03.csv, 130_7a6m1my07s.csv, 192_isa0drpmu4.csv, 79_wkvkhcnsn0.csv, 2_9actgs9mi7.csv, 18_o65lxoj85y.csv, 3_j0hn3a0xzt.csv, 43_1nemhlm74b.csv, 20_z3zjd5sgm4.csv, 117_mgq0ymf5hc.csv, 0_imfn3fsnqb.csv, 88_p169fqr8hl.csv, 1_ofoaso8yxd.csv, 97_ksnb8zg7e2.csv, 159_wmf9y5lb3d.csv, 179_enmmbxkd9b.csv, 18_omkydub7qy.csv, 115_iocn2dpj2o.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|██████████| 2/2 [00:00<00:00, 17.68ba/s]100%|██████████| 2/2 [00:00<00:00, 17.63ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 40.60ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:04<20:46,  4.63s/it]  1%|          | 2/270 [00:05<10:57,  2.45s/it]  1%|          | 3/270 [00:06<07:47,  1.75s/it]  1%|▏         | 4/270 [00:07<06:18,  1.42s/it]  2%|▏         | 5/270 [00:08<05:28,  1.24s/it]  2%|▏         | 6/270 [00:09<04:59,  1.13s/it]  3%|▎         | 7/270 [00:10<04:39,  1.06s/it]  3%|▎         | 8/270 [00:11<04:26,  1.02s/it]  3%|▎         | 9/270 [00:12<04:17,  1.01it/s]  4%|▎         | 10/270 [00:12<04:12,  1.03it/s]  4%|▍         | 11/270 [00:13<04:07,  1.05it/s]  4%|▍         | 12/270 [00:14<04:04,  1.05it/s]  5%|▍         | 13/270 [00:15<04:01,  1.06it/s]  5%|▌         | 14/270 [00:16<04:00,  1.07it/s]  6%|▌         | 15/270 [00:17<03:58,  1.07it/s]  6%|▌         | 16/270 [00:18<03:57,  1.07it/s]  6%|▋         | 17/270 [00:19<03:55,  1.07it/s]  7%|▋         | 18/270 [00:20<03:54,  1.07it/s]  7%|▋         | 19/270 [00:21<03:53,  1.07it/s]  7%|▋         | 20/270 [00:22<03:52,  1.07it/s]  8%|▊         | 21/270 [00:23<03:52,  1.07it/s]  8%|▊         | 22/270 [00:24<03:51,  1.07it/s]  9%|▊         | 23/270 [00:25<03:51,  1.07it/s]  9%|▉         | 24/270 [00:25<03:50,  1.07it/s]  9%|▉         | 25/270 [00:26<03:48,  1.07it/s] 10%|▉         | 26/270 [00:27<03:47,  1.07it/s] 10%|█         | 27/270 [00:28<03:23,  1.20it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.13it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.28it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.71it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.29it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                
                                             [A 10%|█         | 27/270 [00:31<03:23,  1.20it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|█         | 28/270 [00:46<24:40,  6.12s/it] 11%|█         | 29/270 [00:48<18:32,  4.62s/it] 11%|█         | 30/270 [00:48<14:02,  3.51s/it] 11%|█▏        | 31/270 [00:49<10:54,  2.74s/it] 12%|█▏        | 32/270 [00:50<08:42,  2.19s/it] 12%|█▏        | 33/270 [00:51<07:10,  1.82s/it] 13%|█▎        | 34/270 [00:52<06:06,  1.55s/it] 13%|█▎        | 35/270 [00:53<05:21,  1.37s/it] 13%|█▎        | 36/270 [00:54<04:49,  1.24s/it] 14%|█▎        | 37/270 [00:55<04:27,  1.15s/it] 14%|█▍        | 38/270 [00:56<04:11,  1.08s/it] 14%|█▍        | 39/270 [00:57<03:59,  1.04s/it] 15%|█▍        | 40/270 [00:58<03:52,  1.01s/it] 15%|█▌        | 41/270 [00:59<03:46,  1.01it/s] 16%|█▌        | 42/270 [01:00<03:40,  1.03it/s] 16%|█▌        | 43/270 [01:01<03:37,  1.04it/s] 16%|█▋        | 44/270 [01:02<03:35,  1.05it/s] 17%|█▋        | 45/270 [01:02<03:33,  1.05it/s] 17%|█▋        | 46/270 [01:03<03:32,  1.05it/s] 17%|█▋        | 47/270 [01:04<03:31,  1.06it/s] 18%|█▊        | 48/270 [01:05<03:29,  1.06it/s] 18%|█▊        | 49/270 [01:06<03:28,  1.06it/s] 19%|█▊        | 50/270 [01:07<03:27,  1.06it/s]                                                 19%|█▊        | 50/270 [01:07<03:27,  1.06it/s] 19%|█▉        | 51/270 [01:08<03:26,  1.06it/s] 19%|█▉        | 52/270 [01:09<03:25,  1.06it/s] 20%|█▉        | 53/270 [01:10<03:24,  1.06it/s] 20%|██        | 54/270 [01:11<03:02,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 1.3552930355072021, 'eval_f1': 0.39088647110885244, 'eval_runtime': 3.0044, 'eval_samples_per_second': 141.792, 'eval_steps_per_second': 2.33, 'epoch': 1.0}
{'loss': 1.3711, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.14it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.31it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.70it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.27it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                
                                             [A 20%|██        | 54/270 [01:13<03:02,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██        | 55/270 [01:29<22:23,  6.25s/it] 21%|██        | 56/270 [01:30<16:36,  4.66s/it] 21%|██        | 57/270 [01:31<12:33,  3.54s/it] 21%|██▏       | 58/270 [01:32<09:45,  2.76s/it] 22%|██▏       | 59/270 [01:33<07:46,  2.21s/it] 22%|██▏       | 60/270 [01:34<06:24,  1.83s/it] 23%|██▎       | 61/270 [01:35<05:26,  1.56s/it] 23%|██▎       | 62/270 [01:36<04:45,  1.37s/it] 23%|██▎       | 63/270 [01:37<04:17,  1.24s/it] 24%|██▎       | 64/270 [01:38<03:57,  1.15s/it] 24%|██▍       | 65/270 [01:39<03:43,  1.09s/it] 24%|██▍       | 66/270 [01:40<03:33,  1.05s/it] 25%|██▍       | 67/270 [01:41<03:26,  1.02s/it] 25%|██▌       | 68/270 [01:42<03:20,  1.01it/s] 26%|██▌       | 69/270 [01:43<03:16,  1.02it/s] 26%|██▌       | 70/270 [01:44<03:13,  1.04it/s] 26%|██▋       | 71/270 [01:44<03:11,  1.04it/s] 27%|██▋       | 72/270 [01:45<03:08,  1.05it/s] 27%|██▋       | 73/270 [01:46<03:07,  1.05it/s] 27%|██▋       | 74/270 [01:47<03:05,  1.05it/s] 28%|██▊       | 75/270 [01:48<03:04,  1.05it/s] 28%|██▊       | 76/270 [01:49<03:04,  1.05it/s] 29%|██▊       | 77/270 [01:50<03:03,  1.05it/s] 29%|██▉       | 78/270 [01:51<03:02,  1.05it/s] 29%|██▉       | 79/270 [01:52<03:01,  1.05it/s] 30%|██▉       | 80/270 [01:53<03:00,  1.05it/s] 30%|███       | 81/270 [01:54<02:40,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.7166891098022461, 'eval_f1': 0.6815911713934699, 'eval_runtime': 2.667, 'eval_samples_per_second': 159.732, 'eval_steps_per_second': 2.625, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.01it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.66it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.24it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                
                                             [A 30%|███       | 81/270 [01:56<02:40,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|███       | 82/270 [02:12<19:17,  6.16s/it] 31%|███       | 83/270 [02:13<14:19,  4.59s/it] 31%|███       | 84/270 [02:14<10:50,  3.50s/it] 31%|███▏      | 85/270 [02:15<08:24,  2.73s/it] 32%|███▏      | 86/270 [02:16<06:43,  2.19s/it] 32%|███▏      | 87/270 [02:17<05:32,  1.82s/it] 33%|███▎      | 88/270 [02:18<04:43,  1.56s/it] 33%|███▎      | 89/270 [02:19<04:08,  1.37s/it] 33%|███▎      | 90/270 [02:20<03:43,  1.24s/it] 34%|███▎      | 91/270 [02:21<03:26,  1.15s/it] 34%|███▍      | 92/270 [02:22<03:14,  1.09s/it] 34%|███▍      | 93/270 [02:23<03:05,  1.05s/it] 35%|███▍      | 94/270 [02:24<02:59,  1.02s/it] 35%|███▌      | 95/270 [02:24<02:54,  1.00it/s] 36%|███▌      | 96/270 [02:25<02:51,  1.01it/s] 36%|███▌      | 97/270 [02:26<02:48,  1.03it/s] 36%|███▋      | 98/270 [02:27<02:45,  1.04it/s] 37%|███▋      | 99/270 [02:28<02:43,  1.04it/s] 37%|███▋      | 100/270 [02:29<02:42,  1.04it/s]                                                  37%|███▋      | 100/270 [02:29<02:42,  1.04it/s] 37%|███▋      | 101/270 [02:30<02:41,  1.04it/s] 38%|███▊      | 102/270 [02:31<02:40,  1.04it/s] 38%|███▊      | 103/270 [02:32<02:39,  1.05it/s] 39%|███▊      | 104/270 [02:33<02:37,  1.05it/s] 39%|███▉      | 105/270 [02:34<02:37,  1.05it/s] 39%|███▉      | 106/270 [02:35<02:36,  1.05it/s] 40%|███▉      | 107/270 [02:36<02:35,  1.05it/s] 40%|████      | 108/270 [02:36<02:18,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.43678951263427734, 'eval_f1': 0.8953194822737892, 'eval_runtime': 2.6717, 'eval_samples_per_second': 159.449, 'eval_steps_per_second': 2.62, 'epoch': 3.0}
{'loss': 0.562, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.06it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.17it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.63it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.22it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.50it/s][A                                                 
                                             [A 40%|████      | 108/270 [02:39<02:18,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.50it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████      | 109/270 [02:58<18:45,  6.99s/it] 41%|████      | 110/270 [02:59<13:47,  5.17s/it] 41%|████      | 111/270 [03:00<10:20,  3.90s/it] 41%|████▏     | 112/270 [03:01<07:55,  3.01s/it] 42%|████▏     | 113/270 [03:02<06:15,  2.39s/it] 42%|████▏     | 114/270 [03:02<05:04,  1.95s/it] 43%|████▎     | 115/270 [03:03<04:15,  1.65s/it] 43%|████▎     | 116/270 [03:04<03:40,  1.43s/it] 43%|████▎     | 117/270 [03:05<03:16,  1.29s/it] 44%|████▎     | 118/270 [03:06<02:59,  1.18s/it] 44%|████▍     | 119/270 [03:07<02:47,  1.11s/it] 44%|████▍     | 120/270 [03:08<02:38,  1.06s/it] 45%|████▍     | 121/270 [03:09<02:32,  1.02s/it] 45%|████▌     | 122/270 [03:10<02:28,  1.00s/it] 46%|████▌     | 123/270 [03:11<02:24,  1.02it/s] 46%|████▌     | 124/270 [03:12<02:21,  1.03it/s] 46%|████▋     | 125/270 [03:13<02:19,  1.04it/s] 47%|████▋     | 126/270 [03:14<02:17,  1.04it/s] 47%|████▋     | 127/270 [03:15<02:16,  1.05it/s] 47%|████▋     | 128/270 [03:16<02:14,  1.05it/s] 48%|████▊     | 129/270 [03:17<02:13,  1.05it/s] 48%|████▊     | 130/270 [03:18<02:13,  1.05it/s] 49%|████▊     | 131/270 [03:19<02:12,  1.05it/s] 49%|████▉     | 132/270 [03:19<02:11,  1.05it/s] 49%|████▉     | 133/270 [03:20<02:09,  1.05it/s] 50%|████▉     | 134/270 [03:21<02:09,  1.05it/s] 50%|█████     | 135/270 [03:22<01:54,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.25378692150115967, 'eval_f1': 0.9402621556342012, 'eval_runtime': 2.7061, 'eval_samples_per_second': 157.424, 'eval_steps_per_second': 2.587, 'epoch': 4.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.06it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.67it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.51it/s][A                                                 
                                             [A 50%|█████     | 135/270 [03:25<01:54,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 136/270 [03:41<13:49,  6.19s/it] 51%|█████     | 137/270 [03:42<10:13,  4.61s/it] 51%|█████     | 138/270 [03:43<07:43,  3.51s/it] 51%|█████▏    | 139/270 [03:43<05:58,  2.74s/it] 52%|█████▏    | 140/270 [03:44<04:46,  2.20s/it] 52%|█████▏    | 141/270 [03:45<03:55,  1.82s/it] 53%|█████▎    | 142/270 [03:46<03:19,  1.56s/it] 53%|█████▎    | 143/270 [03:47<02:54,  1.37s/it] 53%|█████▎    | 144/270 [03:48<02:36,  1.24s/it] 54%|█████▎    | 145/270 [03:49<02:23,  1.15s/it] 54%|█████▍    | 146/270 [03:50<02:14,  1.09s/it] 54%|█████▍    | 147/270 [03:51<02:08,  1.05s/it] 55%|█████▍    | 148/270 [03:52<02:03,  1.02s/it] 55%|█████▌    | 149/270 [03:53<02:00,  1.00it/s] 56%|█████▌    | 150/270 [03:54<01:58,  1.02it/s]                                                  56%|█████▌    | 150/270 [03:54<01:58,  1.02it/s] 56%|█████▌    | 151/270 [03:55<01:55,  1.03it/s] 56%|█████▋    | 152/270 [03:56<01:54,  1.03it/s] 57%|█████▋    | 153/270 [03:57<01:52,  1.04it/s] 57%|█████▋    | 154/270 [03:58<01:50,  1.05it/s] 57%|█████▋    | 155/270 [03:59<01:49,  1.05it/s] 58%|█████▊    | 156/270 [04:00<01:48,  1.05it/s] 58%|█████▊    | 157/270 [04:00<01:47,  1.05it/s] 59%|█████▊    | 158/270 [04:01<01:46,  1.05it/s] 59%|█████▉    | 159/270 [04:02<01:45,  1.05it/s] 59%|█████▉    | 160/270 [04:03<01:44,  1.05it/s] 60%|█████▉    | 161/270 [04:04<01:43,  1.05it/s] 60%|██████    | 162/270 [04:05<01:31,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.22039377689361572, 'eval_f1': 0.9444674497470859, 'eval_runtime': 2.6606, 'eval_samples_per_second': 160.115, 'eval_steps_per_second': 2.631, 'epoch': 5.0}
{'loss': 0.2331, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.99it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.63it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.40it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  2.68it/s][A
100%|██████████| 7/7 [00:02<00:00,  3.04it/s][A                                                 
                                             [A 60%|██████    | 162/270 [04:08<01:31,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.04it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████    | 163/270 [04:26<12:21,  6.93s/it] 61%|██████    | 164/270 [04:27<09:04,  5.13s/it] 61%|██████    | 165/270 [04:28<06:46,  3.88s/it] 61%|██████▏   | 166/270 [04:29<05:11,  2.99s/it] 62%|██████▏   | 167/270 [04:30<04:05,  2.38s/it] 62%|██████▏   | 168/270 [04:31<03:18,  1.95s/it] 63%|██████▎   | 169/270 [04:32<02:46,  1.65s/it] 63%|██████▎   | 170/270 [04:33<02:23,  1.44s/it] 63%|██████▎   | 171/270 [04:34<02:07,  1.29s/it] 64%|██████▎   | 172/270 [04:34<01:56,  1.18s/it] 64%|██████▍   | 173/270 [04:35<01:47,  1.11s/it] 64%|██████▍   | 174/270 [04:36<01:41,  1.06s/it] 65%|██████▍   | 175/270 [04:37<01:37,  1.02s/it] 65%|██████▌   | 176/270 [04:38<01:34,  1.00s/it] 66%|██████▌   | 177/270 [04:39<01:31,  1.01it/s] 66%|██████▌   | 178/270 [04:40<01:29,  1.03it/s] 66%|██████▋   | 179/270 [04:41<01:27,  1.04it/s] 67%|██████▋   | 180/270 [04:42<01:26,  1.04it/s] 67%|██████▋   | 181/270 [04:43<01:25,  1.05it/s] 67%|██████▋   | 182/270 [04:44<01:23,  1.05it/s] 68%|██████▊   | 183/270 [04:45<01:22,  1.05it/s] 68%|██████▊   | 184/270 [04:46<01:21,  1.05it/s] 69%|██████▊   | 185/270 [04:47<01:20,  1.05it/s] 69%|██████▉   | 186/270 [04:48<01:19,  1.05it/s] 69%|██████▉   | 187/270 [04:49<01:18,  1.05it/s] 70%|██████▉   | 188/270 [04:50<01:17,  1.05it/s] 70%|███████   | 189/270 [04:50<01:08,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.20526793599128723, 'eval_f1': 0.9506353908087654, 'eval_runtime': 2.9077, 'eval_samples_per_second': 146.509, 'eval_steps_per_second': 2.407, 'epoch': 6.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.01it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.20it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.67it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.41it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.24it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.52it/s][A                                                 
                                             [A 70%|███████   | 189/270 [04:53<01:08,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.52it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|███████   | 190/270 [05:08<08:02,  6.03s/it] 71%|███████   | 191/270 [05:09<05:56,  4.51s/it] 71%|███████   | 192/270 [05:10<04:27,  3.43s/it] 71%|███████▏  | 193/270 [05:11<03:26,  2.69s/it] 72%|███████▏  | 194/270 [05:12<02:44,  2.16s/it] 72%|███████▏  | 195/270 [05:13<02:14,  1.80s/it] 73%|███████▎  | 196/270 [05:14<01:54,  1.54s/it] 73%|███████▎  | 197/270 [05:15<01:39,  1.37s/it] 73%|███████▎  | 198/270 [05:16<01:29,  1.24s/it] 74%|███████▎  | 199/270 [05:17<01:22,  1.16s/it] 74%|███████▍  | 200/270 [05:18<01:16,  1.09s/it]                                                  74%|███████▍  | 200/270 [05:18<01:16,  1.09s/it] 74%|███████▍  | 201/270 [05:19<01:12,  1.05s/it] 75%|███████▍  | 202/270 [05:20<01:09,  1.02s/it] 75%|███████▌  | 203/270 [05:21<01:06,  1.00it/s] 76%|███████▌  | 204/270 [05:22<01:05,  1.01it/s] 76%|███████▌  | 205/270 [05:23<01:03,  1.02it/s] 76%|███████▋  | 206/270 [05:24<01:01,  1.03it/s] 77%|███████▋  | 207/270 [05:24<01:00,  1.04it/s] 77%|███████▋  | 208/270 [05:25<00:59,  1.04it/s] 77%|███████▋  | 209/270 [05:26<00:58,  1.04it/s] 78%|███████▊  | 210/270 [05:27<00:57,  1.04it/s] 78%|███████▊  | 211/270 [05:28<00:56,  1.04it/s] 79%|███████▊  | 212/270 [05:29<00:55,  1.04it/s] 79%|███████▉  | 213/270 [05:30<00:54,  1.04it/s] 79%|███████▉  | 214/270 [05:31<00:53,  1.04it/s] 80%|███████▉  | 215/270 [05:32<00:52,  1.04it/s] 80%|████████  | 216/270 [05:33<00:46,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.18442843854427338, 'eval_f1': 0.952654149616252, 'eval_runtime': 2.6684, 'eval_samples_per_second': 159.646, 'eval_steps_per_second': 2.623, 'epoch': 7.0}
{'loss': 0.1422, 'learning_rate': 1.5687918106563326e-06, 'epoch': 7.41}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.01it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.60it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.38it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.48it/s][A                                                 
                                             [A 80%|████████  | 216/270 [05:36<00:46,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.48it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-216
Configuration saved in training-output/checkpoint-216/config.json
Model weights saved in training-output/checkpoint-216/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-216/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-216/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████  | 217/270 [05:53<05:47,  6.55s/it] 81%|████████  | 218/270 [05:54<04:12,  4.86s/it] 81%|████████  | 219/270 [05:54<03:07,  3.69s/it] 81%|████████▏ | 220/270 [05:55<02:23,  2.86s/it] 82%|████████▏ | 221/270 [05:56<01:51,  2.29s/it] 82%|████████▏ | 222/270 [05:57<01:30,  1.88s/it] 83%|████████▎ | 223/270 [05:58<01:15,  1.60s/it] 83%|████████▎ | 224/270 [05:59<01:04,  1.40s/it] 83%|████████▎ | 225/270 [06:00<00:56,  1.26s/it] 84%|████████▎ | 226/270 [06:01<00:51,  1.17s/it] 84%|████████▍ | 227/270 [06:02<00:47,  1.10s/it] 84%|████████▍ | 228/270 [06:03<00:44,  1.05s/it] 85%|████████▍ | 229/270 [06:04<00:41,  1.02s/it] 85%|████████▌ | 230/270 [06:05<00:39,  1.00it/s] 86%|████████▌ | 231/270 [06:06<00:38,  1.01it/s] 86%|████████▌ | 232/270 [06:07<00:36,  1.03it/s] 86%|████████▋ | 233/270 [06:08<00:35,  1.03it/s] 87%|████████▋ | 234/270 [06:09<00:34,  1.04it/s] 87%|████████▋ | 235/270 [06:10<00:33,  1.05it/s] 87%|████████▋ | 236/270 [06:11<00:32,  1.05it/s] 88%|████████▊ | 237/270 [06:11<00:31,  1.05it/s] 88%|████████▊ | 238/270 [06:12<00:30,  1.05it/s] 89%|████████▊ | 239/270 [06:13<00:29,  1.05it/s] 89%|████████▉ | 240/270 [06:14<00:28,  1.05it/s] 89%|████████▉ | 241/270 [06:15<00:27,  1.05it/s] 90%|████████▉ | 242/270 [06:16<00:26,  1.05it/s] 90%|█████████ | 243/270 [06:17<00:22,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.18605174124240875, 'eval_f1': 0.952654149616252, 'eval_runtime': 2.8763, 'eval_samples_per_second': 148.107, 'eval_steps_per_second': 2.434, 'epoch': 8.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.00it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.21it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.62it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.35it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.23it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.50it/s][A                                                 
                                             [A 90%|█████████ | 243/270 [06:20<00:22,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.50it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-243
Configuration saved in training-output/checkpoint-243/config.json
Model weights saved in training-output/checkpoint-243/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-243/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-243/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|█████████ | 244/270 [06:35<02:36,  6.02s/it] 91%|█████████ | 245/270 [06:36<01:52,  4.50s/it] 91%|█████████ | 246/270 [06:37<01:22,  3.43s/it] 91%|█████████▏| 247/270 [06:38<01:01,  2.69s/it] 92%|█████████▏| 248/270 [06:39<00:47,  2.16s/it] 92%|█████████▏| 249/270 [06:40<00:37,  1.80s/it] 93%|█████████▎| 250/270 [06:41<00:30,  1.54s/it]                                                  93%|█████████▎| 250/270 [06:41<00:30,  1.54s/it] 93%|█████████▎| 251/270 [06:42<00:25,  1.37s/it] 93%|█████████▎| 252/270 [06:43<00:22,  1.24s/it] 94%|█████████▎| 253/270 [06:43<00:19,  1.15s/it] 94%|█████████▍| 254/270 [06:44<00:17,  1.09s/it] 94%|█████████▍| 255/270 [06:45<00:15,  1.05s/it] 95%|█████████▍| 256/270 [06:46<00:14,  1.02s/it] 95%|█████████▌| 257/270 [06:47<00:12,  1.00it/s] 96%|█████████▌| 258/270 [06:48<00:11,  1.02it/s] 96%|█████████▌| 259/270 [06:49<00:10,  1.03it/s] 96%|█████████▋| 260/270 [06:50<00:09,  1.04it/s] 97%|█████████▋| 261/270 [06:51<00:08,  1.04it/s] 97%|█████████▋| 262/270 [06:52<00:07,  1.04it/s] 97%|█████████▋| 263/270 [06:53<00:06,  1.04it/s] 98%|█████████▊| 264/270 [06:54<00:05,  1.04it/s] 98%|█████████▊| 265/270 [06:55<00:04,  1.05it/s] 99%|█████████▊| 266/270 [06:56<00:03,  1.05it/s] 99%|█████████▉| 267/270 [06:57<00:02,  1.05it/s] 99%|█████████▉| 268/270 [06:58<00:01,  1.05it/s]100%|█████████▉| 269/270 [06:59<00:00,  1.05it/s]100%|██████████| 270/270 [06:59<00:00,  1.17it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.18840624392032623, 'eval_f1': 0.9531284502028603, 'eval_runtime': 2.682, 'eval_samples_per_second': 158.839, 'eval_steps_per_second': 2.61, 'epoch': 9.0}
{'loss': 0.1217, 'learning_rate': 1.3477564710088097e-07, 'epoch': 9.26}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.98it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.16it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.65it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.38it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.22it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.47it/s][A                                                 
                                             [A100%|██████████| 270/270 [07:02<00:00,  1.17it/s]
100%|██████████| 7/7 [00:02<00:00,  3.47it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-270
Configuration saved in training-output/checkpoint-270/config.json
Model weights saved in training-output/checkpoint-270/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-270/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-270/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-270 (score: 0.9555446199989825).
                                                 100%|██████████| 270/270 [07:18<00:00,  1.17it/s]100%|██████████| 270/270 [07:18<00:00,  1.62s/it]
{'eval_loss': 0.18775367736816406, 'eval_f1': 0.9555446199989825, 'eval_runtime': 2.698, 'eval_samples_per_second': 157.895, 'eval_steps_per_second': 2.595, 'epoch': 10.0}
{'train_runtime': 438.1347, 'train_samples_per_second': 38.801, 'train_steps_per_second': 0.616, 'train_loss': 0.45898440855520745, 'epoch': 10.0}
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:06, 16.64it/s]  4%|▎         | 4/107 [00:00<00:06, 16.93it/s]  7%|▋         | 7/107 [00:00<00:04, 21.76it/s]  9%|▉         | 10/107 [00:00<00:04, 24.19it/s] 12%|█▏        | 13/107 [00:00<00:03, 25.63it/s] 15%|█▍        | 16/107 [00:00<00:03, 26.46it/s] 18%|█▊        | 19/107 [00:00<00:03, 26.84it/s] 21%|██        | 22/107 [00:00<00:03, 27.14it/s] 23%|██▎       | 25/107 [00:00<00:02, 27.42it/s] 26%|██▌       | 28/107 [00:01<00:02, 27.55it/s] 29%|██▉       | 31/107 [00:01<00:02, 27.29it/s] 32%|███▏      | 34/107 [00:01<00:02, 27.39it/s] 35%|███▍      | 37/107 [00:01<00:02, 27.45it/s] 37%|███▋      | 40/107 [00:01<00:02, 27.63it/s] 40%|████      | 43/107 [00:01<00:02, 27.81it/s] 43%|████▎     | 46/107 [00:01<00:02, 27.88it/s] 46%|████▌     | 49/107 [00:01<00:02, 27.88it/s] 49%|████▊     | 52/107 [00:01<00:01, 27.96it/s] 51%|█████▏    | 55/107 [00:02<00:01, 27.78it/s] 54%|█████▍    | 58/107 [00:02<00:01, 27.63it/s] 57%|█████▋    | 61/107 [00:02<00:01, 27.57it/s] 60%|█████▉    | 64/107 [00:02<00:01, 27.60it/s] 63%|██████▎   | 67/107 [00:02<00:01, 27.70it/s] 65%|██████▌   | 70/107 [00:02<00:01, 27.70it/s] 68%|██████▊   | 73/107 [00:02<00:01, 27.76it/s] 71%|███████   | 76/107 [00:02<00:01, 27.84it/s] 74%|███████▍  | 79/107 [00:02<00:01, 27.88it/s] 77%|███████▋  | 82/107 [00:03<00:00, 27.85it/s] 79%|███████▉  | 85/107 [00:03<00:00, 27.78it/s] 82%|████████▏ | 88/107 [00:03<00:00, 27.73it/s] 85%|████████▌ | 91/107 [00:03<00:00, 27.58it/s] 88%|████████▊ | 94/107 [00:03<00:00, 27.65it/s] 91%|█████████ | 97/107 [00:03<00:00, 27.73it/s] 93%|█████████▎| 100/107 [00:03<00:00, 27.79it/s] 96%|█████████▋| 103/107 [00:03<00:00, 27.74it/s] 99%|█████████▉| 106/107 [00:03<00:00, 27.80it/s]100%|██████████| 107/107 [00:03<00:00, 27.23it/s]
Model finished with accuracy: 0.9553990610328639, macro-f1: 0.9555446199989825
Confusion matrix:
[[ 72   0   0   1   2]
 [  0  74   1   0   0]
 [  0   1  62   1   0]
 [  0   1   6 110   0]
 [  2   1   0   3  89]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
14_mx46s0fx52.csv, 58_lvcsk0yy8g.csv, 133_c0kk72b3fm.csv, 132_s5i7ngx7bu.csv, 112_7g35kmtkhr.csv, 101_9l21auewdi.csv, 69_4tlegp20n1.csv, 35_g0ldvn0rjm.csv, 143_w11kkydg7n.csv, 20_1zahygarmq.csv, 53_4t2mju8m3y.csv, 166_5lv4sjg1qv.csv, 105_3acmrslh5t.csv, 198_shm86f7xzz.csv, 155_97dannvd9q.csv, 189_4mh7pjb6hj.csv, 187_kp3lfo9rti.csv, 3_5uxjfwf4c9.csv, 76_f4jqlrtl0d.csv, 152_uhh1hem3ei.csv, 14_apv4u3qebs.csv, 127_q2ifamcjop.csv, 100_5397borrxc.csv, 129_h8c8ny4x7u.csv, 123_g66af1qsqo.csv, 95_6ikda3av09.csv, 42_kr1ailiclk.csv, 162_gzcq0eli5q.csv, 148_nekma0liim.csv, 50_tmt7xio73x.csv, 183_ex67vzlp8e.csv, 64_gjf3ihxyfj.csv, 182_icy94tmqbq.csv, 96_fe55r9bnfd.csv, 37_7swqi7vgnl.csv, 17_qvtzqld86i.csv, 185_9bjzxvbwca.csv, 4_vet6mgh5md.csv, 109_ehl2s3ah2w.csv, 36_4bg4d4bf4q.csv, 165_vcamdtt7pl.csv, 136_dd735psc8g.csv, 24_1y633kulxy.csv, 7_swfaqmk6y6.csv, 46_eg7yytwm57.csv, 12_i0qb55ygnr.csv, 8_v4bqc7mvzy.csv, 120_tfdaxlk9n2.csv, 103_v129kmq3c5.csv, 144_6g4qagf0aw.csv, 72_37k8qe6uf3.csv, 111_4dftcjzv4j.csv, 1_z949c1i541.csv, 178_7rflrddsnn.csv, 6_blz9uaf562.csv, 15_j7tl3bmirm.csv, 83_slgfcmc4xj.csv, 174_vw5a8s9vzj.csv, 9_m11ut0fzvo.csv, 199_hzu91su6jo.csv, 39_3064xz3lgl.csv, 62_6v7alh2xza.csv, 137_eusnnkienm.csv, 118_66k0pevslq.csv, 113_s8v38lnghb.csv, 90_60znzzhvt3.csv, 161_qu7m9mil3a.csv, 32_jitoqenvci.csv, 176_cl4orgir7l.csv, 65_5y9bhpar44.csv, 27_adgb2e3ry7.csv, 33_zybtjc12ef.csv, 140_rpfb54del9.csv, 70_0b98yn7zuy.csv, 158_lhrhfgkazf.csv, 10_ten3l2nuwn.csv, 48_u6mx1k6p64.csv, 29_juqgc9mrcx.csv, 195_nwk7f5mq3h.csv, 119_hdkgwx4tgr.csv, 163_g4ownm07q9.csv, 134_behtf6grhp.csv, 128_y3nqk9k0x4.csv, 93_7s7jo61qgr.csv, 125_onkofvwzgk.csv, 196_h8ebmn0b4i.csv, 186_77g5px55z1.csv, 80_zbkdixjx1u.csv, 154_i7n85wzpa3.csv, 59_tvvx1filo9.csv, 91_6nvvo85k2r.csv, 26_kcs08kmlzv.csv, 146_umxfdzx3sg.csv, 157_7onih683qj.csv, 122_hmocfdllhz.csv, 102_r9vbr9i6qn.csv, 25_zdmhvwm1pk.csv, 16_omnxwedc88.csv, 116_wf72qb1ctj.csv, 87_zzzdc841sd.csv, 168_39vz7oesxk.csv, 24_pa58yc2bsn.csv, 51_km3hwahw1m.csv, 19_sevtqojtws.csv, 21_x8u5ofrlzo.csv, 75_gginvgfabf.csv, 45_4g2awfbbtn.csv, 4_xu9afrhg0o.csv, 25_ceghr7ntgk.csv, 15_jzm9dvtw73.csv, 89_28gvnlaz8j.csv, 57_l8ux51os5l.csv, 153_mwu7z89cr2.csv, 147_qz39am2soc.csv, 67_ip2ys2xhib.csv, 78_tshg4zfas1.csv, 142_d8jaik03id.csv, 5_wa4f0flvdd.csv, 181_wk573tzjde.csv, 193_kjtylxs5x6.csv, 34_egy98kczu6.csv, 85_i7f88e7b7g.csv, 106_oa27xk4eqt.csv, 156_yk739g3286.csv, 11_0na6xsa4cd.csv, 31_jhmyyhmjh5.csv, 149_tvntt1n3zf.csv, 6_qyv4vhzoe.csv, 63_j79b3nmr83.csv, 104_jos9y36cxr.csv, 151_8k3mbw3phz.csv, 60_zry6xyk2v2.csv, 86_kvt6l8gw8d.csv, 131_bivn5or23h.csv, 175_954mvwcwet.csv, 110_zq7vq985bq.csv, 52_wfyl6gdoxn.csv, 92_u8wrf1zhh5.csv, 71_jjxizibivj.csv, 169_2y8d92hhek.csv, 114_fkhid5yyst.csv, 173_op7k51t6eh.csv, 145_1p84lqxs5t.csv, 171_evt2lbu5wz.csv, 2_wrrttduqx0.csv, 107_jfii3uf345.csv, 164_qcoq7yni4y.csv, 160_9xq61i8z5q.csv, 150_hvfxdsp6uq.csv, 16_j1uppkkwpc.csv, 26_2k5thw2ykv.csv, 172_gg34wiyfxn.csv, 27_r6anjd4ie1.csv, 11_7vk8oa1o24.csv, 138_yx53ai19ar.csv, 44_zsqcy1dp0n.csv, 68_4fxf0cl1sa.csv, 77_gup8yvrjg8.csv, 23_lce4ue1yit.csv, 197_ni25hikvxf.csv, 55_o97cc78tyd.csv, 13_xb7ph2k0m6.csv, 126_s6vu9wsf2o.csv, 124_4p9vzltfyn.csv, 135_7g73f18y8h.csv, 194_tnwwh3zbnj.csv, 99_7fcky2iv54.csv, 49_8fcz4c6oqy.csv, 84_q4rh6yklie.csv, 28_7svph4yobz.csv, 38_38doqz3unu.csv, 5_lcampqa5d8.csv, 82_3qr5zxdcxe.csv, 180_jm6cn0n08c.csv, 66_8gquz8hbpz.csv, 40_zh8fjx6tiy.csv, 141_tvldgnl6pt.csv, 30_rb578trmsj.csv, 184_hzlbex83q3.csv, 56_lxflttkcgn.csv, 22_udsmb087i5.csv, 8_hfxm8dkunn.csv, 61_l3jkp84irv.csv, 139_1qp7vpj70o.csv, 73_4s2wt6mgps.csv, 7_viucp080gt.csv, 108_qkxdz8djgt.csv, 54_07djfakfxh.csv, 98_8et5hi6nqf.csv, 74_08dh1ae49m.csv, 21_x90g7ie9t7.csv, 81_dbvr7ry56o.csv, 121_aobrexg5sv.csv, 191_yxm7fc16uc.csv, 94_gcso78g1mh.csv, 167_bk26q01ex6.csv, 41_rrwls6y9fl.csv, 170_szpuak9x3b.csv, 47_qrdef2nnjn.csv, 13_y1i6zh8y9w.csv, 188_5sjzeb4w03.csv, 130_7a6m1my07s.csv, 192_isa0drpmu4.csv, 79_wkvkhcnsn0.csv, 2_9actgs9mi7.csv, 18_o65lxoj85y.csv, 3_j0hn3a0xzt.csv, 43_1nemhlm74b.csv, 20_z3zjd5sgm4.csv, 117_mgq0ymf5hc.csv, 0_imfn3fsnqb.csv, 88_p169fqr8hl.csv, 1_ofoaso8yxd.csv, 97_ksnb8zg7e2.csv, 159_wmf9y5lb3d.csv, 179_enmmbxkd9b.csv, 18_omkydub7qy.csv, 115_iocn2dpj2o.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|██████████| 2/2 [00:00<00:00, 15.44ba/s]100%|██████████| 2/2 [00:00<00:00, 15.39ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|██████████| 1/1 [00:00<00:00, 42.08ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:04<21:24,  4.78s/it]  1%|          | 2/270 [00:05<11:09,  2.50s/it]  1%|          | 3/270 [00:06<07:53,  1.77s/it]  1%|▏         | 4/270 [00:07<06:20,  1.43s/it]  2%|▏         | 5/270 [00:08<05:29,  1.24s/it]  2%|▏         | 6/270 [00:09<04:59,  1.13s/it]  3%|▎         | 7/270 [00:10<04:38,  1.06s/it]  3%|▎         | 8/270 [00:11<04:25,  1.01s/it]  3%|▎         | 9/270 [00:12<04:16,  1.02it/s]  4%|▎         | 10/270 [00:12<04:09,  1.04it/s]  4%|▍         | 11/270 [00:13<04:05,  1.06it/s]  4%|▍         | 12/270 [00:14<04:01,  1.07it/s]  5%|▍         | 13/270 [00:16<04:22,  1.02s/it]  5%|▌         | 14/270 [00:16<04:13,  1.01it/s]  6%|▌         | 15/270 [00:17<04:07,  1.03it/s]  6%|▌         | 16/270 [00:18<04:03,  1.04it/s]  6%|▋         | 17/270 [00:19<03:59,  1.06it/s]  7%|▋         | 18/270 [00:20<03:57,  1.06it/s]  7%|▋         | 19/270 [00:21<03:55,  1.07it/s]  7%|▋         | 20/270 [00:22<03:53,  1.07it/s]  8%|▊         | 21/270 [00:23<03:51,  1.08it/s]  8%|▊         | 22/270 [00:24<03:50,  1.07it/s]  9%|▊         | 23/270 [00:25<03:50,  1.07it/s]  9%|▉         | 24/270 [00:26<03:49,  1.07it/s]  9%|▉         | 25/270 [00:27<03:46,  1.08it/s] 10%|▉         | 26/270 [00:28<03:46,  1.08it/s] 10%|█         | 27/270 [00:28<03:21,  1.21it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.20it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.38it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.79it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.52it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.31it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.55it/s][A                                                
                                             [A 10%|█         | 27/270 [00:31<03:21,  1.21it/s]
100%|██████████| 7/7 [00:02<00:00,  3.55it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|█         | 28/270 [00:48<26:33,  6.58s/it] 11%|█         | 29/270 [00:49<19:36,  4.88s/it] 11%|█         | 30/270 [00:50<14:46,  3.69s/it] 11%|█▏        | 31/270 [00:51<11:23,  2.86s/it] 12%|█▏        | 32/270 [00:52<09:02,  2.28s/it] 12%|█▏        | 33/270 [00:53<07:23,  1.87s/it] 13%|█▎        | 34/270 [00:54<06:14,  1.59s/it] 13%|█▎        | 35/270 [00:55<05:25,  1.39s/it] 13%|█▎        | 36/270 [00:56<04:52,  1.25s/it] 14%|█▎        | 37/270 [00:56<04:29,  1.15s/it] 14%|█▍        | 38/270 [00:57<04:11,  1.09s/it] 14%|█▍        | 39/270 [00:58<03:59,  1.04s/it] 15%|█▍        | 40/270 [00:59<03:51,  1.01s/it] 15%|█▌        | 41/270 [01:00<03:44,  1.02it/s] 16%|█▌        | 42/270 [01:01<03:40,  1.03it/s] 16%|█▌        | 43/270 [01:02<03:36,  1.05it/s] 16%|█▋        | 44/270 [01:03<03:34,  1.05it/s] 17%|█▋        | 45/270 [01:04<03:32,  1.06it/s] 17%|█▋        | 46/270 [01:05<03:29,  1.07it/s] 17%|█▋        | 47/270 [01:06<03:28,  1.07it/s] 18%|█▊        | 48/270 [01:07<03:27,  1.07it/s] 18%|█▊        | 49/270 [01:08<03:26,  1.07it/s] 19%|█▊        | 50/270 [01:09<03:26,  1.07it/s]                                                 19%|█▊        | 50/270 [01:09<03:26,  1.07it/s] 19%|█▉        | 51/270 [01:10<03:25,  1.07it/s] 19%|█▉        | 52/270 [01:10<03:23,  1.07it/s] 20%|█▉        | 53/270 [01:11<03:23,  1.07it/s] 20%|██        | 54/270 [01:12<02:59,  1.20it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 1.3112415075302124, 'eval_f1': 0.4478769546316716, 'eval_runtime': 2.8299, 'eval_samples_per_second': 150.537, 'eval_steps_per_second': 2.474, 'epoch': 1.0}
{'loss': 1.3107, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.13it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.36it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.75it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.48it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.32it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.59it/s][A                                                
                                             [A 20%|██        | 54/270 [01:15<02:59,  1.20it/s]
100%|██████████| 7/7 [00:02<00:00,  3.59it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|██        | 55/270 [01:33<24:27,  6.82s/it] 21%|██        | 56/270 [01:34<18:01,  5.05s/it] 21%|██        | 57/270 [01:35<13:32,  3.81s/it] 21%|██▏       | 58/270 [01:36<10:24,  2.95s/it] 22%|██▏       | 59/270 [01:36<08:14,  2.34s/it] 22%|██▏       | 60/270 [01:37<06:42,  1.92s/it] 23%|██▎       | 61/270 [01:38<05:38,  1.62s/it] 23%|██▎       | 62/270 [01:39<04:54,  1.41s/it] 23%|██▎       | 63/270 [01:40<04:22,  1.27s/it] 24%|██▎       | 64/270 [01:41<04:00,  1.17s/it] 24%|██▍       | 65/270 [01:42<03:45,  1.10s/it] 24%|██▍       | 66/270 [01:43<03:33,  1.05s/it] 25%|██▍       | 67/270 [01:44<03:25,  1.01s/it] 25%|██▌       | 68/270 [01:45<03:19,  1.01it/s] 26%|██▌       | 69/270 [01:46<03:15,  1.03it/s] 26%|██▌       | 70/270 [01:47<03:11,  1.04it/s] 26%|██▋       | 71/270 [01:48<03:09,  1.05it/s] 27%|██▋       | 72/270 [01:49<03:07,  1.06it/s] 27%|██▋       | 73/270 [01:50<03:06,  1.06it/s] 27%|██▋       | 74/270 [01:50<03:04,  1.06it/s] 28%|██▊       | 75/270 [01:51<03:03,  1.06it/s] 28%|██▊       | 76/270 [01:52<03:02,  1.06it/s] 29%|██▊       | 77/270 [01:53<03:01,  1.06it/s] 29%|██▉       | 78/270 [01:54<03:00,  1.07it/s] 29%|██▉       | 79/270 [01:55<02:59,  1.07it/s] 30%|██▉       | 80/270 [01:56<02:58,  1.06it/s] 30%|███       | 81/270 [01:57<02:38,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.6286694407463074, 'eval_f1': 0.7972210983832992, 'eval_runtime': 2.6363, 'eval_samples_per_second': 161.588, 'eval_steps_per_second': 2.655, 'epoch': 2.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.25it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.31it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.72it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.30it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.59it/s][A                                                
                                             [A 30%|███       | 81/270 [01:59<02:38,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.59it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|███       | 82/270 [02:15<19:19,  6.17s/it] 31%|███       | 83/270 [02:16<14:19,  4.60s/it] 31%|███       | 84/270 [02:17<10:50,  3.50s/it] 31%|███▏      | 85/270 [02:18<08:24,  2.73s/it] 32%|███▏      | 86/270 [02:19<06:42,  2.19s/it] 32%|███▏      | 87/270 [02:20<05:31,  1.81s/it] 33%|███▎      | 88/270 [02:21<04:42,  1.55s/it] 33%|███▎      | 89/270 [02:22<04:06,  1.36s/it] 33%|███▎      | 90/270 [02:23<03:43,  1.24s/it] 34%|███▎      | 91/270 [02:24<03:25,  1.15s/it] 34%|███▍      | 92/270 [02:25<03:13,  1.08s/it] 34%|███▍      | 93/270 [02:26<03:03,  1.04s/it] 35%|███▍      | 94/270 [02:26<02:57,  1.01s/it] 35%|███▌      | 95/270 [02:27<02:52,  1.01it/s] 36%|███▌      | 96/270 [02:28<02:49,  1.03it/s] 36%|███▌      | 97/270 [02:29<02:46,  1.04it/s] 36%|███▋      | 98/270 [02:30<02:43,  1.05it/s] 37%|███▋      | 99/270 [02:31<02:42,  1.05it/s] 37%|███▋      | 100/270 [02:32<02:40,  1.06it/s]                                                  37%|███▋      | 100/270 [02:32<02:40,  1.06it/s] 37%|███▋      | 101/270 [02:33<02:39,  1.06it/s] 38%|███▊      | 102/270 [02:34<02:39,  1.06it/s] 38%|███▊      | 103/270 [02:35<02:37,  1.06it/s] 39%|███▊      | 104/270 [02:36<02:36,  1.06it/s] 39%|███▉      | 105/270 [02:37<02:35,  1.06it/s] 39%|███▉      | 106/270 [02:38<02:34,  1.06it/s] 40%|███▉      | 107/270 [02:39<02:33,  1.06it/s] 40%|████      | 108/270 [02:39<02:16,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.37858888506889343, 'eval_f1': 0.889846961622341, 'eval_runtime': 2.6504, 'eval_samples_per_second': 160.731, 'eval_steps_per_second': 2.641, 'epoch': 3.0}
{'loss': 0.4538, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.13it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.29it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.72it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.29it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                 
                                             [A 40%|████      | 108/270 [02:42<02:16,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|████      | 109/270 [02:58<16:57,  6.32s/it] 41%|████      | 110/270 [02:59<12:32,  4.70s/it] 41%|████      | 111/270 [03:00<09:27,  3.57s/it] 41%|████▏     | 112/270 [03:01<07:19,  2.78s/it] 42%|████▏     | 113/270 [03:02<05:49,  2.23s/it] 42%|████▏     | 114/270 [03:03<04:46,  1.84s/it] 43%|████▎     | 115/270 [03:04<04:02,  1.57s/it] 43%|████▎     | 116/270 [03:05<03:31,  1.37s/it] 43%|████▎     | 117/270 [03:06<03:09,  1.24s/it] 44%|████▎     | 118/270 [03:07<02:54,  1.15s/it] 44%|████▍     | 119/270 [03:08<02:43,  1.08s/it] 44%|████▍     | 120/270 [03:09<02:35,  1.04s/it] 45%|████▍     | 121/270 [03:10<02:30,  1.01s/it] 45%|████▌     | 122/270 [03:11<02:26,  1.01it/s] 46%|████▌     | 123/270 [03:11<02:22,  1.03it/s] 46%|████▌     | 124/270 [03:12<02:20,  1.04it/s] 46%|████▋     | 125/270 [03:13<02:18,  1.05it/s] 47%|████▋     | 126/270 [03:14<02:16,  1.05it/s] 47%|████▋     | 127/270 [03:15<02:15,  1.06it/s] 47%|████▋     | 128/270 [03:16<02:14,  1.06it/s] 48%|████▊     | 129/270 [03:17<02:21,  1.00s/it] 48%|████▊     | 130/270 [03:18<02:18,  1.01it/s] 49%|████▊     | 131/270 [03:19<02:15,  1.03it/s] 49%|████▉     | 132/270 [03:20<02:12,  1.04it/s] 49%|████▉     | 133/270 [03:21<02:10,  1.05it/s] 50%|████▉     | 134/270 [03:22<02:09,  1.05it/s] 50%|█████     | 135/270 [03:23<01:54,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.2660997807979584, 'eval_f1': 0.9330575327642242, 'eval_runtime': 2.6598, 'eval_samples_per_second': 160.164, 'eval_steps_per_second': 2.632, 'epoch': 4.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.12it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.27it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.67it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.28it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.56it/s][A                                                 
                                             [A 50%|█████     | 135/270 [03:25<01:54,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|█████     | 136/270 [03:42<14:15,  6.38s/it] 51%|█████     | 137/270 [03:43<10:31,  4.75s/it] 51%|█████     | 138/270 [03:44<07:55,  3.61s/it] 51%|█████▏    | 139/270 [03:45<06:07,  2.81s/it] 52%|█████▏    | 140/270 [03:46<04:51,  2.24s/it] 52%|█████▏    | 141/270 [03:47<03:58,  1.85s/it] 53%|█████▎    | 142/270 [03:48<03:21,  1.57s/it] 53%|█████▎    | 143/270 [03:48<02:55,  1.38s/it] 53%|█████▎    | 144/270 [03:49<02:37,  1.25s/it] 54%|█████▎    | 145/270 [03:50<02:24,  1.15s/it] 54%|█████▍    | 146/270 [03:51<02:14,  1.09s/it] 54%|█████▍    | 147/270 [03:52<02:08,  1.04s/it] 55%|█████▍    | 148/270 [03:53<02:03,  1.01s/it] 55%|█████▌    | 149/270 [03:54<01:59,  1.01it/s] 56%|█████▌    | 150/270 [03:55<01:56,  1.03it/s]                                                  56%|█████▌    | 150/270 [03:55<01:56,  1.03it/s] 56%|█████▌    | 151/270 [03:56<01:54,  1.04it/s] 56%|█████▋    | 152/270 [03:57<01:53,  1.04it/s] 57%|█████▋    | 153/270 [03:58<01:51,  1.05it/s] 57%|█████▋    | 154/270 [03:59<01:50,  1.05it/s] 57%|█████▋    | 155/270 [04:00<01:48,  1.06it/s] 58%|█████▊    | 156/270 [04:01<01:47,  1.06it/s] 58%|█████▊    | 157/270 [04:02<01:46,  1.06it/s] 59%|█████▊    | 158/270 [04:03<01:45,  1.06it/s] 59%|█████▉    | 159/270 [04:03<01:44,  1.06it/s] 59%|█████▉    | 160/270 [04:04<01:43,  1.06it/s] 60%|█████▉    | 161/270 [04:05<01:43,  1.06it/s] 60%|██████    | 162/270 [04:06<01:31,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.21841225028038025, 'eval_f1': 0.9509714975507075, 'eval_runtime': 2.7842, 'eval_samples_per_second': 153.009, 'eval_steps_per_second': 2.514, 'epoch': 5.0}
{'loss': 0.2059, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.01it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.19it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.67it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.29it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                 
                                             [A 60%|██████    | 162/270 [04:09<01:31,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|██████    | 163/270 [04:28<12:37,  7.08s/it] 61%|██████    | 164/270 [04:29<09:14,  5.24s/it] 61%|██████    | 165/270 [04:29<06:54,  3.94s/it] 61%|██████▏   | 166/270 [04:30<05:16,  3.04s/it] 62%|██████▏   | 167/270 [04:31<04:07,  2.41s/it] 62%|██████▏   | 168/270 [04:32<03:20,  1.96s/it] 63%|██████▎   | 169/270 [04:33<02:47,  1.65s/it] 63%|██████▎   | 170/270 [04:34<02:23,  1.44s/it] 63%|██████▎   | 171/270 [04:35<02:07,  1.29s/it] 64%|██████▎   | 172/270 [04:36<01:55,  1.18s/it] 64%|██████▍   | 173/270 [04:37<01:47,  1.11s/it] 64%|██████▍   | 174/270 [04:38<01:41,  1.06s/it] 65%|██████▍   | 175/270 [04:39<01:36,  1.02s/it] 65%|██████▌   | 176/270 [04:40<01:33,  1.00it/s] 66%|██████▌   | 177/270 [04:41<01:31,  1.02it/s] 66%|██████▌   | 178/270 [04:42<01:29,  1.03it/s] 66%|██████▋   | 179/270 [04:43<01:27,  1.04it/s] 67%|██████▋   | 180/270 [04:44<01:25,  1.05it/s] 67%|██████▋   | 181/270 [04:44<01:24,  1.06it/s] 67%|██████▋   | 182/270 [04:45<01:23,  1.06it/s] 68%|██████▊   | 183/270 [04:46<01:21,  1.06it/s] 68%|██████▊   | 184/270 [04:47<01:21,  1.06it/s] 69%|██████▊   | 185/270 [04:48<01:20,  1.06it/s] 69%|██████▉   | 186/270 [04:49<01:19,  1.06it/s] 69%|██████▉   | 187/270 [04:50<01:18,  1.06it/s] 70%|██████▉   | 188/270 [04:51<01:17,  1.06it/s] 70%|███████   | 189/270 [04:52<01:08,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.2121264785528183, 'eval_f1': 0.9448234796607299, 'eval_runtime': 2.6863, 'eval_samples_per_second': 158.585, 'eval_steps_per_second': 2.606, 'epoch': 6.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.21it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.26it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.73it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.43it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.30it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.57it/s][A                                                 
                                             [A 70%|███████   | 189/270 [04:54<01:08,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.57it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 70%|███████   | 190/270 [05:14<09:46,  7.33s/it] 71%|███████   | 191/270 [05:15<07:07,  5.41s/it] 71%|███████   | 192/270 [05:16<05:17,  4.07s/it] 71%|███████▏  | 193/270 [05:17<04:00,  3.12s/it] 72%|███████▏  | 194/270 [05:18<03:07,  2.47s/it] 72%|███████▏  | 195/270 [05:19<02:30,  2.00s/it] 73%|███████▎  | 196/270 [05:20<02:04,  1.68s/it] 73%|███████▎  | 197/270 [05:21<01:46,  1.46s/it] 73%|███████▎  | 198/270 [05:22<01:33,  1.30s/it] 74%|███████▎  | 199/270 [05:22<01:24,  1.19s/it] 74%|███████▍  | 200/270 [05:23<01:17,  1.11s/it]                                                  74%|███████▍  | 200/270 [05:23<01:17,  1.11s/it] 74%|███████▍  | 201/270 [05:24<01:13,  1.06s/it] 75%|███████▍  | 202/270 [05:25<01:09,  1.02s/it] 75%|███████▌  | 203/270 [05:26<01:06,  1.00it/s] 76%|███████▌  | 204/270 [05:27<01:04,  1.02it/s] 76%|███████▌  | 205/270 [05:28<01:02,  1.03it/s] 76%|███████▋  | 206/270 [05:29<01:01,  1.04it/s] 77%|███████▋  | 207/270 [05:30<01:00,  1.05it/s] 77%|███████▋  | 208/270 [05:31<00:58,  1.05it/s] 77%|███████▋  | 209/270 [05:32<00:57,  1.06it/s] 78%|███████▊  | 210/270 [05:33<00:56,  1.06it/s] 78%|███████▊  | 211/270 [05:34<00:55,  1.06it/s] 79%|███████▊  | 212/270 [05:35<00:54,  1.06it/s] 79%|███████▉  | 213/270 [05:36<00:53,  1.06it/s] 79%|███████▉  | 214/270 [05:37<00:52,  1.06it/s] 80%|███████▉  | 215/270 [05:38<00:51,  1.06it/s] 80%|████████  | 216/270 [05:38<00:45,  1.19it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.19574441015720367, 'eval_f1': 0.9545341804165334, 'eval_runtime': 2.6611, 'eval_samples_per_second': 160.083, 'eval_steps_per_second': 2.63, 'epoch': 7.0}
{'loss': 0.1552, 'learning_rate': 1.5687918106563326e-06, 'epoch': 7.41}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:01,  3.89it/s][A
 43%|████▎     | 3/7 [00:00<00:01,  3.44it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.26it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.16it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.11it/s][A
100%|██████████| 7/7 [00:02<00:00,  3.41it/s][A                                                 
                                             [A 80%|████████  | 216/270 [05:41<00:45,  1.19it/s]
100%|██████████| 7/7 [00:02<00:00,  3.41it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-216
Configuration saved in training-output/checkpoint-216/config.json
Model weights saved in training-output/checkpoint-216/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-216/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-216/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 80%|████████  | 217/270 [05:56<05:22,  6.09s/it] 81%|████████  | 218/270 [05:57<03:56,  4.54s/it] 81%|████████  | 219/270 [05:58<02:56,  3.46s/it] 81%|████████▏ | 220/270 [05:59<02:15,  2.70s/it] 82%|████████▏ | 221/270 [06:00<01:46,  2.17s/it] 82%|████████▏ | 222/270 [06:01<01:26,  1.80s/it] 83%|████████▎ | 223/270 [06:02<01:12,  1.54s/it] 83%|████████▎ | 224/270 [06:03<01:02,  1.36s/it] 83%|████████▎ | 225/270 [06:04<00:55,  1.23s/it] 84%|████████▎ | 226/270 [06:05<00:50,  1.14s/it] 84%|████████▍ | 227/270 [06:06<00:46,  1.08s/it] 84%|████████▍ | 228/270 [06:07<00:43,  1.04s/it] 85%|████████▍ | 229/270 [06:08<00:41,  1.01s/it] 85%|████████▌ | 230/270 [06:09<00:39,  1.01it/s] 86%|████████▌ | 231/270 [06:10<00:37,  1.03it/s] 86%|████████▌ | 232/270 [06:11<00:36,  1.03it/s] 86%|████████▋ | 233/270 [06:11<00:35,  1.04it/s] 87%|████████▋ | 234/270 [06:13<00:35,  1.00it/s] 87%|████████▋ | 235/270 [06:13<00:34,  1.02it/s] 87%|████████▋ | 236/270 [06:14<00:32,  1.03it/s] 88%|████████▊ | 237/270 [06:15<00:31,  1.04it/s] 88%|████████▊ | 238/270 [06:16<00:30,  1.05it/s] 89%|████████▊ | 239/270 [06:17<00:29,  1.05it/s] 89%|████████▉ | 240/270 [06:18<00:28,  1.05it/s] 89%|████████▉ | 241/270 [06:19<00:27,  1.05it/s] 90%|████████▉ | 242/270 [06:20<00:26,  1.06it/s] 90%|█████████ | 243/270 [06:21<00:22,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.18906018137931824, 'eval_f1': 0.9527067050932223, 'eval_runtime': 2.8971, 'eval_samples_per_second': 147.042, 'eval_steps_per_second': 2.416, 'epoch': 8.0}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  5.97it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.18it/s][A
 57%|█████▋    | 4/7 [00:01<00:00,  3.65it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.39it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.25it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.54it/s][A                                                 
                                             [A 90%|█████████ | 243/270 [06:23<00:22,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-243
Configuration saved in training-output/checkpoint-243/config.json
Model weights saved in training-output/checkpoint-243/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-243/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-243/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 90%|█████████ | 244/270 [06:38<02:33,  5.89s/it] 91%|█████████ | 245/270 [06:39<01:50,  4.40s/it] 91%|█████████ | 246/270 [06:40<01:20,  3.37s/it] 91%|█████████▏| 247/270 [06:41<01:00,  2.64s/it] 92%|█████████▏| 248/270 [06:42<00:46,  2.13s/it] 92%|█████████▏| 249/270 [06:43<00:37,  1.77s/it] 93%|█████████▎| 250/270 [06:44<00:30,  1.52s/it]                                                  93%|█████████▎| 250/270 [06:44<00:30,  1.52s/it] 93%|█████████▎| 251/270 [06:45<00:25,  1.35s/it] 93%|█████████▎| 252/270 [06:46<00:22,  1.23s/it] 94%|█████████▎| 253/270 [06:47<00:19,  1.14s/it] 94%|█████████▍| 254/270 [06:48<00:17,  1.08s/it] 94%|█████████▍| 255/270 [06:49<00:15,  1.04s/it] 95%|█████████▍| 256/270 [06:50<00:14,  1.01s/it] 95%|█████████▌| 257/270 [06:51<00:12,  1.01it/s] 96%|█████████▌| 258/270 [06:52<00:11,  1.02it/s] 96%|█████████▌| 259/270 [06:52<00:10,  1.04it/s] 96%|█████████▋| 260/270 [06:53<00:09,  1.04it/s] 97%|█████████▋| 261/270 [06:54<00:08,  1.05it/s] 97%|█████████▋| 262/270 [06:55<00:07,  1.05it/s] 97%|█████████▋| 263/270 [06:56<00:06,  1.06it/s] 98%|█████████▊| 264/270 [06:57<00:05,  1.06it/s] 98%|█████████▊| 265/270 [06:58<00:04,  1.05it/s] 99%|█████████▊| 266/270 [06:59<00:03,  1.05it/s] 99%|█████████▉| 267/270 [07:00<00:02,  1.05it/s] 99%|█████████▉| 268/270 [07:01<00:01,  1.06it/s]100%|█████████▉| 269/270 [07:02<00:00,  1.06it/s]100%|██████████| 270/270 [07:03<00:00,  1.18it/s]***** Running Evaluation *****
  Num examples = 426
  Batch size = 32
{'eval_loss': 0.18434683978557587, 'eval_f1': 0.9544102179327736, 'eval_runtime': 2.6481, 'eval_samples_per_second': 160.872, 'eval_steps_per_second': 2.643, 'epoch': 9.0}
{'loss': 0.1204, 'learning_rate': 1.3477564710088097e-07, 'epoch': 9.26}

  0%|          | 0/7 [00:00<?, ?it/s][A
 29%|██▊       | 2/7 [00:00<00:00,  6.08it/s][A
 43%|████▎     | 3/7 [00:00<00:00,  4.19it/s][A
 57%|█████▋    | 4/7 [00:00<00:00,  3.67it/s][A
 71%|███████▏  | 5/7 [00:01<00:00,  3.42it/s][A
 86%|████████▌ | 6/7 [00:01<00:00,  3.26it/s][A
100%|██████████| 7/7 [00:01<00:00,  3.53it/s][A                                                 
                                             [A100%|██████████| 270/270 [07:05<00:00,  1.18it/s]
100%|██████████| 7/7 [00:02<00:00,  3.53it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-270
Configuration saved in training-output/checkpoint-270/config.json
Model weights saved in training-output/checkpoint-270/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-270/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-270/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-189 (score: 0.9545341804165334).
                                                 100%|██████████| 270/270 [07:21<00:00,  1.18it/s]100%|██████████| 270/270 [07:21<00:00,  1.63s/it]
{'eval_loss': 0.18480610847473145, 'eval_f1': 0.9544102179327736, 'eval_runtime': 2.6628, 'eval_samples_per_second': 159.981, 'eval_steps_per_second': 2.629, 'epoch': 10.0}
{'train_runtime': 441.4268, 'train_samples_per_second': 38.511, 'train_steps_per_second': 0.612, 'train_loss': 0.4254081867359303, 'epoch': 10.0}
  0%|          | 0/107 [00:00<?, ?it/s]  2%|▏         | 2/107 [00:00<00:06, 16.63it/s]  4%|▎         | 4/107 [00:00<00:05, 17.46it/s]  7%|▋         | 7/107 [00:00<00:04, 22.16it/s]  9%|▉         | 10/107 [00:00<00:03, 24.41it/s] 12%|█▏        | 13/107 [00:00<00:03, 25.69it/s] 15%|█▍        | 16/107 [00:00<00:03, 26.37it/s] 18%|█▊        | 19/107 [00:00<00:03, 26.92it/s] 21%|██        | 22/107 [00:00<00:03, 27.30it/s] 23%|██▎       | 25/107 [00:00<00:02, 27.59it/s] 26%|██▌       | 28/107 [00:01<00:02, 27.50it/s] 29%|██▉       | 31/107 [00:01<00:02, 27.40it/s] 32%|███▏      | 34/107 [00:01<00:02, 27.58it/s] 35%|███▍      | 37/107 [00:01<00:02, 27.64it/s] 37%|███▋      | 40/107 [00:01<00:02, 27.66it/s] 40%|████      | 43/107 [00:01<00:02, 27.78it/s] 43%|████▎     | 46/107 [00:01<00:02, 27.90it/s] 46%|████▌     | 49/107 [00:01<00:02, 27.94it/s] 49%|████▊     | 52/107 [00:01<00:01, 27.91it/s] 51%|█████▏    | 55/107 [00:02<00:01, 27.98it/s] 54%|█████▍    | 58/107 [00:02<00:01, 27.82it/s] 57%|█████▋    | 61/107 [00:02<00:01, 27.69it/s] 60%|█████▉    | 64/107 [00:02<00:01, 27.76it/s] 63%|██████▎   | 67/107 [00:02<00:01, 27.80it/s] 65%|██████▌   | 70/107 [00:02<00:01, 27.79it/s] 68%|██████▊   | 73/107 [00:02<00:01, 27.87it/s] 71%|███████   | 76/107 [00:02<00:01, 27.94it/s] 74%|███████▍  | 79/107 [00:02<00:01, 27.87it/s] 77%|███████▋  | 82/107 [00:03<00:00, 27.82it/s] 79%|███████▉  | 85/107 [00:03<00:00, 27.82it/s] 82%|████████▏ | 88/107 [00:03<00:00, 27.64it/s] 85%|████████▌ | 91/107 [00:03<00:00, 27.75it/s] 88%|████████▊ | 94/107 [00:03<00:00, 27.85it/s] 91%|█████████ | 97/107 [00:03<00:00, 27.74it/s] 93%|█████████▎| 100/107 [00:03<00:00, 27.87it/s] 96%|█████████▋| 103/107 [00:03<00:00, 27.88it/s] 99%|█████████▉| 106/107 [00:03<00:00, 27.85it/s]100%|██████████| 107/107 [00:03<00:00, 27.33it/s]
Model finished with accuracy: 0.9553990610328639, macro-f1: 0.9545341804165334
Confusion matrix:
[[ 83   0   0   1   0]
 [  0  82   0   1   0]
 [  0   2  60   2   0]
 [  0   1   6 104   0]
 [  3   2   0   1  78]]
 15:45:35 up 86 days, 22:17,  2 users,  load average: 1,89, 2,30, 2,41
