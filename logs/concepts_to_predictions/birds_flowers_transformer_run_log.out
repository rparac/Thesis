Mon Jun 13 20:50:50 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA TITAN Xp     On   | 00000000:1D:00.0 Off |                  N/A |
| 23%   29C    P8     9W / 250W |      1MiB / 12196MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA TITAN Xp     On   | 00000000:B1:00.0 Off |                  N/A |
| 23%   25C    P8     8W / 250W |      1MiB / 12196MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
explanations.csv,   0%|          | 0/2 [00:00<?, ?ba/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.55s/ba]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.30s/ba]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 40.26ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:11<51:36, 11.51s/it]  1%|          | 2/270 [00:12<23:39,  5.30s/it]  1%|          | 3/270 [00:12<13:52,  3.12s/it]  1%|â–         | 4/270 [00:13<09:16,  2.09s/it]  2%|â–         | 5/270 [00:14<06:44,  1.53s/it]  2%|â–         | 6/270 [00:14<05:10,  1.18s/it]  3%|â–Ž         | 7/270 [00:15<04:12,  1.04it/s]  3%|â–Ž         | 8/270 [00:15<03:35,  1.22it/s]  3%|â–Ž         | 9/270 [00:16<03:10,  1.37it/s]  4%|â–Ž         | 10/270 [00:16<02:52,  1.50it/s]  4%|â–         | 11/270 [00:17<02:40,  1.62it/s]  4%|â–         | 12/270 [00:17<02:30,  1.71it/s]  5%|â–         | 13/270 [00:18<02:25,  1.77it/s]  5%|â–Œ         | 14/270 [00:18<02:20,  1.82it/s]  6%|â–Œ         | 15/270 [00:19<02:17,  1.85it/s]  6%|â–Œ         | 16/270 [00:19<02:15,  1.87it/s]  6%|â–‹         | 17/270 [00:20<02:14,  1.88it/s]  7%|â–‹         | 18/270 [00:20<02:12,  1.89it/s]  7%|â–‹         | 19/270 [00:21<02:11,  1.91it/s]  7%|â–‹         | 20/270 [00:21<02:09,  1.92it/s]  8%|â–Š         | 21/270 [00:22<02:21,  1.76it/s]  8%|â–Š         | 22/270 [00:22<02:17,  1.81it/s]  9%|â–Š         | 23/270 [00:23<02:13,  1.85it/s]  9%|â–‰         | 24/270 [00:24<02:12,  1.86it/s]  9%|â–‰         | 25/270 [00:24<02:09,  1.89it/s] 10%|â–‰         | 26/270 [00:25<02:08,  1.90it/s] 10%|â–ˆ         | 27/270 [00:25<01:56,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.30it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.74it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.83it/s][A                                                
                                             [A 10%|â–ˆ         | 27/270 [00:27<01:56,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.83it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|â–ˆ         | 28/270 [00:44<24:01,  5.96s/it] 11%|â–ˆ         | 29/270 [00:44<17:21,  4.32s/it] 11%|â–ˆ         | 30/270 [00:45<12:43,  3.18s/it] 11%|â–ˆâ–        | 31/270 [00:45<09:29,  2.38s/it] 12%|â–ˆâ–        | 32/270 [00:46<07:15,  1.83s/it] 12%|â–ˆâ–        | 33/270 [00:46<05:40,  1.44s/it] 13%|â–ˆâ–Ž        | 34/270 [00:47<04:33,  1.16s/it] 13%|â–ˆâ–Ž        | 35/270 [00:47<03:46,  1.04it/s] 13%|â–ˆâ–Ž        | 36/270 [00:48<03:15,  1.20it/s] 14%|â–ˆâ–Ž        | 37/270 [00:48<02:52,  1.35it/s] 14%|â–ˆâ–        | 38/270 [00:49<02:35,  1.49it/s] 14%|â–ˆâ–        | 39/270 [00:49<02:25,  1.59it/s] 15%|â–ˆâ–        | 40/270 [00:50<02:17,  1.67it/s] 15%|â–ˆâ–Œ        | 41/270 [00:50<02:11,  1.74it/s] 16%|â–ˆâ–Œ        | 42/270 [00:51<02:06,  1.81it/s] 16%|â–ˆâ–Œ        | 43/270 [00:51<02:03,  1.84it/s] 16%|â–ˆâ–‹        | 44/270 [00:52<02:01,  1.86it/s] 17%|â–ˆâ–‹        | 45/270 [00:52<02:00,  1.87it/s] 17%|â–ˆâ–‹        | 46/270 [00:53<01:58,  1.88it/s] 17%|â–ˆâ–‹        | 47/270 [00:54<01:57,  1.90it/s] 18%|â–ˆâ–Š        | 48/270 [00:54<01:56,  1.91it/s] 18%|â–ˆâ–Š        | 49/270 [00:55<01:55,  1.92it/s] 19%|â–ˆâ–Š        | 50/270 [00:55<01:54,  1.93it/s]                                                 19%|â–ˆâ–Š        | 50/270 [00:55<01:54,  1.93it/s] 19%|â–ˆâ–‰        | 51/270 [00:56<01:53,  1.93it/s] 19%|â–ˆâ–‰        | 52/270 [00:56<01:53,  1.92it/s] 20%|â–ˆâ–‰        | 53/270 [00:57<01:52,  1.93it/s] 20%|â–ˆâ–ˆ        | 54/270 [00:57<01:43,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.14035889506340027, 'eval_f1': 1.0, 'eval_runtime': 1.6896, 'eval_samples_per_second': 177.554, 'eval_steps_per_second': 2.959, 'epoch': 1.0}
{'loss': 0.3483, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.11it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.84it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.74it/s][A                                                
                                             [A 20%|â–ˆâ–ˆ        | 54/270 [00:58<01:43,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.74it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|â–ˆâ–ˆ        | 55/270 [01:15<21:02,  5.87s/it] 21%|â–ˆâ–ˆ        | 56/270 [01:16<15:12,  4.27s/it] 21%|â–ˆâ–ˆ        | 57/270 [01:17<11:09,  3.14s/it] 21%|â–ˆâ–ˆâ–       | 58/270 [01:17<08:19,  2.36s/it] 22%|â–ˆâ–ˆâ–       | 59/270 [01:18<06:21,  1.81s/it] 22%|â–ˆâ–ˆâ–       | 60/270 [01:18<04:58,  1.42s/it] 23%|â–ˆâ–ˆâ–Ž       | 61/270 [01:19<04:00,  1.15s/it] 23%|â–ˆâ–ˆâ–Ž       | 62/270 [01:19<03:19,  1.04it/s] 23%|â–ˆâ–ˆâ–Ž       | 63/270 [01:20<02:51,  1.21it/s] 24%|â–ˆâ–ˆâ–Ž       | 64/270 [01:20<02:32,  1.35it/s] 24%|â–ˆâ–ˆâ–       | 65/270 [01:21<02:18,  1.48it/s] 24%|â–ˆâ–ˆâ–       | 66/270 [01:21<02:08,  1.59it/s] 25%|â–ˆâ–ˆâ–       | 67/270 [01:22<02:00,  1.68it/s] 25%|â–ˆâ–ˆâ–Œ       | 68/270 [01:22<01:55,  1.74it/s] 26%|â–ˆâ–ˆâ–Œ       | 69/270 [01:23<01:52,  1.79it/s] 26%|â–ˆâ–ˆâ–Œ       | 70/270 [01:23<01:49,  1.83it/s] 26%|â–ˆâ–ˆâ–‹       | 71/270 [01:24<01:47,  1.86it/s] 27%|â–ˆâ–ˆâ–‹       | 72/270 [01:24<01:45,  1.88it/s] 27%|â–ˆâ–ˆâ–‹       | 73/270 [01:25<01:43,  1.90it/s] 27%|â–ˆâ–ˆâ–‹       | 74/270 [01:25<01:42,  1.90it/s] 28%|â–ˆâ–ˆâ–Š       | 75/270 [01:26<01:41,  1.91it/s] 28%|â–ˆâ–ˆâ–Š       | 76/270 [01:26<01:41,  1.91it/s] 29%|â–ˆâ–ˆâ–Š       | 77/270 [01:27<01:41,  1.91it/s] 29%|â–ˆâ–ˆâ–‰       | 78/270 [01:27<01:40,  1.90it/s] 29%|â–ˆâ–ˆâ–‰       | 79/270 [01:28<01:39,  1.91it/s] 30%|â–ˆâ–ˆâ–‰       | 80/270 [01:29<01:39,  1.91it/s] 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:29<01:30,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.002289238851517439, 'eval_f1': 1.0, 'eval_runtime': 1.3469, 'eval_samples_per_second': 222.731, 'eval_steps_per_second': 3.712, 'epoch': 2.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00,  9.93it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:00<00:00,  7.32it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.51it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.61it/s][A                                                
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:30<01:30,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.61it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|â–ˆâ–ˆâ–ˆ       | 82/270 [01:50<20:55,  6.68s/it] 31%|â–ˆâ–ˆâ–ˆ       | 83/270 [01:51<15:03,  4.83s/it] 31%|â–ˆâ–ˆâ–ˆ       | 84/270 [01:51<10:59,  3.54s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 85/270 [01:52<08:08,  2.64s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 86/270 [01:52<06:08,  2.00s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 87/270 [01:53<04:44,  1.55s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 88/270 [01:53<03:46,  1.24s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 89/270 [01:54<03:05,  1.03s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/270 [01:54<02:38,  1.14it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/270 [01:55<02:18,  1.30it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 92/270 [01:55<02:03,  1.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 93/270 [01:56<01:54,  1.54it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 94/270 [01:56<01:47,  1.63it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 95/270 [01:57<01:42,  1.71it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/270 [01:57<01:38,  1.77it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/270 [01:58<01:35,  1.81it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 98/270 [01:58<01:33,  1.85it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 99/270 [01:59<01:31,  1.87it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:59<01:30,  1.88it/s]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:59<01:30,  1.88it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/270 [02:00<01:29,  1.89it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 102/270 [02:00<01:28,  1.90it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 103/270 [02:01<01:27,  1.91it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 104/270 [02:02<01:27,  1.91it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 105/270 [02:02<01:26,  1.91it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 106/270 [02:03<01:25,  1.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 107/270 [02:03<01:25,  1.90it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [02:04<01:19,  2.05it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0009151117992587388, 'eval_f1': 1.0, 'eval_runtime': 1.3679, 'eval_samples_per_second': 219.306, 'eval_steps_per_second': 3.655, 'epoch': 3.0}
{'loss': 0.0024, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.95it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.92it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.81it/s][A                                                 
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [02:05<01:19,  2.05it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.81it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-216] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 109/270 [02:22<16:05,  6.00s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 110/270 [02:23<11:36,  4.36s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/270 [02:23<08:29,  3.21s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/270 [02:24<06:18,  2.40s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/270 [02:24<04:46,  1.83s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/270 [02:25<03:44,  1.44s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 115/270 [02:25<03:00,  1.16s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 116/270 [02:26<02:29,  1.03it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 117/270 [02:26<02:07,  1.20it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/270 [02:27<01:52,  1.36it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119/270 [02:28<01:41,  1.48it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120/270 [02:28<01:34,  1.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/270 [02:29<01:29,  1.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 122/270 [02:29<01:25,  1.72it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 123/270 [02:30<01:22,  1.77it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 124/270 [02:30<01:20,  1.81it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 125/270 [02:31<01:18,  1.85it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126/270 [02:31<01:16,  1.88it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 127/270 [02:32<01:15,  1.89it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 128/270 [02:32<01:14,  1.90it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 129/270 [02:33<01:13,  1.92it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 130/270 [02:33<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/270 [02:34<01:13,  1.88it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 132/270 [02:34<01:13,  1.87it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 133/270 [02:35<01:12,  1.88it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 134/270 [02:35<01:11,  1.89it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:36<01:05,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0006510911043733358, 'eval_f1': 1.0, 'eval_runtime': 1.356, 'eval_samples_per_second': 221.236, 'eval_steps_per_second': 3.687, 'epoch': 4.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.59it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.89it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.78it/s][A                                                 
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:37<01:05,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.78it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-243] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 136/270 [02:54<13:12,  5.92s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137/270 [02:55<09:31,  4.30s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 138/270 [02:55<06:58,  3.17s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/270 [02:56<05:11,  2.38s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/270 [02:57<03:56,  1.82s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/270 [02:57<03:04,  1.43s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 142/270 [02:58<02:27,  1.15s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 143/270 [02:58<02:01,  1.04it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144/270 [02:59<01:50,  1.14it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/270 [02:59<01:36,  1.30it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 146/270 [03:00<01:26,  1.43it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 147/270 [03:00<01:19,  1.55it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/270 [03:01<01:13,  1.66it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/270 [03:01<01:09,  1.73it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [03:02<01:07,  1.77it/s]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [03:02<01:07,  1.77it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 151/270 [03:02<01:05,  1.81it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/270 [03:03<01:04,  1.83it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153/270 [03:03<01:02,  1.86it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 154/270 [03:04<01:01,  1.87it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 155/270 [03:04<01:00,  1.89it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 156/270 [03:05<00:59,  1.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157/270 [03:06<00:59,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 158/270 [03:06<00:58,  1.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 159/270 [03:07<00:57,  1.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 160/270 [03:07<00:57,  1.91it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 161/270 [03:08<00:57,  1.90it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:08<00:52,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0005338601768016815, 'eval_f1': 1.0, 'eval_runtime': 1.3564, 'eval_samples_per_second': 221.18, 'eval_steps_per_second': 3.686, 'epoch': 5.0}
{'loss': 0.0011, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.83it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.90it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.92it/s][A                                                 
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:09<00:52,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.92it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-270] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-27 (score: 1.0).
                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:27<00:52,  2.07it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:27<02:18,  1.28s/it]
{'eval_loss': 0.0004683836887124926, 'eval_f1': 1.0, 'eval_runtime': 1.327, 'eval_samples_per_second': 226.07, 'eval_steps_per_second': 3.768, 'epoch': 6.0}
{'train_runtime': 207.9067, 'train_samples_per_second': 81.767, 'train_steps_per_second': 1.299, 'train_loss': 0.1086608423207553, 'epoch': 6.0}
  0%|          | 0/75 [00:00<?, ?it/s]  5%|â–Œ         | 4/75 [00:00<00:01, 37.30it/s] 12%|â–ˆâ–        | 9/75 [00:00<00:01, 41.38it/s] 19%|â–ˆâ–Š        | 14/75 [00:00<00:01, 43.43it/s] 25%|â–ˆâ–ˆâ–Œ       | 19/75 [00:00<00:01, 45.41it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 24/75 [00:00<00:01, 46.82it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 29/75 [00:00<00:00, 47.77it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 34/75 [00:00<00:00, 48.44it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 39/75 [00:00<00:00, 48.84it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 44/75 [00:00<00:00, 49.13it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 49/75 [00:01<00:00, 49.33it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 54/75 [00:01<00:00, 49.51it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 59/75 [00:01<00:00, 49.63it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 64/75 [00:01<00:00, 49.47it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 69/75 [00:01<00:00, 49.44it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 74/75 [00:01<00:00, 49.48it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 48.06it/s]
Model finished with accuracy: 1.0, macro-f1: 1.0
Confusion matrix:
[[159   0]
 [  0 141]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
explanations.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.15ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.12ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 45.28ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:06<26:55,  6.01s/it]  1%|          | 2/270 [00:06<12:58,  2.90s/it]  1%|          | 3/270 [00:07<08:04,  1.81s/it]  1%|â–         | 4/270 [00:07<05:47,  1.31s/it]  2%|â–         | 5/270 [00:08<04:31,  1.03s/it]  2%|â–         | 6/270 [00:08<03:45,  1.17it/s]  3%|â–Ž         | 7/270 [00:09<03:16,  1.34it/s]  3%|â–Ž         | 8/270 [00:09<02:55,  1.49it/s]  3%|â–Ž         | 9/270 [00:10<02:43,  1.60it/s]  4%|â–Ž         | 10/270 [00:10<02:34,  1.69it/s]  4%|â–         | 11/270 [00:11<02:28,  1.75it/s]  4%|â–         | 12/270 [00:11<02:23,  1.80it/s]  5%|â–         | 13/270 [00:12<02:19,  1.84it/s]  5%|â–Œ         | 14/270 [00:12<02:17,  1.87it/s]  6%|â–Œ         | 15/270 [00:13<02:15,  1.89it/s]  6%|â–Œ         | 16/270 [00:14<02:14,  1.88it/s]  6%|â–‹         | 17/270 [00:14<02:13,  1.89it/s]  7%|â–‹         | 18/270 [00:15<02:12,  1.90it/s]  7%|â–‹         | 19/270 [00:15<02:11,  1.91it/s]  7%|â–‹         | 20/270 [00:16<02:10,  1.92it/s]  8%|â–Š         | 21/270 [00:16<02:21,  1.76it/s]  8%|â–Š         | 22/270 [00:17<02:17,  1.81it/s]  9%|â–Š         | 23/270 [00:17<02:14,  1.83it/s]  9%|â–‰         | 24/270 [00:18<02:12,  1.86it/s]  9%|â–‰         | 25/270 [00:18<02:11,  1.87it/s] 10%|â–‰         | 26/270 [00:19<02:09,  1.89it/s] 10%|â–ˆ         | 27/270 [00:19<01:57,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.67it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.75it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.86it/s][A                                                
                                             [A 10%|â–ˆ         | 27/270 [00:21<01:57,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.86it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|â–ˆ         | 28/270 [00:38<24:07,  5.98s/it] 11%|â–ˆ         | 29/270 [00:39<17:26,  4.34s/it] 11%|â–ˆ         | 30/270 [00:39<12:46,  3.19s/it] 11%|â–ˆâ–        | 31/270 [00:40<09:31,  2.39s/it] 12%|â–ˆâ–        | 32/270 [00:40<07:15,  1.83s/it] 12%|â–ˆâ–        | 33/270 [00:41<05:40,  1.44s/it] 13%|â–ˆâ–Ž        | 34/270 [00:41<04:34,  1.16s/it] 13%|â–ˆâ–Ž        | 35/270 [00:42<03:48,  1.03it/s] 13%|â–ˆâ–Ž        | 36/270 [00:42<03:16,  1.19it/s] 14%|â–ˆâ–Ž        | 37/270 [00:43<02:53,  1.34it/s] 14%|â–ˆâ–        | 38/270 [00:43<02:36,  1.48it/s] 14%|â–ˆâ–        | 39/270 [00:44<02:25,  1.58it/s] 15%|â–ˆâ–        | 40/270 [00:44<02:17,  1.68it/s] 15%|â–ˆâ–Œ        | 41/270 [00:45<02:11,  1.74it/s] 16%|â–ˆâ–Œ        | 42/270 [00:45<02:07,  1.78it/s] 16%|â–ˆâ–Œ        | 43/270 [00:46<02:04,  1.83it/s] 16%|â–ˆâ–‹        | 44/270 [00:46<02:02,  1.85it/s] 17%|â–ˆâ–‹        | 45/270 [00:47<01:59,  1.88it/s] 17%|â–ˆâ–‹        | 46/270 [00:47<01:59,  1.88it/s] 17%|â–ˆâ–‹        | 47/270 [00:48<01:58,  1.88it/s] 18%|â–ˆâ–Š        | 48/270 [00:49<01:58,  1.88it/s] 18%|â–ˆâ–Š        | 49/270 [00:49<01:57,  1.88it/s] 19%|â–ˆâ–Š        | 50/270 [00:50<01:56,  1.89it/s]                                                 19%|â–ˆâ–Š        | 50/270 [00:50<01:56,  1.89it/s] 19%|â–ˆâ–‰        | 51/270 [00:50<01:55,  1.89it/s] 19%|â–ˆâ–‰        | 52/270 [00:51<01:54,  1.91it/s] 20%|â–ˆâ–‰        | 53/270 [00:51<01:53,  1.91it/s] 20%|â–ˆâ–ˆ        | 54/270 [00:52<01:43,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.05832761526107788, 'eval_f1': 0.9966666296292181, 'eval_runtime': 1.3684, 'eval_samples_per_second': 219.227, 'eval_steps_per_second': 3.654, 'epoch': 1.0}
{'loss': 0.2926, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.44it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.72it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.78it/s][A                                                
                                             [A 20%|â–ˆâ–ˆ        | 54/270 [00:53<01:43,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.78it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|â–ˆâ–ˆ        | 55/270 [01:10<21:02,  5.87s/it] 21%|â–ˆâ–ˆ        | 56/270 [01:10<15:11,  4.26s/it] 21%|â–ˆâ–ˆ        | 57/270 [01:11<11:08,  3.14s/it] 21%|â–ˆâ–ˆâ–       | 58/270 [01:12<08:18,  2.35s/it] 22%|â–ˆâ–ˆâ–       | 59/270 [01:12<06:20,  1.80s/it] 22%|â–ˆâ–ˆâ–       | 60/270 [01:13<04:58,  1.42s/it] 23%|â–ˆâ–ˆâ–Ž       | 61/270 [01:13<04:00,  1.15s/it] 23%|â–ˆâ–ˆâ–Ž       | 62/270 [01:14<03:19,  1.04it/s] 23%|â–ˆâ–ˆâ–Ž       | 63/270 [01:14<02:51,  1.21it/s] 24%|â–ˆâ–ˆâ–Ž       | 64/270 [01:15<02:31,  1.36it/s] 24%|â–ˆâ–ˆâ–       | 65/270 [01:15<02:17,  1.49it/s] 24%|â–ˆâ–ˆâ–       | 66/270 [01:16<02:08,  1.59it/s] 25%|â–ˆâ–ˆâ–       | 67/270 [01:16<02:01,  1.67it/s] 25%|â–ˆâ–ˆâ–Œ       | 68/270 [01:17<01:55,  1.74it/s] 26%|â–ˆâ–ˆâ–Œ       | 69/270 [01:17<01:51,  1.81it/s] 26%|â–ˆâ–ˆâ–Œ       | 70/270 [01:18<01:48,  1.85it/s] 26%|â–ˆâ–ˆâ–‹       | 71/270 [01:18<01:47,  1.85it/s] 27%|â–ˆâ–ˆâ–‹       | 72/270 [01:19<01:46,  1.86it/s] 27%|â–ˆâ–ˆâ–‹       | 73/270 [01:19<01:45,  1.87it/s] 27%|â–ˆâ–ˆâ–‹       | 74/270 [01:20<01:44,  1.88it/s] 28%|â–ˆâ–ˆâ–Š       | 75/270 [01:20<01:43,  1.89it/s] 28%|â–ˆâ–ˆâ–Š       | 76/270 [01:21<01:42,  1.90it/s] 29%|â–ˆâ–ˆâ–Š       | 77/270 [01:21<01:40,  1.91it/s] 29%|â–ˆâ–ˆâ–‰       | 78/270 [01:22<01:40,  1.92it/s] 29%|â–ˆâ–ˆâ–‰       | 79/270 [01:22<01:38,  1.93it/s] 30%|â–ˆâ–ˆâ–‰       | 80/270 [01:23<01:38,  1.93it/s] 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:23<01:31,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0022624912671744823, 'eval_f1': 1.0, 'eval_runtime': 1.3727, 'eval_samples_per_second': 218.551, 'eval_steps_per_second': 3.643, 'epoch': 2.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.06it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.90it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.81it/s][A                                                
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:25<01:31,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.81it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|â–ˆâ–ˆâ–ˆ       | 82/270 [01:42<18:32,  5.92s/it] 31%|â–ˆâ–ˆâ–ˆ       | 83/270 [01:43<13:24,  4.30s/it] 31%|â–ˆâ–ˆâ–ˆ       | 84/270 [01:43<09:48,  3.17s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 85/270 [01:44<07:19,  2.37s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 86/270 [01:44<05:34,  1.82s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 87/270 [01:45<04:21,  1.43s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 88/270 [01:45<03:30,  1.16s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 89/270 [01:46<02:54,  1.03it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/270 [01:46<02:29,  1.20it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/270 [01:47<02:11,  1.36it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 92/270 [01:47<01:59,  1.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 93/270 [01:48<01:51,  1.59it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 94/270 [01:48<01:45,  1.67it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 95/270 [01:49<01:41,  1.73it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/270 [01:49<01:37,  1.78it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/270 [01:50<01:34,  1.83it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 98/270 [01:50<01:32,  1.85it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 99/270 [01:51<01:31,  1.87it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:51<01:30,  1.87it/s]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:51<01:30,  1.87it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/270 [01:52<01:30,  1.87it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 102/270 [01:52<01:30,  1.86it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 103/270 [01:53<01:29,  1.86it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 104/270 [01:54<01:28,  1.88it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 105/270 [01:54<01:26,  1.90it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 106/270 [01:55<01:25,  1.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 107/270 [01:55<01:25,  1.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:55<01:17,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0007662350544705987, 'eval_f1': 1.0, 'eval_runtime': 1.4912, 'eval_samples_per_second': 201.176, 'eval_steps_per_second': 3.353, 'epoch': 3.0}
{'loss': 0.0018, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.73it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.87it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.91it/s][A                                                 
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:57<01:17,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.91it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 109/270 [02:14<15:43,  5.86s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 110/270 [02:14<11:21,  4.26s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/270 [02:15<08:18,  3.14s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/270 [02:15<06:11,  2.35s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/270 [02:16<04:42,  1.80s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/270 [02:16<03:40,  1.42s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 115/270 [02:17<02:59,  1.16s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 116/270 [02:18<02:29,  1.03it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 117/270 [02:18<02:07,  1.20it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/270 [02:19<01:52,  1.35it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119/270 [02:19<01:41,  1.48it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120/270 [02:20<01:34,  1.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/270 [02:20<01:29,  1.66it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 122/270 [02:21<01:25,  1.73it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 123/270 [02:21<01:22,  1.78it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 124/270 [02:22<01:20,  1.82it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 125/270 [02:22<01:18,  1.85it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126/270 [02:23<01:17,  1.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 127/270 [02:23<01:15,  1.89it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 128/270 [02:24<01:14,  1.91it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 129/270 [02:24<01:13,  1.91it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 130/270 [02:25<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/270 [02:25<01:13,  1.89it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 132/270 [02:26<01:13,  1.88it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 133/270 [02:26<01:12,  1.89it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 134/270 [02:27<01:12,  1.89it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:27<01:05,  2.05it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0005493175121955574, 'eval_f1': 1.0, 'eval_runtime': 1.3519, 'eval_samples_per_second': 221.915, 'eval_steps_per_second': 3.699, 'epoch': 4.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.43it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.55it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.65it/s][A                                                 
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:29<01:05,  2.05it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.65it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 136/270 [02:46<13:15,  5.93s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137/270 [02:47<09:32,  4.31s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 138/270 [02:47<06:58,  3.17s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/270 [02:48<05:11,  2.38s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/270 [02:48<03:56,  1.82s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/270 [02:49<03:05,  1.44s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 142/270 [02:49<02:28,  1.16s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 143/270 [02:50<02:03,  1.03it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144/270 [02:50<01:45,  1.20it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/270 [02:51<01:32,  1.36it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 146/270 [02:51<01:23,  1.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 147/270 [02:52<01:17,  1.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/270 [02:52<01:12,  1.67it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/270 [02:53<01:09,  1.74it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:53<01:07,  1.79it/s]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:53<01:07,  1.79it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 151/270 [02:54<01:05,  1.81it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/270 [02:54<01:04,  1.83it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153/270 [02:55<01:03,  1.85it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 154/270 [02:55<01:02,  1.86it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 155/270 [02:56<01:01,  1.88it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 156/270 [02:57<01:00,  1.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157/270 [02:57<00:59,  1.90it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 158/270 [02:58<00:58,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 159/270 [02:58<00:57,  1.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 160/270 [02:59<00:57,  1.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 161/270 [02:59<00:56,  1.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [02:59<00:51,  2.10it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0004534149484243244, 'eval_f1': 1.0, 'eval_runtime': 1.4034, 'eval_samples_per_second': 213.765, 'eval_steps_per_second': 3.563, 'epoch': 5.0}
{'loss': 0.0009, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.82it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.93it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.94it/s][A                                                 
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:01<00:51,  2.10it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.94it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 163/270 [03:18<10:29,  5.88s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 164/270 [03:18<07:32,  4.27s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 165/270 [03:19<05:30,  3.15s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 166/270 [03:20<04:05,  2.36s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 167/270 [03:20<03:06,  1.81s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 168/270 [03:21<02:25,  1.42s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 169/270 [03:21<01:56,  1.16s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 170/270 [03:22<01:36,  1.04it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 171/270 [03:22<01:21,  1.21it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 172/270 [03:23<01:12,  1.35it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 173/270 [03:23<01:05,  1.49it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 174/270 [03:24<01:00,  1.58it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 175/270 [03:24<00:57,  1.66it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 176/270 [03:25<00:54,  1.73it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 177/270 [03:25<00:52,  1.79it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 178/270 [03:26<00:49,  1.84it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 179/270 [03:26<00:49,  1.85it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 180/270 [03:27<00:48,  1.87it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 181/270 [03:27<00:47,  1.87it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 182/270 [03:28<00:46,  1.89it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 183/270 [03:28<00:46,  1.89it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 184/270 [03:29<00:45,  1.90it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 185/270 [03:29<00:44,  1.90it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 186/270 [03:30<00:44,  1.90it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 187/270 [03:31<00:43,  1.91it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 188/270 [03:31<00:42,  1.91it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 189/270 [03:31<00:39,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.00039932355866767466, 'eval_f1': 1.0, 'eval_runtime': 1.3231, 'eval_samples_per_second': 226.742, 'eval_steps_per_second': 3.779, 'epoch': 6.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.93it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.96it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.91it/s][A                                                 
                                             [A 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 189/270 [03:33<00:39,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.91it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-189
Configuration saved in training-output/checkpoint-189/config.json
Model weights saved in training-output/checkpoint-189/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-189/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-189/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-27] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-54 (score: 1.0).
                                                  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 189/270 [03:50<00:39,  2.07it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 189/270 [03:50<01:38,  1.22s/it]
{'eval_loss': 0.00036872480995953083, 'eval_f1': 1.0, 'eval_runtime': 1.3437, 'eval_samples_per_second': 223.267, 'eval_steps_per_second': 3.721, 'epoch': 7.0}
{'train_runtime': 230.8874, 'train_samples_per_second': 73.629, 'train_steps_per_second': 1.169, 'train_loss': 0.07826603177402701, 'epoch': 7.0}
  0%|          | 0/75 [00:00<?, ?it/s]  7%|â–‹         | 5/75 [00:00<00:01, 48.73it/s] 15%|â–ˆâ–        | 11/75 [00:00<00:01, 49.56it/s] 23%|â–ˆâ–ˆâ–Ž       | 17/75 [00:00<00:01, 49.79it/s] 29%|â–ˆâ–ˆâ–‰       | 22/75 [00:00<00:01, 49.85it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 27/75 [00:00<00:00, 49.88it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 32/75 [00:00<00:00, 49.90it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 38/75 [00:00<00:00, 49.95it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 43/75 [00:00<00:00, 49.96it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 48/75 [00:00<00:00, 49.90it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 53/75 [00:01<00:00, 49.66it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 58/75 [00:01<00:00, 49.57it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 63/75 [00:01<00:00, 49.66it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 68/75 [00:01<00:00, 49.71it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 73/75 [00:01<00:00, 49.76it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 49.75it/s]
Model finished with accuracy: 1.0, macro-f1: 1.0
Confusion matrix:
[[150   0]
 [  0 150]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
explanations.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.83ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.80ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 43.11ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:04<22:19,  4.98s/it]  1%|          | 2/270 [00:05<11:32,  2.58s/it]  1%|          | 3/270 [00:06<07:17,  1.64s/it]  1%|â–         | 4/270 [00:06<05:18,  1.20s/it]  2%|â–         | 5/270 [00:07<04:12,  1.05it/s]  2%|â–         | 6/270 [00:07<03:32,  1.24it/s]  3%|â–Ž         | 7/270 [00:08<03:07,  1.40it/s]  3%|â–Ž         | 8/270 [00:09<02:51,  1.53it/s]  3%|â–Ž         | 9/270 [00:09<02:40,  1.63it/s]  4%|â–Ž         | 10/270 [00:10<02:31,  1.71it/s]  4%|â–         | 11/270 [00:10<02:25,  1.78it/s]  4%|â–         | 12/270 [00:11<02:21,  1.82it/s]  5%|â–         | 13/270 [00:11<02:18,  1.85it/s]  5%|â–Œ         | 14/270 [00:12<02:17,  1.86it/s]  6%|â–Œ         | 15/270 [00:12<02:15,  1.88it/s]  6%|â–Œ         | 16/270 [00:13<02:13,  1.91it/s]  6%|â–‹         | 17/270 [00:13<02:12,  1.91it/s]  7%|â–‹         | 18/270 [00:14<02:10,  1.93it/s]  7%|â–‹         | 19/270 [00:14<02:10,  1.92it/s]  7%|â–‹         | 20/270 [00:15<02:10,  1.92it/s]  8%|â–Š         | 21/270 [00:15<02:18,  1.80it/s]  8%|â–Š         | 22/270 [00:16<02:15,  1.83it/s]  9%|â–Š         | 23/270 [00:16<02:14,  1.84it/s]  9%|â–‰         | 24/270 [00:17<02:12,  1.85it/s]  9%|â–‰         | 25/270 [00:17<02:11,  1.87it/s] 10%|â–‰         | 26/270 [00:18<02:08,  1.89it/s] 10%|â–ˆ         | 27/270 [00:18<01:57,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.44it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.91it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.93it/s][A                                                
                                             [A 10%|â–ˆ         | 27/270 [00:20<01:57,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.93it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-54] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|â–ˆ         | 28/270 [00:37<23:25,  5.81s/it] 11%|â–ˆ         | 29/270 [00:37<16:56,  4.22s/it] 11%|â–ˆ         | 30/270 [00:38<12:26,  3.11s/it] 11%|â–ˆâ–        | 31/270 [00:38<09:18,  2.34s/it] 12%|â–ˆâ–        | 32/270 [00:39<07:07,  1.79s/it] 12%|â–ˆâ–        | 33/270 [00:39<05:34,  1.41s/it] 13%|â–ˆâ–Ž        | 34/270 [00:40<04:29,  1.14s/it] 13%|â–ˆâ–Ž        | 35/270 [00:40<03:43,  1.05it/s] 13%|â–ˆâ–Ž        | 36/270 [00:41<03:12,  1.22it/s] 14%|â–ˆâ–Ž        | 37/270 [00:41<02:50,  1.37it/s] 14%|â–ˆâ–        | 38/270 [00:42<02:34,  1.50it/s] 14%|â–ˆâ–        | 39/270 [00:42<02:24,  1.60it/s] 15%|â–ˆâ–        | 40/270 [00:43<02:16,  1.68it/s] 15%|â–ˆâ–Œ        | 41/270 [00:43<02:11,  1.74it/s] 16%|â–ˆâ–Œ        | 42/270 [00:44<02:06,  1.81it/s] 16%|â–ˆâ–Œ        | 43/270 [00:44<02:02,  1.85it/s] 16%|â–ˆâ–‹        | 44/270 [00:45<02:01,  1.85it/s] 17%|â–ˆâ–‹        | 45/270 [00:45<02:00,  1.86it/s] 17%|â–ˆâ–‹        | 46/270 [00:46<01:59,  1.87it/s] 17%|â–ˆâ–‹        | 47/270 [00:46<01:57,  1.89it/s] 18%|â–ˆâ–Š        | 48/270 [00:47<01:56,  1.91it/s] 18%|â–ˆâ–Š        | 49/270 [00:48<01:55,  1.91it/s] 19%|â–ˆâ–Š        | 50/270 [00:48<01:53,  1.93it/s]                                                 19%|â–ˆâ–Š        | 50/270 [00:48<01:53,  1.93it/s] 19%|â–ˆâ–‰        | 51/270 [00:49<01:53,  1.93it/s] 19%|â–ˆâ–‰        | 52/270 [00:49<01:53,  1.93it/s] 20%|â–ˆâ–‰        | 53/270 [00:50<01:53,  1.91it/s] 20%|â–ˆâ–ˆ        | 54/270 [00:50<01:44,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.03379248455166817, 'eval_f1': 1.0, 'eval_runtime': 1.3513, 'eval_samples_per_second': 222.013, 'eval_steps_per_second': 3.7, 'epoch': 1.0}
{'loss': 0.247, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.35it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.63it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.62it/s][A                                                
                                             [A 20%|â–ˆâ–ˆ        | 54/270 [00:51<01:44,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.62it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-81] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|â–ˆâ–ˆ        | 55/270 [01:08<20:42,  5.78s/it] 21%|â–ˆâ–ˆ        | 56/270 [01:09<14:58,  4.20s/it] 21%|â–ˆâ–ˆ        | 57/270 [01:09<10:59,  3.10s/it] 21%|â–ˆâ–ˆâ–       | 58/270 [01:10<08:12,  2.32s/it] 22%|â–ˆâ–ˆâ–       | 59/270 [01:10<06:15,  1.78s/it] 22%|â–ˆâ–ˆâ–       | 60/270 [01:11<04:55,  1.41s/it] 23%|â–ˆâ–ˆâ–Ž       | 61/270 [01:11<03:58,  1.14s/it] 23%|â–ˆâ–ˆâ–Ž       | 62/270 [01:12<03:18,  1.05it/s] 23%|â–ˆâ–ˆâ–Ž       | 63/270 [01:12<02:49,  1.22it/s] 24%|â–ˆâ–ˆâ–Ž       | 64/270 [01:13<02:30,  1.37it/s] 24%|â–ˆâ–ˆâ–       | 65/270 [01:13<02:16,  1.50it/s] 24%|â–ˆâ–ˆâ–       | 66/270 [01:14<02:07,  1.60it/s] 25%|â–ˆâ–ˆâ–       | 67/270 [01:14<02:00,  1.68it/s] 25%|â–ˆâ–ˆâ–Œ       | 68/270 [01:15<01:55,  1.75it/s] 26%|â–ˆâ–ˆâ–Œ       | 69/270 [01:15<01:51,  1.81it/s] 26%|â–ˆâ–ˆâ–Œ       | 70/270 [01:16<01:48,  1.85it/s] 26%|â–ˆâ–ˆâ–‹       | 71/270 [01:16<01:45,  1.88it/s] 27%|â–ˆâ–ˆâ–‹       | 72/270 [01:17<01:44,  1.90it/s] 27%|â–ˆâ–ˆâ–‹       | 73/270 [01:17<01:43,  1.90it/s] 27%|â–ˆâ–ˆâ–‹       | 74/270 [01:18<01:43,  1.90it/s] 28%|â–ˆâ–ˆâ–Š       | 75/270 [01:19<01:42,  1.90it/s] 28%|â–ˆâ–ˆâ–Š       | 76/270 [01:19<01:41,  1.92it/s] 29%|â–ˆâ–ˆâ–Š       | 77/270 [01:20<01:39,  1.93it/s] 29%|â–ˆâ–ˆâ–‰       | 78/270 [01:20<01:39,  1.92it/s] 29%|â–ˆâ–ˆâ–‰       | 79/270 [01:21<01:39,  1.93it/s] 30%|â–ˆâ–ˆâ–‰       | 80/270 [01:21<01:39,  1.92it/s] 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:21<01:30,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.001553278067149222, 'eval_f1': 1.0, 'eval_runtime': 1.3679, 'eval_samples_per_second': 219.318, 'eval_steps_per_second': 3.655, 'epoch': 2.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.13it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.87it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.76it/s][A                                                
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:23<01:30,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.76it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-108] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|â–ˆâ–ˆâ–ˆ       | 82/270 [01:42<20:01,  6.39s/it] 31%|â–ˆâ–ˆâ–ˆ       | 83/270 [01:42<14:26,  4.63s/it] 31%|â–ˆâ–ˆâ–ˆ       | 84/270 [01:43<10:32,  3.40s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 85/270 [01:43<07:49,  2.54s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 86/270 [01:44<05:55,  1.93s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 87/270 [01:44<04:36,  1.51s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 88/270 [01:45<03:40,  1.21s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 89/270 [01:45<03:02,  1.01s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/270 [01:46<02:34,  1.16it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/270 [01:46<02:15,  1.32it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 92/270 [01:47<02:03,  1.44it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 93/270 [01:47<01:53,  1.56it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 94/270 [01:48<01:45,  1.66it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 95/270 [01:48<01:40,  1.74it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/270 [01:49<01:36,  1.80it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/270 [01:50<01:34,  1.83it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 98/270 [01:50<01:33,  1.85it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 99/270 [01:51<01:31,  1.87it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:51<01:29,  1.90it/s]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:51<01:29,  1.90it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/270 [01:52<01:28,  1.91it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 102/270 [01:52<01:27,  1.91it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 103/270 [01:53<01:26,  1.92it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 104/270 [01:53<01:25,  1.93it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 105/270 [01:54<01:25,  1.93it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 106/270 [01:54<01:25,  1.93it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 107/270 [01:55<01:24,  1.93it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:55<01:16,  2.12it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0007334970869123936, 'eval_f1': 1.0, 'eval_runtime': 1.3894, 'eval_samples_per_second': 215.925, 'eval_steps_per_second': 3.599, 'epoch': 3.0}
{'loss': 0.0019, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.09it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.93it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.81it/s][A                                                 
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:56<01:16,  2.12it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.81it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-135] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 109/270 [02:13<15:31,  5.78s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 110/270 [02:14<11:13,  4.21s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/270 [02:14<08:12,  3.10s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/270 [02:15<06:07,  2.33s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/270 [02:15<04:39,  1.78s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/270 [02:16<03:39,  1.41s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 115/270 [02:16<02:57,  1.14s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 116/270 [02:17<02:27,  1.05it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 117/270 [02:17<02:06,  1.21it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/270 [02:18<01:51,  1.37it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119/270 [02:18<01:40,  1.50it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120/270 [02:19<01:33,  1.61it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/270 [02:19<01:28,  1.69it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 122/270 [02:20<01:24,  1.75it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 123/270 [02:21<01:22,  1.78it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 124/270 [02:21<01:20,  1.81it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 125/270 [02:22<01:19,  1.83it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126/270 [02:22<01:17,  1.85it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 127/270 [02:23<01:16,  1.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 128/270 [02:23<01:15,  1.89it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 129/270 [02:24<01:14,  1.90it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 130/270 [02:24<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/270 [02:25<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 132/270 [02:25<01:12,  1.89it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 133/270 [02:26<01:12,  1.89it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 134/270 [02:26<01:12,  1.88it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:27<01:05,  2.05it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.000544012407772243, 'eval_f1': 1.0, 'eval_runtime': 1.3789, 'eval_samples_per_second': 217.567, 'eval_steps_per_second': 3.626, 'epoch': 4.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.78it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.86it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.88it/s][A                                                 
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:28<01:05,  2.05it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.88it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-162] due to args.save_total_limit
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 136/270 [02:45<13:17,  5.95s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137/270 [02:46<09:34,  4.32s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 138/270 [02:46<07:00,  3.18s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/270 [02:47<05:12,  2.39s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/270 [02:47<03:57,  1.82s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/270 [02:48<03:04,  1.43s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 142/270 [02:49<02:28,  1.16s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 143/270 [02:49<02:02,  1.04it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144/270 [02:50<01:44,  1.20it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/270 [02:50<01:32,  1.35it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 146/270 [02:51<01:23,  1.49it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 147/270 [02:51<01:16,  1.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/270 [02:52<01:12,  1.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/270 [02:52<01:09,  1.75it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:53<01:06,  1.80it/s]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:53<01:06,  1.80it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 151/270 [02:53<01:05,  1.82it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/270 [02:54<01:04,  1.84it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153/270 [02:54<01:03,  1.85it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 154/270 [02:55<01:02,  1.86it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 155/270 [02:55<01:01,  1.88it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 156/270 [02:56<00:59,  1.91it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157/270 [02:56<00:59,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 158/270 [02:57<00:58,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 159/270 [02:57<00:57,  1.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 160/270 [02:58<00:57,  1.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 161/270 [02:58<00:56,  1.93it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [02:59<00:51,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.00044671475188806653, 'eval_f1': 1.0, 'eval_runtime': 1.5267, 'eval_samples_per_second': 196.501, 'eval_steps_per_second': 3.275, 'epoch': 5.0}
{'loss': 0.001, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.76it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.71it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.63it/s][A                                                 
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:00<00:51,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.63it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json
Deleting older checkpoint [training-output/checkpoint-189] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-27 (score: 1.0).
                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:17<00:51,  2.09it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:17<02:11,  1.22s/it]
{'eval_loss': 0.0003931555838789791, 'eval_f1': 1.0, 'eval_runtime': 1.3705, 'eval_samples_per_second': 218.89, 'eval_steps_per_second': 3.648, 'epoch': 6.0}
{'train_runtime': 197.614, 'train_samples_per_second': 86.026, 'train_steps_per_second': 1.366, 'train_loss': 0.07720300831176617, 'epoch': 6.0}
  0%|          | 0/75 [00:00<?, ?it/s]  7%|â–‹         | 5/75 [00:00<00:01, 42.84it/s] 13%|â–ˆâ–Ž        | 10/75 [00:00<00:01, 44.64it/s] 20%|â–ˆâ–ˆ        | 15/75 [00:00<00:01, 45.95it/s] 27%|â–ˆâ–ˆâ–‹       | 20/75 [00:00<00:01, 47.15it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 25/75 [00:00<00:01, 48.10it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 30/75 [00:00<00:00, 48.69it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 35/75 [00:00<00:00, 49.09it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 40/75 [00:00<00:00, 49.35it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 45/75 [00:00<00:00, 49.52it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 50/75 [00:01<00:00, 49.66it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 55/75 [00:01<00:00, 49.72it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 60/75 [00:01<00:00, 49.68it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 65/75 [00:01<00:00, 49.50it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 70/75 [00:01<00:00, 49.44it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 49.49it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 48.74it/s]
Model finished with accuracy: 1.0, macro-f1: 1.0
Confusion matrix:
[[147   0]
 [  0 153]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
explanations.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.73ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.69ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 40.23ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:05<26:10,  5.84s/it]  1%|          | 2/270 [00:06<13:14,  2.96s/it]  1%|          | 3/270 [00:07<08:11,  1.84s/it]  1%|â–         | 4/270 [00:07<05:50,  1.32s/it]  2%|â–         | 5/270 [00:08<04:33,  1.03s/it]  2%|â–         | 6/270 [00:08<03:47,  1.16it/s]  3%|â–Ž         | 7/270 [00:09<03:17,  1.33it/s]  3%|â–Ž         | 8/270 [00:09<02:57,  1.48it/s]  3%|â–Ž         | 9/270 [00:10<02:43,  1.60it/s]  4%|â–Ž         | 10/270 [00:10<02:33,  1.69it/s]  4%|â–         | 11/270 [00:11<02:27,  1.76it/s]  4%|â–         | 12/270 [00:11<02:22,  1.80it/s]  5%|â–         | 13/270 [00:12<02:19,  1.85it/s]  5%|â–Œ         | 14/270 [00:13<02:17,  1.86it/s]  6%|â–Œ         | 15/270 [00:13<02:15,  1.88it/s]  6%|â–Œ         | 16/270 [00:14<02:15,  1.88it/s]  6%|â–‹         | 17/270 [00:14<02:13,  1.89it/s]  7%|â–‹         | 18/270 [00:15<02:13,  1.89it/s]  7%|â–‹         | 19/270 [00:15<02:12,  1.90it/s]  7%|â–‹         | 20/270 [00:16<02:11,  1.90it/s]  8%|â–Š         | 21/270 [00:16<02:23,  1.74it/s]  8%|â–Š         | 22/270 [00:17<02:17,  1.80it/s]  9%|â–Š         | 23/270 [00:17<02:13,  1.84it/s]  9%|â–‰         | 24/270 [00:18<02:11,  1.87it/s]  9%|â–‰         | 25/270 [00:18<02:10,  1.88it/s] 10%|â–‰         | 26/270 [00:19<02:09,  1.89it/s] 10%|â–ˆ         | 27/270 [00:19<01:57,  2.06it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.04it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  7.03it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  7.04it/s][A                                                
                                             [A 10%|â–ˆ         | 27/270 [00:21<01:57,  2.06it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  7.04it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|â–ˆ         | 28/270 [00:37<23:11,  5.75s/it] 11%|â–ˆ         | 29/270 [00:38<16:48,  4.19s/it] 11%|â–ˆ         | 30/270 [00:38<12:20,  3.09s/it] 11%|â–ˆâ–        | 31/270 [00:39<09:13,  2.31s/it] 12%|â–ˆâ–        | 32/270 [00:39<07:02,  1.77s/it] 12%|â–ˆâ–        | 33/270 [00:40<05:31,  1.40s/it] 13%|â–ˆâ–Ž        | 34/270 [00:40<04:28,  1.14s/it] 13%|â–ˆâ–Ž        | 35/270 [00:41<03:44,  1.05it/s] 13%|â–ˆâ–Ž        | 36/270 [00:42<03:12,  1.21it/s] 14%|â–ˆâ–Ž        | 37/270 [00:42<02:50,  1.36it/s] 14%|â–ˆâ–        | 38/270 [00:43<02:35,  1.49it/s] 14%|â–ˆâ–        | 39/270 [00:43<02:24,  1.60it/s] 15%|â–ˆâ–        | 40/270 [00:44<02:16,  1.68it/s] 15%|â–ˆâ–Œ        | 41/270 [00:44<02:11,  1.74it/s] 16%|â–ˆâ–Œ        | 42/270 [00:45<02:07,  1.79it/s] 16%|â–ˆâ–Œ        | 43/270 [00:45<02:04,  1.83it/s] 16%|â–ˆâ–‹        | 44/270 [00:46<02:01,  1.86it/s] 17%|â–ˆâ–‹        | 45/270 [00:46<02:00,  1.87it/s] 17%|â–ˆâ–‹        | 46/270 [00:47<01:58,  1.89it/s] 17%|â–ˆâ–‹        | 47/270 [00:47<01:56,  1.91it/s] 18%|â–ˆâ–Š        | 48/270 [00:48<01:56,  1.91it/s] 18%|â–ˆâ–Š        | 49/270 [00:48<01:55,  1.92it/s] 19%|â–ˆâ–Š        | 50/270 [00:49<01:55,  1.91it/s]                                                 19%|â–ˆâ–Š        | 50/270 [00:49<01:55,  1.91it/s] 19%|â–ˆâ–‰        | 51/270 [00:49<01:55,  1.89it/s] 19%|â–ˆâ–‰        | 52/270 [00:50<01:55,  1.88it/s] 20%|â–ˆâ–‰        | 53/270 [00:50<01:55,  1.88it/s] 20%|â–ˆâ–ˆ        | 54/270 [00:51<01:45,  2.06it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0376802459359169, 'eval_f1': 1.0, 'eval_runtime': 1.3271, 'eval_samples_per_second': 226.059, 'eval_steps_per_second': 3.768, 'epoch': 1.0}
{'loss': 0.2805, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.64it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.65it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.58it/s][A                                                
                                             [A 20%|â–ˆâ–ˆ        | 54/270 [00:52<01:45,  2.06it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.58it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|â–ˆâ–ˆ        | 55/270 [01:13<25:09,  7.02s/it] 21%|â–ˆâ–ˆ        | 56/270 [01:14<18:04,  5.07s/it] 21%|â–ˆâ–ˆ        | 57/270 [01:14<13:08,  3.70s/it] 21%|â–ˆâ–ˆâ–       | 58/270 [01:15<09:41,  2.74s/it] 22%|â–ˆâ–ˆâ–       | 59/270 [01:15<07:17,  2.08s/it] 22%|â–ˆâ–ˆâ–       | 60/270 [01:16<05:38,  1.61s/it] 23%|â–ˆâ–ˆâ–Ž       | 61/270 [01:16<04:28,  1.29s/it] 23%|â–ˆâ–ˆâ–Ž       | 62/270 [01:17<03:40,  1.06s/it] 23%|â–ˆâ–ˆâ–Ž       | 63/270 [01:17<03:06,  1.11it/s] 24%|â–ˆâ–ˆâ–Ž       | 64/270 [01:18<02:41,  1.28it/s] 24%|â–ˆâ–ˆâ–       | 65/270 [01:18<02:23,  1.43it/s] 24%|â–ˆâ–ˆâ–       | 66/270 [01:19<02:11,  1.55it/s] 25%|â–ˆâ–ˆâ–       | 67/270 [01:19<02:04,  1.63it/s] 25%|â–ˆâ–ˆâ–Œ       | 68/270 [01:20<01:58,  1.70it/s] 26%|â–ˆâ–ˆâ–Œ       | 69/270 [01:20<01:54,  1.76it/s] 26%|â–ˆâ–ˆâ–Œ       | 70/270 [01:21<01:50,  1.81it/s] 26%|â–ˆâ–ˆâ–‹       | 71/270 [01:21<01:47,  1.84it/s] 27%|â–ˆâ–ˆâ–‹       | 72/270 [01:22<01:46,  1.86it/s] 27%|â–ˆâ–ˆâ–‹       | 73/270 [01:22<01:45,  1.87it/s] 27%|â–ˆâ–ˆâ–‹       | 74/270 [01:23<01:44,  1.88it/s] 28%|â–ˆâ–ˆâ–Š       | 75/270 [01:24<01:43,  1.89it/s] 28%|â–ˆâ–ˆâ–Š       | 76/270 [01:24<01:41,  1.90it/s] 29%|â–ˆâ–ˆâ–Š       | 77/270 [01:25<01:41,  1.90it/s] 29%|â–ˆâ–ˆâ–‰       | 78/270 [01:25<01:40,  1.91it/s] 29%|â–ˆâ–ˆâ–‰       | 79/270 [01:26<01:40,  1.89it/s] 30%|â–ˆâ–ˆâ–‰       | 80/270 [01:26<01:40,  1.89it/s] 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:27<01:31,  2.06it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0013494372833520174, 'eval_f1': 1.0, 'eval_runtime': 1.386, 'eval_samples_per_second': 216.444, 'eval_steps_per_second': 3.607, 'epoch': 2.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.45it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.63it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.57it/s][A                                                
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:28<01:31,  2.06it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.57it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|â–ˆâ–ˆâ–ˆ       | 82/270 [01:45<18:09,  5.79s/it] 31%|â–ˆâ–ˆâ–ˆ       | 83/270 [01:45<13:07,  4.21s/it] 31%|â–ˆâ–ˆâ–ˆ       | 84/270 [01:46<09:37,  3.10s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 85/270 [01:46<07:10,  2.33s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 86/270 [01:47<05:27,  1.78s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 87/270 [01:47<04:16,  1.40s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 88/270 [01:48<03:26,  1.14s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 89/270 [01:48<02:52,  1.05it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/270 [01:49<02:29,  1.21it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/270 [01:49<02:12,  1.35it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 92/270 [01:50<02:00,  1.48it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 93/270 [01:50<01:50,  1.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 94/270 [01:51<01:44,  1.68it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 95/270 [01:51<01:39,  1.76it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/270 [01:52<01:36,  1.81it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/270 [01:53<01:34,  1.83it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 98/270 [01:53<01:33,  1.85it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 99/270 [01:54<01:32,  1.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:54<01:30,  1.87it/s]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:54<01:30,  1.87it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/270 [01:55<01:29,  1.89it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 102/270 [01:55<01:28,  1.90it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 103/270 [01:56<01:27,  1.90it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 104/270 [01:56<01:27,  1.90it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 105/270 [01:57<01:26,  1.90it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 106/270 [01:57<01:26,  1.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 107/270 [01:58<01:25,  1.92it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:58<01:17,  2.10it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0006879334105178714, 'eval_f1': 1.0, 'eval_runtime': 1.3817, 'eval_samples_per_second': 217.119, 'eval_steps_per_second': 3.619, 'epoch': 3.0}
{'loss': 0.0019, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.52it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.76it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.83it/s][A                                                 
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:59<01:17,  2.10it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.83it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 109/270 [02:21<19:02,  7.09s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 110/270 [02:21<13:39,  5.12s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/270 [02:22<09:54,  3.74s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/270 [02:22<07:17,  2.77s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/270 [02:23<05:29,  2.10s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/270 [02:23<04:13,  1.62s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 115/270 [02:24<03:20,  1.29s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 116/270 [02:24<02:44,  1.07s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 117/270 [02:25<02:18,  1.11it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/270 [02:25<01:59,  1.27it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119/270 [02:26<01:46,  1.42it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120/270 [02:26<01:37,  1.54it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/270 [02:27<01:30,  1.64it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 122/270 [02:27<01:26,  1.72it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 123/270 [02:28<01:22,  1.79it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 124/270 [02:28<01:19,  1.84it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 125/270 [02:29<01:17,  1.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126/270 [02:29<01:17,  1.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 127/270 [02:30<01:16,  1.88it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 128/270 [02:31<01:15,  1.88it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 129/270 [02:31<01:14,  1.89it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 130/270 [02:32<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/270 [02:32<01:12,  1.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 132/270 [02:33<01:12,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 133/270 [02:33<01:11,  1.91it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 134/270 [02:34<01:11,  1.91it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:34<01:04,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0005201325984671712, 'eval_f1': 1.0, 'eval_runtime': 1.3257, 'eval_samples_per_second': 226.293, 'eval_steps_per_second': 3.772, 'epoch': 4.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.93it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.96it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.97it/s][A                                                 
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:35<01:04,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.97it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 136/270 [02:52<12:48,  5.73s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137/270 [02:53<09:14,  4.17s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 138/270 [02:53<06:52,  3.12s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/270 [02:54<05:06,  2.34s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/270 [02:54<03:53,  1.79s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/270 [02:55<03:02,  1.41s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 142/270 [02:55<02:26,  1.15s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 143/270 [02:56<02:01,  1.05it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144/270 [02:56<01:43,  1.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/270 [02:57<01:31,  1.36it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 146/270 [02:57<01:23,  1.49it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 147/270 [02:58<01:17,  1.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/270 [02:58<01:12,  1.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/270 [02:59<01:09,  1.74it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:59<01:06,  1.80it/s]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:59<01:06,  1.80it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 151/270 [03:00<01:04,  1.84it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/270 [03:01<01:03,  1.86it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153/270 [03:01<01:02,  1.88it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 154/270 [03:02<01:01,  1.88it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 155/270 [03:02<01:00,  1.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 156/270 [03:03<01:00,  1.89it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157/270 [03:03<00:59,  1.90it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 158/270 [03:04<00:58,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 159/270 [03:04<00:57,  1.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 160/270 [03:05<00:56,  1.94it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 161/270 [03:05<00:56,  1.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:06<00:51,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.00042296425090171397, 'eval_f1': 1.0, 'eval_runtime': 1.3357, 'eval_samples_per_second': 224.596, 'eval_steps_per_second': 3.743, 'epoch': 5.0}
{'loss': 0.0009, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.40it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.72it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.79it/s][A                                                 
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:07<00:51,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.79it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-27 (score: 1.0).
                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:24<00:51,  2.09it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:24<02:16,  1.26s/it]
{'eval_loss': 0.0003710992750711739, 'eval_f1': 1.0, 'eval_runtime': 1.3352, 'eval_samples_per_second': 224.68, 'eval_steps_per_second': 3.745, 'epoch': 6.0}
{'train_runtime': 204.381, 'train_samples_per_second': 83.178, 'train_steps_per_second': 1.321, 'train_loss': 0.08752207056745703, 'epoch': 6.0}
  0%|          | 0/75 [00:00<?, ?it/s]  7%|â–‹         | 5/75 [00:00<00:01, 49.14it/s] 13%|â–ˆâ–Ž        | 10/75 [00:00<00:01, 49.43it/s] 20%|â–ˆâ–ˆ        | 15/75 [00:00<00:01, 49.69it/s] 28%|â–ˆâ–ˆâ–Š       | 21/75 [00:00<00:01, 49.83it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 27/75 [00:00<00:00, 49.90it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 33/75 [00:00<00:00, 49.97it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 39/75 [00:00<00:00, 50.00it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 45/75 [00:00<00:00, 50.01it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 51/75 [00:01<00:00, 49.72it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 56/75 [00:01<00:00, 49.42it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 61/75 [00:01<00:00, 49.40it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 66/75 [00:01<00:00, 49.51it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 71/75 [00:01<00:00, 49.59it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 49.68it/s]
Model finished with accuracy: 1.0, macro-f1: 1.0
Confusion matrix:
[[147   0]
 [  0 153]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
explanations.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.90ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.86ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 49.46ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:06<27:42,  6.18s/it]  1%|          | 2/270 [00:07<13:49,  3.10s/it]  1%|          | 3/270 [00:07<08:31,  1.92s/it]  1%|â–         | 4/270 [00:08<06:01,  1.36s/it]  2%|â–         | 5/270 [00:08<04:39,  1.05s/it]  2%|â–         | 6/270 [00:09<03:49,  1.15it/s]  3%|â–Ž         | 7/270 [00:09<03:17,  1.33it/s]  3%|â–Ž         | 8/270 [00:10<02:57,  1.47it/s]  3%|â–Ž         | 9/270 [00:10<02:44,  1.59it/s]  4%|â–Ž         | 10/270 [00:11<02:33,  1.69it/s]  4%|â–         | 11/270 [00:11<02:27,  1.76it/s]  4%|â–         | 12/270 [00:12<02:22,  1.81it/s]  5%|â–         | 13/270 [00:12<02:20,  1.83it/s]  5%|â–Œ         | 14/270 [00:13<02:17,  1.86it/s]  6%|â–Œ         | 15/270 [00:13<02:15,  1.88it/s]  6%|â–Œ         | 16/270 [00:14<02:14,  1.89it/s]  6%|â–‹         | 17/270 [00:14<02:11,  1.92it/s]  7%|â–‹         | 18/270 [00:15<02:11,  1.91it/s]  7%|â–‹         | 19/270 [00:15<02:11,  1.91it/s]  7%|â–‹         | 20/270 [00:16<02:10,  1.91it/s]  8%|â–Š         | 21/270 [00:17<02:19,  1.78it/s]  8%|â–Š         | 22/270 [00:17<02:16,  1.82it/s]  9%|â–Š         | 23/270 [00:18<02:13,  1.85it/s]  9%|â–‰         | 24/270 [00:18<02:12,  1.86it/s]  9%|â–‰         | 25/270 [00:19<02:11,  1.87it/s] 10%|â–‰         | 26/270 [00:19<02:11,  1.86it/s] 10%|â–ˆ         | 27/270 [00:20<02:00,  2.02it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.52it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.71it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.70it/s][A                                                
                                             [A 10%|â–ˆ         | 27/270 [00:21<02:00,  2.02it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.70it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|â–ˆ         | 28/270 [00:42<28:07,  6.97s/it] 11%|â–ˆ         | 29/270 [00:42<20:13,  5.04s/it] 11%|â–ˆ         | 30/270 [00:43<14:43,  3.68s/it] 11%|â–ˆâ–        | 31/270 [00:43<10:53,  2.74s/it] 12%|â–ˆâ–        | 32/270 [00:44<08:13,  2.07s/it] 12%|â–ˆâ–        | 33/270 [00:44<06:20,  1.60s/it] 13%|â–ˆâ–Ž        | 34/270 [00:45<05:01,  1.28s/it] 13%|â–ˆâ–Ž        | 35/270 [00:45<04:06,  1.05s/it] 13%|â–ˆâ–Ž        | 36/270 [00:46<03:28,  1.12it/s] 14%|â–ˆâ–Ž        | 37/270 [00:46<03:02,  1.28it/s] 14%|â–ˆâ–        | 38/270 [00:47<02:43,  1.42it/s] 14%|â–ˆâ–        | 39/270 [00:47<02:30,  1.53it/s] 15%|â–ˆâ–        | 40/270 [00:48<02:20,  1.64it/s] 15%|â–ˆâ–Œ        | 41/270 [00:48<02:13,  1.71it/s] 16%|â–ˆâ–Œ        | 42/270 [00:49<02:08,  1.78it/s] 16%|â–ˆâ–Œ        | 43/270 [00:49<02:04,  1.83it/s] 16%|â–ˆâ–‹        | 44/270 [00:50<02:02,  1.85it/s] 17%|â–ˆâ–‹        | 45/270 [00:51<02:00,  1.86it/s] 17%|â–ˆâ–‹        | 46/270 [00:51<02:00,  1.86it/s] 17%|â–ˆâ–‹        | 47/270 [00:52<01:58,  1.88it/s] 18%|â–ˆâ–Š        | 48/270 [00:52<01:56,  1.90it/s] 18%|â–ˆâ–Š        | 49/270 [00:53<01:55,  1.91it/s] 19%|â–ˆâ–Š        | 50/270 [00:53<01:54,  1.92it/s]                                                 19%|â–ˆâ–Š        | 50/270 [00:53<01:54,  1.92it/s] 19%|â–ˆâ–‰        | 51/270 [00:54<01:54,  1.90it/s] 19%|â–ˆâ–‰        | 52/270 [00:54<01:54,  1.91it/s] 20%|â–ˆâ–‰        | 53/270 [00:55<01:54,  1.89it/s] 20%|â–ˆâ–ˆ        | 54/270 [00:55<01:44,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.017788734287023544, 'eval_f1': 1.0, 'eval_runtime': 1.6329, 'eval_samples_per_second': 183.725, 'eval_steps_per_second': 3.062, 'epoch': 1.0}
{'loss': 0.2147, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.88it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.72it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.83it/s][A                                                
                                             [A 20%|â–ˆâ–ˆ        | 54/270 [00:56<01:44,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.83it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|â–ˆâ–ˆ        | 55/270 [01:13<20:41,  5.77s/it] 21%|â–ˆâ–ˆ        | 56/270 [01:14<14:57,  4.19s/it] 21%|â–ˆâ–ˆ        | 57/270 [01:14<10:58,  3.09s/it] 21%|â–ˆâ–ˆâ–       | 58/270 [01:15<08:10,  2.32s/it] 22%|â–ˆâ–ˆâ–       | 59/270 [01:15<06:14,  1.78s/it] 22%|â–ˆâ–ˆâ–       | 60/270 [01:16<04:54,  1.40s/it] 23%|â–ˆâ–ˆâ–Ž       | 61/270 [01:16<03:57,  1.14s/it] 23%|â–ˆâ–ˆâ–Ž       | 62/270 [01:17<03:17,  1.06it/s] 23%|â–ˆâ–ˆâ–Ž       | 63/270 [01:17<02:48,  1.23it/s] 24%|â–ˆâ–ˆâ–Ž       | 64/270 [01:18<02:30,  1.37it/s] 24%|â–ˆâ–ˆâ–       | 65/270 [01:18<02:17,  1.49it/s] 24%|â–ˆâ–ˆâ–       | 66/270 [01:19<02:07,  1.60it/s] 25%|â–ˆâ–ˆâ–       | 67/270 [01:19<02:00,  1.69it/s] 25%|â–ˆâ–ˆâ–Œ       | 68/270 [01:20<01:55,  1.74it/s] 26%|â–ˆâ–ˆâ–Œ       | 69/270 [01:20<01:51,  1.80it/s] 26%|â–ˆâ–ˆâ–Œ       | 70/270 [01:21<01:48,  1.84it/s] 26%|â–ˆâ–ˆâ–‹       | 71/270 [01:22<01:46,  1.87it/s] 27%|â–ˆâ–ˆâ–‹       | 72/270 [01:22<01:44,  1.90it/s] 27%|â–ˆâ–ˆâ–‹       | 73/270 [01:23<01:43,  1.90it/s] 27%|â–ˆâ–ˆâ–‹       | 74/270 [01:23<01:43,  1.90it/s] 28%|â–ˆâ–ˆâ–Š       | 75/270 [01:24<01:42,  1.90it/s] 28%|â–ˆâ–ˆâ–Š       | 76/270 [01:24<01:40,  1.93it/s] 29%|â–ˆâ–ˆâ–Š       | 77/270 [01:25<01:39,  1.94it/s] 29%|â–ˆâ–ˆâ–‰       | 78/270 [01:25<01:39,  1.93it/s] 29%|â–ˆâ–ˆâ–‰       | 79/270 [01:26<01:39,  1.92it/s] 30%|â–ˆâ–ˆâ–‰       | 80/270 [01:26<01:39,  1.91it/s] 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:27<01:30,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0013073708396404982, 'eval_f1': 1.0, 'eval_runtime': 1.3325, 'eval_samples_per_second': 225.143, 'eval_steps_per_second': 3.752, 'epoch': 2.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.04it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  7.00it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.94it/s][A                                                
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:28<01:30,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.94it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|â–ˆâ–ˆâ–ˆ       | 82/270 [01:45<18:10,  5.80s/it] 31%|â–ˆâ–ˆâ–ˆ       | 83/270 [01:45<13:07,  4.21s/it] 31%|â–ˆâ–ˆâ–ˆ       | 84/270 [01:46<09:36,  3.10s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 85/270 [01:46<07:10,  2.33s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 86/270 [01:47<05:28,  1.78s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 87/270 [01:47<04:17,  1.40s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 88/270 [01:48<03:27,  1.14s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 89/270 [01:48<02:53,  1.04it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/270 [01:49<02:29,  1.21it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/270 [01:49<02:11,  1.36it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 92/270 [01:50<01:59,  1.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 93/270 [01:50<01:49,  1.61it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 94/270 [01:51<01:44,  1.68it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 95/270 [01:52<01:39,  1.75it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/270 [01:52<01:37,  1.79it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/270 [01:53<01:34,  1.83it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 98/270 [01:53<01:32,  1.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 99/270 [01:54<01:30,  1.88it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:54<01:29,  1.89it/s]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:54<01:29,  1.89it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/270 [01:55<01:29,  1.90it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 102/270 [01:55<01:28,  1.90it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 103/270 [01:56<01:28,  1.89it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 104/270 [01:56<01:27,  1.89it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 105/270 [01:57<01:26,  1.91it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 106/270 [01:57<01:25,  1.92it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 107/270 [01:58<01:25,  1.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:58<01:18,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0007055023452267051, 'eval_f1': 1.0, 'eval_runtime': 1.3407, 'eval_samples_per_second': 223.771, 'eval_steps_per_second': 3.73, 'epoch': 3.0}
{'loss': 0.0018, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.32it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.57it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.54it/s][A                                                 
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [02:00<01:18,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.54it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 109/270 [02:20<18:10,  6.78s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 110/270 [02:20<13:03,  4.90s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/270 [02:21<09:29,  3.58s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/270 [02:21<07:01,  2.67s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/270 [02:22<05:17,  2.02s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/270 [02:22<04:05,  1.57s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 115/270 [02:23<03:15,  1.26s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 116/270 [02:23<02:40,  1.04s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 117/270 [02:24<02:15,  1.13it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/270 [02:24<01:56,  1.30it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119/270 [02:25<01:44,  1.44it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120/270 [02:25<01:36,  1.55it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/270 [02:26<01:30,  1.65it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 122/270 [02:26<01:26,  1.72it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 123/270 [02:27<01:22,  1.78it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 124/270 [02:27<01:19,  1.83it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 125/270 [02:28<01:17,  1.86it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126/270 [02:29<01:17,  1.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 127/270 [02:29<01:16,  1.88it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 128/270 [02:30<01:15,  1.89it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 129/270 [02:30<01:14,  1.90it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 130/270 [02:31<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/270 [02:31<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 132/270 [02:32<01:12,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 133/270 [02:32<01:11,  1.91it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 134/270 [02:33<01:10,  1.93it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:33<01:04,  2.10it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.000530284596607089, 'eval_f1': 1.0, 'eval_runtime': 1.3743, 'eval_samples_per_second': 218.301, 'eval_steps_per_second': 3.638, 'epoch': 4.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.13it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.81it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.70it/s][A                                                 
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:34<01:04,  2.10it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.70it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 136/270 [02:51<13:06,  5.87s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137/270 [02:52<09:26,  4.26s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 138/270 [02:53<07:00,  3.19s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/270 [02:53<05:12,  2.39s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/270 [02:54<03:57,  1.83s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/270 [02:54<03:05,  1.44s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 142/270 [02:55<02:28,  1.16s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 143/270 [02:55<02:02,  1.03it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144/270 [02:56<01:44,  1.20it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/270 [02:56<01:32,  1.36it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 146/270 [02:57<01:23,  1.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 147/270 [02:57<01:17,  1.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/270 [02:58<01:13,  1.67it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/270 [02:58<01:09,  1.73it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:59<01:07,  1.78it/s]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:59<01:07,  1.78it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 151/270 [03:00<01:06,  1.80it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/270 [03:00<01:04,  1.82it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153/270 [03:01<01:03,  1.85it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 154/270 [03:01<01:01,  1.89it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 155/270 [03:02<01:01,  1.88it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 156/270 [03:02<01:00,  1.89it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157/270 [03:03<01:00,  1.88it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 158/270 [03:03<00:59,  1.90it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 159/270 [03:04<00:58,  1.90it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 160/270 [03:04<00:57,  1.90it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 161/270 [03:05<00:57,  1.90it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:05<00:52,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0004393009003251791, 'eval_f1': 1.0, 'eval_runtime': 1.3573, 'eval_samples_per_second': 221.029, 'eval_steps_per_second': 3.684, 'epoch': 5.0}
{'loss': 0.001, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.85it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.72it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.65it/s][A                                                 
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:07<00:52,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.65it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-27 (score: 1.0).
                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:48<00:52,  2.07it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:48<02:32,  1.41s/it]
{'eval_loss': 0.0003874524845741689, 'eval_f1': 1.0, 'eval_runtime': 1.3692, 'eval_samples_per_second': 219.103, 'eval_steps_per_second': 3.652, 'epoch': 6.0}
{'train_runtime': 228.6995, 'train_samples_per_second': 74.333, 'train_steps_per_second': 1.181, 'train_loss': 0.06718668702092988, 'epoch': 6.0}
  0%|          | 0/75 [00:00<?, ?it/s]  7%|â–‹         | 5/75 [00:00<00:01, 44.61it/s] 13%|â–ˆâ–Ž        | 10/75 [00:00<00:01, 46.53it/s] 20%|â–ˆâ–ˆ        | 15/75 [00:00<00:01, 48.03it/s] 28%|â–ˆâ–ˆâ–Š       | 21/75 [00:00<00:01, 49.10it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 27/75 [00:00<00:00, 49.54it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 33/75 [00:00<00:00, 49.83it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 39/75 [00:00<00:00, 49.93it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 45/75 [00:00<00:00, 49.98it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 51/75 [00:01<00:00, 50.08it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 57/75 [00:01<00:00, 49.87it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 62/75 [00:01<00:00, 49.69it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 68/75 [00:01<00:00, 49.82it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 74/75 [00:01<00:00, 49.94it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 49.50it/s]
Model finished with accuracy: 1.0, macro-f1: 1.0
Confusion matrix:
[[143   0]
 [  0 157]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
explanations.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.97ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.94ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 42.04ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:05<23:45,  5.30s/it]  1%|          | 2/270 [00:05<11:33,  2.59s/it]  1%|          | 3/270 [00:06<07:19,  1.64s/it]  1%|â–         | 4/270 [00:07<05:19,  1.20s/it]  2%|â–         | 5/270 [00:07<04:11,  1.05it/s]  2%|â–         | 6/270 [00:08<03:31,  1.25it/s]  3%|â–Ž         | 7/270 [00:08<03:06,  1.41it/s]  3%|â–Ž         | 8/270 [00:09<02:49,  1.54it/s]  3%|â–Ž         | 9/270 [00:09<02:38,  1.65it/s]  4%|â–Ž         | 10/270 [00:10<02:29,  1.74it/s]  4%|â–         | 11/270 [00:10<02:25,  1.78it/s]  4%|â–         | 12/270 [00:11<02:20,  1.83it/s]  5%|â–         | 13/270 [00:11<02:17,  1.87it/s]  5%|â–Œ         | 14/270 [00:12<02:15,  1.89it/s]  6%|â–Œ         | 15/270 [00:12<02:14,  1.90it/s]  6%|â–Œ         | 16/270 [00:13<02:12,  1.91it/s]  6%|â–‹         | 17/270 [00:13<02:12,  1.91it/s]  7%|â–‹         | 18/270 [00:14<02:12,  1.91it/s]  7%|â–‹         | 19/270 [00:14<02:11,  1.91it/s]  7%|â–‹         | 20/270 [00:15<02:10,  1.92it/s]  8%|â–Š         | 21/270 [00:15<02:20,  1.77it/s]  8%|â–Š         | 22/270 [00:16<02:16,  1.82it/s]  9%|â–Š         | 23/270 [00:16<02:13,  1.85it/s]  9%|â–‰         | 24/270 [00:17<02:11,  1.87it/s]  9%|â–‰         | 25/270 [00:18<02:09,  1.89it/s] 10%|â–‰         | 26/270 [00:18<02:07,  1.92it/s] 10%|â–ˆ         | 27/270 [00:18<01:56,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.09it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.97it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.89it/s][A                                                
                                             [A 10%|â–ˆ         | 27/270 [00:20<01:56,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.89it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|â–ˆ         | 28/270 [00:37<23:27,  5.82s/it] 11%|â–ˆ         | 29/270 [00:37<16:59,  4.23s/it] 11%|â–ˆ         | 30/270 [00:38<12:28,  3.12s/it] 11%|â–ˆâ–        | 31/270 [00:38<09:18,  2.34s/it] 12%|â–ˆâ–        | 32/270 [00:39<07:05,  1.79s/it] 12%|â–ˆâ–        | 33/270 [00:39<05:34,  1.41s/it] 13%|â–ˆâ–Ž        | 34/270 [00:40<04:29,  1.14s/it] 13%|â–ˆâ–Ž        | 35/270 [00:40<03:45,  1.04it/s] 13%|â–ˆâ–Ž        | 36/270 [00:41<03:13,  1.21it/s] 14%|â–ˆâ–Ž        | 37/270 [00:41<02:50,  1.36it/s] 14%|â–ˆâ–        | 38/270 [00:42<02:35,  1.49it/s] 14%|â–ˆâ–        | 39/270 [00:42<02:24,  1.60it/s] 15%|â–ˆâ–        | 40/270 [00:43<02:16,  1.68it/s] 15%|â–ˆâ–Œ        | 41/270 [00:43<02:10,  1.75it/s] 16%|â–ˆâ–Œ        | 42/270 [00:44<02:06,  1.81it/s] 16%|â–ˆâ–Œ        | 43/270 [00:44<02:02,  1.85it/s] 16%|â–ˆâ–‹        | 44/270 [00:45<02:01,  1.87it/s] 17%|â–ˆâ–‹        | 45/270 [00:46<01:59,  1.88it/s] 17%|â–ˆâ–‹        | 46/270 [00:46<01:59,  1.88it/s] 17%|â–ˆâ–‹        | 47/270 [00:47<01:58,  1.89it/s] 18%|â–ˆâ–Š        | 48/270 [00:47<01:56,  1.91it/s] 18%|â–ˆâ–Š        | 49/270 [00:48<01:54,  1.93it/s] 19%|â–ˆâ–Š        | 50/270 [00:48<01:53,  1.94it/s]                                                 19%|â–ˆâ–Š        | 50/270 [00:48<01:53,  1.94it/s] 19%|â–ˆâ–‰        | 51/270 [00:49<01:53,  1.93it/s] 19%|â–ˆâ–‰        | 52/270 [00:49<01:53,  1.93it/s] 20%|â–ˆâ–‰        | 53/270 [00:50<01:52,  1.92it/s] 20%|â–ˆâ–ˆ        | 54/270 [00:50<01:42,  2.10it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0233454592525959, 'eval_f1': 1.0, 'eval_runtime': 1.3294, 'eval_samples_per_second': 225.66, 'eval_steps_per_second': 3.761, 'epoch': 1.0}
{'loss': 0.2336, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.92it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.98it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.88it/s][A                                                
                                             [A 20%|â–ˆâ–ˆ        | 54/270 [00:51<01:42,  2.10it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.88it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|â–ˆâ–ˆ        | 55/270 [01:08<20:44,  5.79s/it] 21%|â–ˆâ–ˆ        | 56/270 [01:09<15:01,  4.21s/it] 21%|â–ˆâ–ˆ        | 57/270 [01:09<11:01,  3.10s/it] 21%|â–ˆâ–ˆâ–       | 58/270 [01:10<08:13,  2.33s/it] 22%|â–ˆâ–ˆâ–       | 59/270 [01:10<06:16,  1.78s/it] 22%|â–ˆâ–ˆâ–       | 60/270 [01:11<04:54,  1.40s/it] 23%|â–ˆâ–ˆâ–Ž       | 61/270 [01:11<03:58,  1.14s/it] 23%|â–ˆâ–ˆâ–Ž       | 62/270 [01:12<03:17,  1.05it/s] 23%|â–ˆâ–ˆâ–Ž       | 63/270 [01:12<02:49,  1.22it/s] 24%|â–ˆâ–ˆâ–Ž       | 64/270 [01:13<02:30,  1.37it/s] 24%|â–ˆâ–ˆâ–       | 65/270 [01:13<02:17,  1.50it/s] 24%|â–ˆâ–ˆâ–       | 66/270 [01:14<02:07,  1.60it/s] 25%|â–ˆâ–ˆâ–       | 67/270 [01:14<01:59,  1.69it/s] 25%|â–ˆâ–ˆâ–Œ       | 68/270 [01:15<01:53,  1.77it/s] 26%|â–ˆâ–ˆâ–Œ       | 69/270 [01:15<01:50,  1.82it/s] 26%|â–ˆâ–ˆâ–Œ       | 70/270 [01:16<01:48,  1.85it/s] 26%|â–ˆâ–ˆâ–‹       | 71/270 [01:17<01:46,  1.87it/s] 27%|â–ˆâ–ˆâ–‹       | 72/270 [01:17<01:45,  1.88it/s] 27%|â–ˆâ–ˆâ–‹       | 73/270 [01:18<01:44,  1.89it/s] 27%|â–ˆâ–ˆâ–‹       | 74/270 [01:18<01:44,  1.88it/s] 28%|â–ˆâ–ˆâ–Š       | 75/270 [01:19<01:44,  1.87it/s] 28%|â–ˆâ–ˆâ–Š       | 76/270 [01:19<01:43,  1.88it/s] 29%|â–ˆâ–ˆâ–Š       | 77/270 [01:20<01:41,  1.90it/s] 29%|â–ˆâ–ˆâ–‰       | 78/270 [01:20<01:40,  1.92it/s] 29%|â–ˆâ–ˆâ–‰       | 79/270 [01:21<01:40,  1.90it/s] 30%|â–ˆâ–ˆâ–‰       | 80/270 [01:21<01:39,  1.90it/s] 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:22<01:30,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0013679955154657364, 'eval_f1': 1.0, 'eval_runtime': 1.3325, 'eval_samples_per_second': 225.138, 'eval_steps_per_second': 3.752, 'epoch': 2.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.89it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.69it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.75it/s][A                                                
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:23<01:30,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.75it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|â–ˆâ–ˆâ–ˆ       | 82/270 [01:46<23:45,  7.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 83/270 [01:46<17:00,  5.46s/it] 31%|â–ˆâ–ˆâ–ˆ       | 84/270 [01:47<12:19,  3.97s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 85/270 [01:47<09:03,  2.94s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 86/270 [01:48<06:46,  2.21s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 87/270 [01:48<05:11,  1.70s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 88/270 [01:49<04:04,  1.34s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 89/270 [01:49<03:17,  1.09s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/270 [01:50<02:46,  1.08it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/270 [01:50<02:24,  1.24it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 92/270 [01:51<02:07,  1.39it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 93/270 [01:51<01:55,  1.53it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 94/270 [01:52<01:47,  1.64it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 95/270 [01:52<01:42,  1.71it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/270 [01:53<01:38,  1.76it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/270 [01:54<01:35,  1.80it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 98/270 [01:54<01:33,  1.84it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 99/270 [01:55<01:31,  1.87it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:55<01:30,  1.89it/s]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:55<01:30,  1.89it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/270 [01:56<01:29,  1.89it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 102/270 [01:56<01:28,  1.90it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 103/270 [01:57<01:27,  1.90it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 104/270 [01:57<01:26,  1.92it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 105/270 [01:58<01:25,  1.93it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 106/270 [01:58<01:25,  1.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 107/270 [01:59<01:25,  1.90it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:59<01:18,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.000747514539398253, 'eval_f1': 1.0, 'eval_runtime': 1.5568, 'eval_samples_per_second': 192.703, 'eval_steps_per_second': 3.212, 'epoch': 3.0}
{'loss': 0.0018, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.62it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.42it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.46it/s][A                                                 
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [02:01<01:18,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.46it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 109/270 [02:17<15:31,  5.78s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 110/270 [02:18<11:12,  4.20s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/270 [02:18<08:12,  3.10s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/270 [02:19<06:07,  2.32s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/270 [02:19<04:39,  1.78s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/270 [02:20<03:38,  1.40s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 115/270 [02:20<02:55,  1.14s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 116/270 [02:21<02:26,  1.05it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 117/270 [02:21<02:06,  1.21it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/270 [02:22<01:51,  1.36it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119/270 [02:22<01:40,  1.50it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120/270 [02:23<01:32,  1.62it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/270 [02:23<01:27,  1.70it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 122/270 [02:24<01:23,  1.77it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 123/270 [02:24<01:21,  1.81it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 124/270 [02:25<01:19,  1.84it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 125/270 [02:26<01:17,  1.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126/270 [02:26<01:16,  1.89it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 127/270 [02:27<01:14,  1.92it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 128/270 [02:27<01:14,  1.91it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 129/270 [02:28<01:13,  1.91it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 130/270 [02:28<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/270 [02:29<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 132/270 [02:29<01:11,  1.92it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 133/270 [02:30<01:10,  1.93it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 134/270 [02:30<01:10,  1.93it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:31<01:04,  2.10it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0005504759028553963, 'eval_f1': 1.0, 'eval_runtime': 1.3973, 'eval_samples_per_second': 214.694, 'eval_steps_per_second': 3.578, 'epoch': 4.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.97it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.80it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.74it/s][A                                                 
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:32<01:04,  2.10it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.74it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 136/270 [02:49<13:05,  5.86s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137/270 [02:50<09:26,  4.26s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 138/270 [02:50<06:53,  3.14s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/270 [02:51<05:08,  2.35s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/270 [02:51<03:54,  1.81s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/270 [02:52<03:03,  1.42s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 142/270 [02:52<02:26,  1.15s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 143/270 [02:53<02:01,  1.04it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144/270 [02:53<01:44,  1.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/270 [02:54<01:31,  1.36it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 146/270 [02:54<01:23,  1.49it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 147/270 [02:55<01:16,  1.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/270 [02:55<01:11,  1.70it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/270 [02:56<01:08,  1.77it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:56<01:06,  1.81it/s]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:56<01:06,  1.81it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 151/270 [02:57<01:04,  1.84it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/270 [02:57<01:03,  1.86it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153/270 [02:58<01:01,  1.89it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 154/270 [02:58<01:00,  1.91it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 155/270 [02:59<01:00,  1.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 156/270 [02:59<00:59,  1.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157/270 [03:00<00:59,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 158/270 [03:00<00:58,  1.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 159/270 [03:01<00:57,  1.93it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 160/270 [03:01<00:56,  1.93it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 161/270 [03:02<00:56,  1.93it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:02<00:51,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.00045308208791539073, 'eval_f1': 1.0, 'eval_runtime': 1.3516, 'eval_samples_per_second': 221.959, 'eval_steps_per_second': 3.699, 'epoch': 5.0}
{'loss': 0.001, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.44it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.64it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.51it/s][A                                                 
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:04<00:51,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.51it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-27 (score: 1.0).
                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:21<00:51,  2.09it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:21<02:14,  1.24s/it]
{'eval_loss': 0.00039952649967744946, 'eval_f1': 1.0, 'eval_runtime': 1.3802, 'eval_samples_per_second': 217.361, 'eval_steps_per_second': 3.623, 'epoch': 6.0}
{'train_runtime': 201.0755, 'train_samples_per_second': 84.545, 'train_steps_per_second': 1.343, 'train_loss': 0.07302925476436446, 'epoch': 6.0}
  0%|          | 0/75 [00:00<?, ?it/s]  7%|â–‹         | 5/75 [00:00<00:01, 48.92it/s] 15%|â–ˆâ–        | 11/75 [00:00<00:01, 49.77it/s] 23%|â–ˆâ–ˆâ–Ž       | 17/75 [00:00<00:01, 50.00it/s] 31%|â–ˆâ–ˆâ–ˆ       | 23/75 [00:00<00:01, 50.12it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 29/75 [00:00<00:00, 50.07it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 35/75 [00:00<00:00, 50.06it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 41/75 [00:00<00:00, 50.11it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 47/75 [00:00<00:00, 50.07it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 53/75 [00:01<00:00, 49.75it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 58/75 [00:01<00:00, 49.72it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 63/75 [00:01<00:00, 49.79it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 69/75 [00:01<00:00, 49.87it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 74/75 [00:01<00:00, 49.84it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 49.88it/s]
Model finished with accuracy: 1.0, macro-f1: 1.0
Confusion matrix:
[[154   0]
 [  0 146]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
explanations.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.94ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.90ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 51.20ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:05<25:28,  5.68s/it]  1%|          | 2/270 [00:06<12:39,  2.83s/it]  1%|          | 3/270 [00:07<07:55,  1.78s/it]  1%|â–         | 4/270 [00:07<05:41,  1.28s/it]  2%|â–         | 5/270 [00:08<04:27,  1.01s/it]  2%|â–         | 6/270 [00:08<03:43,  1.18it/s]  3%|â–Ž         | 7/270 [00:09<03:13,  1.36it/s]  3%|â–Ž         | 8/270 [00:09<02:55,  1.49it/s]  3%|â–Ž         | 9/270 [00:10<02:42,  1.61it/s]  4%|â–Ž         | 10/270 [00:10<02:33,  1.69it/s]  4%|â–         | 11/270 [00:11<02:26,  1.77it/s]  4%|â–         | 12/270 [00:11<02:21,  1.82it/s]  5%|â–         | 13/270 [00:12<02:19,  1.84it/s]  5%|â–Œ         | 14/270 [00:12<02:18,  1.85it/s]  6%|â–Œ         | 15/270 [00:13<02:16,  1.87it/s]  6%|â–Œ         | 16/270 [00:13<02:14,  1.89it/s]  6%|â–‹         | 17/270 [00:14<02:13,  1.90it/s]  7%|â–‹         | 18/270 [00:14<02:12,  1.90it/s]  7%|â–‹         | 19/270 [00:15<02:12,  1.90it/s]  7%|â–‹         | 20/270 [00:15<02:12,  1.89it/s]  8%|â–Š         | 21/270 [00:16<02:23,  1.73it/s]  8%|â–Š         | 22/270 [00:17<02:17,  1.80it/s]  9%|â–Š         | 23/270 [00:17<02:14,  1.84it/s]  9%|â–‰         | 24/270 [00:18<02:12,  1.86it/s]  9%|â–‰         | 25/270 [00:18<02:11,  1.86it/s] 10%|â–‰         | 26/270 [00:19<02:10,  1.88it/s] 10%|â–ˆ         | 27/270 [00:19<01:58,  2.05it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.87it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.92it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.95it/s][A                                                
                                             [A 10%|â–ˆ         | 27/270 [00:20<01:58,  2.05it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.95it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|â–ˆ         | 28/270 [00:37<22:57,  5.69s/it] 11%|â–ˆ         | 29/270 [00:37<16:37,  4.14s/it] 11%|â–ˆ         | 30/270 [00:38<12:13,  3.06s/it] 11%|â–ˆâ–        | 31/270 [00:39<09:07,  2.29s/it] 12%|â–ˆâ–        | 32/270 [00:39<06:59,  1.76s/it] 12%|â–ˆâ–        | 33/270 [00:40<05:29,  1.39s/it] 13%|â–ˆâ–Ž        | 34/270 [00:40<04:26,  1.13s/it] 13%|â–ˆâ–Ž        | 35/270 [00:41<03:42,  1.06it/s] 13%|â–ˆâ–Ž        | 36/270 [00:41<03:11,  1.22it/s] 14%|â–ˆâ–Ž        | 37/270 [00:42<02:50,  1.37it/s] 14%|â–ˆâ–        | 38/270 [00:42<02:35,  1.49it/s] 14%|â–ˆâ–        | 39/270 [00:43<02:24,  1.59it/s] 15%|â–ˆâ–        | 40/270 [00:43<02:16,  1.68it/s] 15%|â–ˆâ–Œ        | 41/270 [00:44<02:10,  1.75it/s] 16%|â–ˆâ–Œ        | 42/270 [00:44<02:06,  1.80it/s] 16%|â–ˆâ–Œ        | 43/270 [00:45<02:03,  1.84it/s] 16%|â–ˆâ–‹        | 44/270 [00:45<02:00,  1.87it/s] 17%|â–ˆâ–‹        | 45/270 [00:46<01:58,  1.90it/s] 17%|â–ˆâ–‹        | 46/270 [00:46<01:57,  1.91it/s] 17%|â–ˆâ–‹        | 47/270 [00:47<01:56,  1.91it/s] 18%|â–ˆâ–Š        | 48/270 [00:47<01:55,  1.91it/s] 18%|â–ˆâ–Š        | 49/270 [00:48<01:55,  1.92it/s] 19%|â–ˆâ–Š        | 50/270 [00:48<01:55,  1.91it/s]                                                 19%|â–ˆâ–Š        | 50/270 [00:48<01:55,  1.91it/s] 19%|â–ˆâ–‰        | 51/270 [00:49<01:55,  1.90it/s] 19%|â–ˆâ–‰        | 52/270 [00:49<01:54,  1.90it/s] 20%|â–ˆâ–‰        | 53/270 [00:50<01:53,  1.91it/s] 20%|â–ˆâ–ˆ        | 54/270 [00:50<01:43,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.016632622107863426, 'eval_f1': 1.0, 'eval_runtime': 1.3127, 'eval_samples_per_second': 228.542, 'eval_steps_per_second': 3.809, 'epoch': 1.0}
{'loss': 0.2073, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.92it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.87it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.92it/s][A                                                
                                             [A 20%|â–ˆâ–ˆ        | 54/270 [00:52<01:43,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.92it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|â–ˆâ–ˆ        | 55/270 [01:08<20:37,  5.76s/it] 21%|â–ˆâ–ˆ        | 56/270 [01:09<14:55,  4.19s/it] 21%|â–ˆâ–ˆ        | 57/270 [01:09<10:57,  3.09s/it] 21%|â–ˆâ–ˆâ–       | 58/270 [01:10<08:10,  2.32s/it] 22%|â–ˆâ–ˆâ–       | 59/270 [01:10<06:15,  1.78s/it] 22%|â–ˆâ–ˆâ–       | 60/270 [01:11<04:54,  1.40s/it] 23%|â–ˆâ–ˆâ–Ž       | 61/270 [01:12<03:58,  1.14s/it] 23%|â–ˆâ–ˆâ–Ž       | 62/270 [01:12<03:18,  1.05it/s] 23%|â–ˆâ–ˆâ–Ž       | 63/270 [01:13<02:50,  1.22it/s] 24%|â–ˆâ–ˆâ–Ž       | 64/270 [01:13<02:30,  1.37it/s] 24%|â–ˆâ–ˆâ–       | 65/270 [01:14<02:16,  1.50it/s] 24%|â–ˆâ–ˆâ–       | 66/270 [01:14<02:07,  1.61it/s] 25%|â–ˆâ–ˆâ–       | 67/270 [01:15<01:59,  1.70it/s] 25%|â–ˆâ–ˆâ–Œ       | 68/270 [01:15<01:54,  1.76it/s] 26%|â–ˆâ–ˆâ–Œ       | 69/270 [01:16<01:51,  1.80it/s] 26%|â–ˆâ–ˆâ–Œ       | 70/270 [01:16<01:48,  1.84it/s] 26%|â–ˆâ–ˆâ–‹       | 71/270 [01:17<01:46,  1.86it/s] 27%|â–ˆâ–ˆâ–‹       | 72/270 [01:17<01:45,  1.88it/s] 27%|â–ˆâ–ˆâ–‹       | 73/270 [01:18<01:45,  1.87it/s] 27%|â–ˆâ–ˆâ–‹       | 74/270 [01:18<01:44,  1.88it/s] 28%|â–ˆâ–ˆâ–Š       | 75/270 [01:19<01:43,  1.89it/s] 28%|â–ˆâ–ˆâ–Š       | 76/270 [01:19<01:41,  1.91it/s] 29%|â–ˆâ–ˆâ–Š       | 77/270 [01:20<01:40,  1.93it/s] 29%|â–ˆâ–ˆâ–‰       | 78/270 [01:20<01:40,  1.92it/s] 29%|â–ˆâ–ˆâ–‰       | 79/270 [01:21<01:39,  1.92it/s] 30%|â–ˆâ–ˆâ–‰       | 80/270 [01:21<01:39,  1.91it/s] 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:22<01:31,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0016175685450434685, 'eval_f1': 1.0, 'eval_runtime': 1.3185, 'eval_samples_per_second': 227.529, 'eval_steps_per_second': 3.792, 'epoch': 2.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.47it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.63it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.60it/s][A                                                
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:23<01:31,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.60it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|â–ˆâ–ˆâ–ˆ       | 82/270 [01:40<18:05,  5.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 83/270 [01:40<13:05,  4.20s/it] 31%|â–ˆâ–ˆâ–ˆ       | 84/270 [01:41<09:35,  3.09s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 85/270 [01:41<07:09,  2.32s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 86/270 [01:42<05:27,  1.78s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 87/270 [01:43<04:16,  1.40s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 88/270 [01:43<03:27,  1.14s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 89/270 [01:44<02:52,  1.05it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/270 [01:44<02:28,  1.21it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/270 [01:45<02:10,  1.37it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 92/270 [01:45<01:59,  1.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 93/270 [01:46<01:50,  1.59it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 94/270 [01:46<01:44,  1.69it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 95/270 [01:47<01:39,  1.76it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/270 [01:47<01:36,  1.81it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/270 [01:48<01:35,  1.82it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 98/270 [01:48<01:33,  1.85it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 99/270 [01:49<01:31,  1.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:49<01:31,  1.87it/s]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:49<01:31,  1.87it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/270 [01:50<01:30,  1.87it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 102/270 [01:50<01:29,  1.87it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 103/270 [01:51<01:28,  1.89it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 104/270 [01:51<01:26,  1.92it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 105/270 [01:52<01:25,  1.92it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 106/270 [01:52<01:25,  1.92it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 107/270 [01:53<01:25,  1.90it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:53<01:18,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0007783928303979337, 'eval_f1': 1.0, 'eval_runtime': 1.3721, 'eval_samples_per_second': 218.644, 'eval_steps_per_second': 3.644, 'epoch': 3.0}
{'loss': 0.0017, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.47it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.59it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.57it/s][A                                                 
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:55<01:18,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.57it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 109/270 [02:11<15:22,  5.73s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 110/270 [02:12<11:06,  4.17s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/270 [02:12<08:09,  3.08s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/270 [02:13<06:04,  2.31s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/270 [02:13<04:37,  1.77s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/270 [02:14<03:37,  1.39s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 115/270 [02:14<02:55,  1.13s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 116/270 [02:15<02:26,  1.05it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 117/270 [02:16<02:05,  1.22it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/270 [02:16<01:51,  1.37it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119/270 [02:17<01:41,  1.49it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120/270 [02:17<01:34,  1.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/270 [02:18<01:29,  1.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 122/270 [02:18<01:25,  1.73it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 123/270 [02:19<01:21,  1.80it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 124/270 [02:19<01:19,  1.83it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 125/270 [02:20<01:18,  1.85it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126/270 [02:20<01:17,  1.86it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 127/270 [02:21<01:16,  1.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 128/270 [02:21<01:15,  1.89it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 129/270 [02:22<01:14,  1.89it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 130/270 [02:22<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/270 [02:23<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 132/270 [02:23<01:12,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 133/270 [02:24<01:11,  1.91it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 134/270 [02:24<01:11,  1.91it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:25<01:05,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0005860310047864914, 'eval_f1': 1.0, 'eval_runtime': 1.3801, 'eval_samples_per_second': 217.375, 'eval_steps_per_second': 3.623, 'epoch': 4.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.34it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.62it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.62it/s][A                                                 
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:26<01:05,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.62it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 136/270 [02:43<12:53,  5.77s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137/270 [02:43<09:17,  4.19s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 138/270 [02:44<06:54,  3.14s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/270 [02:45<05:09,  2.36s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/270 [02:45<03:55,  1.81s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/270 [02:46<03:03,  1.43s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 142/270 [02:46<02:27,  1.15s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 143/270 [02:47<02:01,  1.04it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144/270 [02:47<01:44,  1.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/270 [02:48<01:32,  1.36it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 146/270 [02:48<01:23,  1.48it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 147/270 [02:49<01:17,  1.59it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/270 [02:49<01:12,  1.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/270 [02:50<01:09,  1.75it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:50<01:06,  1.80it/s]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:50<01:06,  1.80it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 151/270 [02:51<01:05,  1.82it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/270 [02:51<01:03,  1.85it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153/270 [02:52<01:02,  1.87it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 154/270 [02:53<01:01,  1.88it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 155/270 [02:53<01:00,  1.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 156/270 [02:54<00:59,  1.91it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157/270 [02:54<00:58,  1.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 158/270 [02:55<00:58,  1.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 159/270 [02:55<00:57,  1.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 160/270 [02:56<00:57,  1.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 161/270 [02:56<00:57,  1.91it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [02:57<00:52,  2.06it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0004846210067626089, 'eval_f1': 1.0, 'eval_runtime': 1.3589, 'eval_samples_per_second': 220.772, 'eval_steps_per_second': 3.68, 'epoch': 5.0}
{'loss': 0.001, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.44it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.61it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.65it/s][A                                                 
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [02:58<00:52,  2.06it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.65it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-27 (score: 1.0).
                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:15<00:52,  2.06it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:15<02:10,  1.21s/it]
{'eval_loss': 0.00042555053369142115, 'eval_f1': 1.0, 'eval_runtime': 1.6175, 'eval_samples_per_second': 185.467, 'eval_steps_per_second': 3.091, 'epoch': 6.0}
{'train_runtime': 195.2884, 'train_samples_per_second': 87.051, 'train_steps_per_second': 1.383, 'train_loss': 0.06484653449467855, 'epoch': 6.0}
  0%|          | 0/75 [00:00<?, ?it/s]  7%|â–‹         | 5/75 [00:00<00:01, 43.06it/s] 13%|â–ˆâ–Ž        | 10/75 [00:00<00:01, 44.77it/s] 20%|â–ˆâ–ˆ        | 15/75 [00:00<00:01, 46.25it/s] 27%|â–ˆâ–ˆâ–‹       | 20/75 [00:00<00:01, 47.34it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 26/75 [00:00<00:01, 48.42it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 31/75 [00:00<00:00, 48.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 37/75 [00:00<00:00, 49.33it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 42/75 [00:00<00:00, 49.43it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 47/75 [00:00<00:00, 49.59it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 52/75 [00:01<00:00, 49.64it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 57/75 [00:01<00:00, 49.67it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 62/75 [00:01<00:00, 49.56it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 67/75 [00:01<00:00, 49.40it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 72/75 [00:01<00:00, 49.40it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 48.80it/s]
Model finished with accuracy: 1.0, macro-f1: 1.0
Confusion matrix:
[[155   0]
 [  0 145]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
explanations.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.08ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.05ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 40.09ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:06<27:22,  6.10s/it]  1%|          | 2/270 [00:07<13:39,  3.06s/it]  1%|          | 3/270 [00:07<08:27,  1.90s/it]  1%|â–         | 4/270 [00:08<06:00,  1.36s/it]  2%|â–         | 5/270 [00:08<04:37,  1.05s/it]  2%|â–         | 6/270 [00:09<03:49,  1.15it/s]  3%|â–Ž         | 7/270 [00:09<03:18,  1.32it/s]  3%|â–Ž         | 8/270 [00:10<02:59,  1.46it/s]  3%|â–Ž         | 9/270 [00:10<02:46,  1.57it/s]  4%|â–Ž         | 10/270 [00:11<02:36,  1.67it/s]  4%|â–         | 11/270 [00:11<02:29,  1.73it/s]  4%|â–         | 12/270 [00:12<02:23,  1.79it/s]  5%|â–         | 13/270 [00:12<02:20,  1.83it/s]  5%|â–Œ         | 14/270 [00:13<02:16,  1.87it/s]  6%|â–Œ         | 15/270 [00:13<02:14,  1.89it/s]  6%|â–Œ         | 16/270 [00:14<02:13,  1.90it/s]  6%|â–‹         | 17/270 [00:14<02:12,  1.90it/s]  7%|â–‹         | 18/270 [00:15<02:11,  1.92it/s]  7%|â–‹         | 19/270 [00:15<02:09,  1.94it/s]  7%|â–‹         | 20/270 [00:16<02:08,  1.94it/s]  8%|â–Š         | 21/270 [00:17<02:20,  1.77it/s]  8%|â–Š         | 22/270 [00:17<02:16,  1.82it/s]  9%|â–Š         | 23/270 [00:18<02:12,  1.86it/s]  9%|â–‰         | 24/270 [00:18<02:10,  1.89it/s]  9%|â–‰         | 25/270 [00:19<02:08,  1.90it/s] 10%|â–‰         | 26/270 [00:19<02:08,  1.90it/s] 10%|â–ˆ         | 27/270 [00:19<01:56,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.77it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.95it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.99it/s][A                                                
                                             [A 10%|â–ˆ         | 27/270 [00:21<01:56,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.99it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|â–ˆ         | 28/270 [00:37<22:58,  5.69s/it] 11%|â–ˆ         | 29/270 [00:38<16:37,  4.14s/it] 11%|â–ˆ         | 30/270 [00:38<12:12,  3.05s/it] 11%|â–ˆâ–        | 31/270 [00:39<09:07,  2.29s/it] 12%|â–ˆâ–        | 32/270 [00:39<06:59,  1.76s/it] 12%|â–ˆâ–        | 33/270 [00:40<05:29,  1.39s/it] 13%|â–ˆâ–Ž        | 34/270 [00:40<04:26,  1.13s/it] 13%|â–ˆâ–Ž        | 35/270 [00:41<03:42,  1.06it/s] 13%|â–ˆâ–Ž        | 36/270 [00:41<03:11,  1.22it/s] 14%|â–ˆâ–Ž        | 37/270 [00:42<02:49,  1.37it/s] 14%|â–ˆâ–        | 38/270 [00:43<02:34,  1.50it/s] 14%|â–ˆâ–        | 39/270 [00:43<02:23,  1.61it/s] 15%|â–ˆâ–        | 40/270 [00:44<02:16,  1.69it/s] 15%|â–ˆâ–Œ        | 41/270 [00:44<02:11,  1.74it/s] 16%|â–ˆâ–Œ        | 42/270 [00:45<02:08,  1.77it/s] 16%|â–ˆâ–Œ        | 43/270 [00:45<02:06,  1.80it/s] 16%|â–ˆâ–‹        | 44/270 [00:46<02:02,  1.84it/s] 17%|â–ˆâ–‹        | 45/270 [00:46<01:59,  1.88it/s] 17%|â–ˆâ–‹        | 46/270 [00:47<01:58,  1.89it/s] 17%|â–ˆâ–‹        | 47/270 [00:47<01:57,  1.90it/s] 18%|â–ˆâ–Š        | 48/270 [00:48<01:56,  1.91it/s] 18%|â–ˆâ–Š        | 49/270 [00:48<01:55,  1.91it/s] 19%|â–ˆâ–Š        | 50/270 [00:49<01:54,  1.93it/s]                                                 19%|â–ˆâ–Š        | 50/270 [00:49<01:54,  1.93it/s] 19%|â–ˆâ–‰        | 51/270 [00:49<01:53,  1.93it/s] 19%|â–ˆâ–‰        | 52/270 [00:50<01:53,  1.92it/s] 20%|â–ˆâ–‰        | 53/270 [00:50<01:52,  1.92it/s] 20%|â–ˆâ–ˆ        | 54/270 [00:51<01:42,  2.11it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.06338081508874893, 'eval_f1': 1.0, 'eval_runtime': 1.3377, 'eval_samples_per_second': 224.268, 'eval_steps_per_second': 3.738, 'epoch': 1.0}
{'loss': 0.2863, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.75it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.80it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.79it/s][A                                                
                                             [A 20%|â–ˆâ–ˆ        | 54/270 [00:52<01:42,  2.11it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.79it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|â–ˆâ–ˆ        | 55/270 [01:09<20:36,  5.75s/it] 21%|â–ˆâ–ˆ        | 56/270 [01:09<14:54,  4.18s/it] 21%|â–ˆâ–ˆ        | 57/270 [01:10<10:56,  3.08s/it] 21%|â–ˆâ–ˆâ–       | 58/270 [01:10<08:11,  2.32s/it] 22%|â–ˆâ–ˆâ–       | 59/270 [01:11<06:15,  1.78s/it] 22%|â–ˆâ–ˆâ–       | 60/270 [01:11<04:53,  1.40s/it] 23%|â–ˆâ–ˆâ–Ž       | 61/270 [01:12<03:56,  1.13s/it] 23%|â–ˆâ–ˆâ–Ž       | 62/270 [01:12<03:16,  1.06it/s] 23%|â–ˆâ–ˆâ–Ž       | 63/270 [01:13<02:49,  1.22it/s] 24%|â–ˆâ–ˆâ–Ž       | 64/270 [01:13<02:30,  1.37it/s] 24%|â–ˆâ–ˆâ–       | 65/270 [01:14<02:17,  1.49it/s] 24%|â–ˆâ–ˆâ–       | 66/270 [01:15<02:07,  1.60it/s] 25%|â–ˆâ–ˆâ–       | 67/270 [01:15<01:59,  1.69it/s] 25%|â–ˆâ–ˆâ–Œ       | 68/270 [01:16<01:54,  1.76it/s] 26%|â–ˆâ–ˆâ–Œ       | 69/270 [01:16<01:51,  1.80it/s] 26%|â–ˆâ–ˆâ–Œ       | 70/270 [01:17<01:49,  1.82it/s] 26%|â–ˆâ–ˆâ–‹       | 71/270 [01:17<01:47,  1.84it/s] 27%|â–ˆâ–ˆâ–‹       | 72/270 [01:18<01:46,  1.86it/s] 27%|â–ˆâ–ˆâ–‹       | 73/270 [01:18<01:44,  1.88it/s] 27%|â–ˆâ–ˆâ–‹       | 74/270 [01:19<01:43,  1.88it/s] 28%|â–ˆâ–ˆâ–Š       | 75/270 [01:19<01:42,  1.91it/s] 28%|â–ˆâ–ˆâ–Š       | 76/270 [01:20<01:41,  1.91it/s] 29%|â–ˆâ–ˆâ–Š       | 77/270 [01:20<01:40,  1.91it/s] 29%|â–ˆâ–ˆâ–‰       | 78/270 [01:21<01:40,  1.91it/s] 29%|â–ˆâ–ˆâ–‰       | 79/270 [01:21<01:39,  1.92it/s] 30%|â–ˆâ–ˆâ–‰       | 80/270 [01:22<01:38,  1.92it/s] 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:22<01:30,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0018107134383171797, 'eval_f1': 1.0, 'eval_runtime': 1.3466, 'eval_samples_per_second': 222.786, 'eval_steps_per_second': 3.713, 'epoch': 2.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.09it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.76it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.71it/s][A                                                
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:24<01:30,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.71it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|â–ˆâ–ˆâ–ˆ       | 82/270 [01:40<18:09,  5.79s/it] 31%|â–ˆâ–ˆâ–ˆ       | 83/270 [01:41<13:07,  4.21s/it] 31%|â–ˆâ–ˆâ–ˆ       | 84/270 [01:41<09:37,  3.10s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 85/270 [01:42<07:11,  2.33s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 86/270 [01:42<05:29,  1.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 87/270 [01:43<04:17,  1.41s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 88/270 [01:44<03:27,  1.14s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 89/270 [01:44<02:52,  1.05it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/270 [01:45<02:27,  1.22it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/270 [01:45<02:10,  1.37it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 92/270 [01:46<01:58,  1.51it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 93/270 [01:46<01:49,  1.62it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 94/270 [01:47<01:43,  1.70it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 95/270 [01:47<01:39,  1.77it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/270 [01:48<01:36,  1.80it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/270 [01:48<01:34,  1.84it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 98/270 [01:49<01:32,  1.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 99/270 [01:49<01:30,  1.88it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:50<01:30,  1.88it/s]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:50<01:30,  1.88it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/270 [01:50<01:29,  1.88it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 102/270 [01:51<01:28,  1.90it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 103/270 [01:51<01:27,  1.91it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 104/270 [01:52<01:26,  1.92it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 105/270 [01:52<01:26,  1.91it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 106/270 [01:53<01:25,  1.92it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 107/270 [01:53<01:25,  1.90it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:54<01:18,  2.06it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0008197426795959473, 'eval_f1': 1.0, 'eval_runtime': 1.3593, 'eval_samples_per_second': 220.708, 'eval_steps_per_second': 3.678, 'epoch': 3.0}
{'loss': 0.0021, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.33it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.65it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.69it/s][A                                                 
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:55<01:18,  2.06it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.69it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 109/270 [02:12<15:22,  5.73s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 110/270 [02:12<11:06,  4.17s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/270 [02:13<08:08,  3.07s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/270 [02:13<06:03,  2.30s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/270 [02:14<04:37,  1.76s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/270 [02:14<03:37,  1.39s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 115/270 [02:15<02:54,  1.13s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 116/270 [02:15<02:24,  1.06it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 117/270 [02:16<02:05,  1.22it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/270 [02:16<01:50,  1.37it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119/270 [02:17<01:41,  1.49it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120/270 [02:17<01:34,  1.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/270 [02:18<01:29,  1.67it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 122/270 [02:18<01:24,  1.74it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 123/270 [02:19<01:21,  1.81it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 124/270 [02:20<01:19,  1.83it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 125/270 [02:20<01:17,  1.86it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126/270 [02:21<01:16,  1.88it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 127/270 [02:21<01:15,  1.89it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 128/270 [02:22<01:14,  1.91it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 129/270 [02:22<01:13,  1.92it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 130/270 [02:23<01:13,  1.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/270 [02:23<01:12,  1.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 132/270 [02:24<01:12,  1.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 133/270 [02:24<01:11,  1.92it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 134/270 [02:25<01:11,  1.91it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:25<01:05,  2.06it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0005987315671518445, 'eval_f1': 1.0, 'eval_runtime': 1.3738, 'eval_samples_per_second': 218.365, 'eval_steps_per_second': 3.639, 'epoch': 4.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.43it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.73it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.79it/s][A                                                 
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:26<01:05,  2.06it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.79it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 136/270 [02:43<13:00,  5.82s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137/270 [02:44<09:23,  4.23s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 138/270 [02:45<06:58,  3.17s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/270 [02:45<05:11,  2.38s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/270 [02:46<03:56,  1.82s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/270 [02:46<03:04,  1.43s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 142/270 [02:47<02:28,  1.16s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 143/270 [02:47<02:02,  1.03it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144/270 [02:48<01:44,  1.20it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/270 [02:48<01:32,  1.35it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 146/270 [02:49<01:24,  1.47it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 147/270 [02:49<01:18,  1.56it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/270 [02:50<01:14,  1.65it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/270 [02:50<01:10,  1.71it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:51<01:07,  1.77it/s]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:51<01:07,  1.77it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 151/270 [02:51<01:05,  1.82it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/270 [02:52<01:03,  1.84it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153/270 [02:53<01:02,  1.86it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 154/270 [02:53<01:02,  1.87it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 155/270 [02:54<01:01,  1.87it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 156/270 [02:54<01:00,  1.88it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157/270 [02:55<00:59,  1.90it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 158/270 [02:55<00:58,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 159/270 [02:56<00:57,  1.92it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 160/270 [02:56<00:57,  1.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 161/270 [02:57<00:57,  1.91it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [02:57<00:52,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.000489412690512836, 'eval_f1': 1.0, 'eval_runtime': 1.3554, 'eval_samples_per_second': 221.335, 'eval_steps_per_second': 3.689, 'epoch': 5.0}
{'loss': 0.0011, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.54it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.77it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.83it/s][A                                                 
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [02:58<00:52,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.83it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-27 (score: 1.0).
                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [05:23<00:52,  2.08it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [05:23<03:35,  2.00s/it]
{'eval_loss': 0.0004291452933102846, 'eval_f1': 1.0, 'eval_runtime': 1.3744, 'eval_samples_per_second': 218.281, 'eval_steps_per_second': 3.638, 'epoch': 6.0}
{'train_runtime': 323.3191, 'train_samples_per_second': 52.58, 'train_steps_per_second': 0.835, 'train_loss': 0.08940091387104289, 'epoch': 6.0}
  0%|          | 0/75 [00:00<?, ?it/s]  7%|â–‹         | 5/75 [00:00<00:01, 42.86it/s] 13%|â–ˆâ–Ž        | 10/75 [00:00<00:01, 44.66it/s] 20%|â–ˆâ–ˆ        | 15/75 [00:00<00:01, 46.12it/s] 27%|â–ˆâ–ˆâ–‹       | 20/75 [00:00<00:01, 47.18it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 26/75 [00:00<00:01, 48.32it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 32/75 [00:00<00:00, 48.92it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 38/75 [00:00<00:00, 49.29it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 44/75 [00:00<00:00, 49.52it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 50/75 [00:01<00:00, 49.73it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 56/75 [00:01<00:00, 49.83it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 61/75 [00:01<00:00, 49.80it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 66/75 [00:01<00:00, 49.62it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 71/75 [00:01<00:00, 49.56it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 48.87it/s]
Model finished with accuracy: 1.0, macro-f1: 1.0
Confusion matrix:
[[137   0]
 [  0 163]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
explanations.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.41ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.38ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 41.75ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:05<25:00,  5.58s/it]  1%|          | 2/270 [00:06<12:44,  2.85s/it]  1%|          | 3/270 [00:07<07:57,  1.79s/it]  1%|â–         | 4/270 [00:07<05:42,  1.29s/it]  2%|â–         | 5/270 [00:08<04:28,  1.01s/it]  2%|â–         | 6/270 [00:08<03:44,  1.18it/s]  3%|â–Ž         | 7/270 [00:09<03:14,  1.35it/s]  3%|â–Ž         | 8/270 [00:09<02:55,  1.49it/s]  3%|â–Ž         | 9/270 [00:10<02:43,  1.59it/s]  4%|â–Ž         | 10/270 [00:10<02:34,  1.68it/s]  4%|â–         | 11/270 [00:11<02:27,  1.76it/s]  4%|â–         | 12/270 [00:11<02:23,  1.80it/s]  5%|â–         | 13/270 [00:12<02:19,  1.85it/s]  5%|â–Œ         | 14/270 [00:12<02:17,  1.86it/s]  6%|â–Œ         | 15/270 [00:13<02:15,  1.89it/s]  6%|â–Œ         | 16/270 [00:13<02:12,  1.92it/s]  6%|â–‹         | 17/270 [00:14<02:11,  1.92it/s]  7%|â–‹         | 18/270 [00:14<02:10,  1.93it/s]  7%|â–‹         | 19/270 [00:15<02:20,  1.78it/s]  7%|â–‹         | 20/270 [00:15<02:16,  1.83it/s]  8%|â–Š         | 21/270 [00:16<02:13,  1.87it/s]  8%|â–Š         | 22/270 [00:17<02:11,  1.89it/s]  9%|â–Š         | 23/270 [00:17<02:09,  1.91it/s]  9%|â–‰         | 24/270 [00:18<02:09,  1.90it/s]  9%|â–‰         | 25/270 [00:18<02:09,  1.89it/s] 10%|â–‰         | 26/270 [00:19<02:08,  1.89it/s] 10%|â–ˆ         | 27/270 [00:19<01:57,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.15it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  7.06it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.99it/s][A                                                
                                             [A 10%|â–ˆ         | 27/270 [00:21<01:57,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.99it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|â–ˆ         | 28/270 [00:37<23:18,  5.78s/it] 11%|â–ˆ         | 29/270 [00:38<16:52,  4.20s/it] 11%|â–ˆ         | 30/270 [00:38<12:23,  3.10s/it] 11%|â–ˆâ–        | 31/270 [00:39<09:14,  2.32s/it] 12%|â–ˆâ–        | 32/270 [00:39<07:03,  1.78s/it] 12%|â–ˆâ–        | 33/270 [00:40<05:32,  1.40s/it] 13%|â–ˆâ–Ž        | 34/270 [00:40<04:29,  1.14s/it] 13%|â–ˆâ–Ž        | 35/270 [00:41<03:44,  1.05it/s] 13%|â–ˆâ–Ž        | 36/270 [00:41<03:13,  1.21it/s] 14%|â–ˆâ–Ž        | 37/270 [00:42<02:51,  1.36it/s] 14%|â–ˆâ–        | 38/270 [00:42<02:36,  1.48it/s] 14%|â–ˆâ–        | 39/270 [00:43<02:24,  1.60it/s] 15%|â–ˆâ–        | 40/270 [00:43<02:15,  1.69it/s] 15%|â–ˆâ–Œ        | 41/270 [00:44<02:10,  1.76it/s] 16%|â–ˆâ–Œ        | 42/270 [00:44<02:05,  1.81it/s] 16%|â–ˆâ–Œ        | 43/270 [00:45<02:04,  1.83it/s] 16%|â–ˆâ–‹        | 44/270 [00:45<02:02,  1.84it/s] 17%|â–ˆâ–‹        | 45/270 [00:46<02:01,  1.85it/s] 17%|â–ˆâ–‹        | 46/270 [00:47<02:00,  1.87it/s] 17%|â–ˆâ–‹        | 47/270 [00:47<01:57,  1.89it/s] 18%|â–ˆâ–Š        | 48/270 [00:48<01:57,  1.89it/s] 18%|â–ˆâ–Š        | 49/270 [00:48<01:55,  1.91it/s] 19%|â–ˆâ–Š        | 50/270 [00:49<01:55,  1.90it/s]                                                 19%|â–ˆâ–Š        | 50/270 [00:49<01:55,  1.90it/s] 19%|â–ˆâ–‰        | 51/270 [00:49<01:55,  1.90it/s] 19%|â–ˆâ–‰        | 52/270 [00:50<01:54,  1.90it/s] 20%|â–ˆâ–‰        | 53/270 [00:50<01:54,  1.90it/s] 20%|â–ˆâ–ˆ        | 54/270 [00:51<01:44,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.030630510300397873, 'eval_f1': 1.0, 'eval_runtime': 1.5347, 'eval_samples_per_second': 195.473, 'eval_steps_per_second': 3.258, 'epoch': 1.0}
{'loss': 0.2488, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.88it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.92it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.98it/s][A                                                
                                             [A 20%|â–ˆâ–ˆ        | 54/270 [00:52<01:44,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.98it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|â–ˆâ–ˆ        | 55/270 [01:09<20:37,  5.75s/it] 21%|â–ˆâ–ˆ        | 56/270 [01:09<14:55,  4.19s/it] 21%|â–ˆâ–ˆ        | 57/270 [01:10<10:57,  3.09s/it] 21%|â–ˆâ–ˆâ–       | 58/270 [01:10<08:11,  2.32s/it] 22%|â–ˆâ–ˆâ–       | 59/270 [01:11<06:15,  1.78s/it] 22%|â–ˆâ–ˆâ–       | 60/270 [01:11<04:54,  1.40s/it] 23%|â–ˆâ–ˆâ–Ž       | 61/270 [01:12<03:57,  1.14s/it] 23%|â–ˆâ–ˆâ–Ž       | 62/270 [01:12<03:17,  1.05it/s] 23%|â–ˆâ–ˆâ–Ž       | 63/270 [01:13<02:49,  1.22it/s] 24%|â–ˆâ–ˆâ–Ž       | 64/270 [01:13<02:29,  1.38it/s] 24%|â–ˆâ–ˆâ–       | 65/270 [01:14<02:15,  1.51it/s] 24%|â–ˆâ–ˆâ–       | 66/270 [01:14<02:06,  1.61it/s] 25%|â–ˆâ–ˆâ–       | 67/270 [01:15<02:00,  1.69it/s] 25%|â–ˆâ–ˆâ–Œ       | 68/270 [01:15<01:55,  1.74it/s] 26%|â–ˆâ–ˆâ–Œ       | 69/270 [01:16<01:51,  1.80it/s] 26%|â–ˆâ–ˆâ–Œ       | 70/270 [01:16<01:49,  1.83it/s] 26%|â–ˆâ–ˆâ–‹       | 71/270 [01:17<01:47,  1.86it/s] 27%|â–ˆâ–ˆâ–‹       | 72/270 [01:17<01:46,  1.87it/s] 27%|â–ˆâ–ˆâ–‹       | 73/270 [01:18<01:44,  1.88it/s] 27%|â–ˆâ–ˆâ–‹       | 74/270 [01:19<01:43,  1.90it/s] 28%|â–ˆâ–ˆâ–Š       | 75/270 [01:19<01:41,  1.92it/s] 28%|â–ˆâ–ˆâ–Š       | 76/270 [01:20<01:40,  1.92it/s] 29%|â–ˆâ–ˆâ–Š       | 77/270 [01:20<01:40,  1.92it/s] 29%|â–ˆâ–ˆâ–‰       | 78/270 [01:21<01:40,  1.92it/s] 29%|â–ˆâ–ˆâ–‰       | 79/270 [01:21<01:39,  1.91it/s] 30%|â–ˆâ–ˆâ–‰       | 80/270 [01:22<01:39,  1.92it/s] 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:22<01:30,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0013564418768510222, 'eval_f1': 1.0, 'eval_runtime': 1.3259, 'eval_samples_per_second': 226.256, 'eval_steps_per_second': 3.771, 'epoch': 2.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.13it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.86it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.80it/s][A                                                
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:23<01:30,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.80it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|â–ˆâ–ˆâ–ˆ       | 82/270 [01:40<18:04,  5.77s/it] 31%|â–ˆâ–ˆâ–ˆ       | 83/270 [01:41<13:04,  4.19s/it] 31%|â–ˆâ–ˆâ–ˆ       | 84/270 [01:41<09:34,  3.09s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 85/270 [01:42<07:10,  2.33s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 86/270 [01:42<05:28,  1.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 87/270 [01:43<04:18,  1.41s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 88/270 [01:43<03:28,  1.14s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 89/270 [01:44<02:52,  1.05it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/270 [01:44<02:28,  1.21it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/270 [01:45<02:10,  1.37it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 92/270 [01:45<01:59,  1.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 93/270 [01:46<01:50,  1.60it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 94/270 [01:46<01:44,  1.69it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 95/270 [01:47<01:39,  1.76it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/270 [01:47<01:36,  1.81it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/270 [01:48<01:34,  1.83it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 98/270 [01:48<01:32,  1.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 99/270 [01:49<01:31,  1.88it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:50<01:30,  1.88it/s]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:50<01:30,  1.88it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/270 [01:50<01:29,  1.88it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 102/270 [01:51<01:28,  1.89it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 103/270 [01:51<01:27,  1.90it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 104/270 [01:52<01:27,  1.90it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 105/270 [01:52<01:26,  1.91it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 106/270 [01:53<01:25,  1.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 107/270 [01:53<01:26,  1.89it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:54<01:19,  2.05it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0006810169434174895, 'eval_f1': 1.0, 'eval_runtime': 1.3283, 'eval_samples_per_second': 225.861, 'eval_steps_per_second': 3.764, 'epoch': 3.0}
{'loss': 0.0018, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.45it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.68it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.72it/s][A                                                 
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:55<01:19,  2.05it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.72it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 109/270 [02:12<15:22,  5.73s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 110/270 [02:12<11:06,  4.17s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/270 [02:13<08:08,  3.08s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/270 [02:13<06:04,  2.31s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/270 [02:14<04:38,  1.78s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/270 [02:14<03:39,  1.40s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 115/270 [02:15<02:56,  1.14s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 116/270 [02:15<02:26,  1.05it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 117/270 [02:16<02:05,  1.22it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/270 [02:16<01:51,  1.37it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119/270 [02:17<01:41,  1.49it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120/270 [02:17<01:34,  1.59it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/270 [02:18<01:28,  1.68it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 122/270 [02:18<01:24,  1.76it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 123/270 [02:19<01:21,  1.80it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 124/270 [02:19<01:19,  1.83it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 125/270 [02:20<01:18,  1.84it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126/270 [02:20<01:17,  1.86it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 127/270 [02:21<01:16,  1.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 128/270 [02:22<01:15,  1.89it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 129/270 [02:22<01:13,  1.91it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 130/270 [02:23<01:13,  1.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/270 [02:23<01:12,  1.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 132/270 [02:24<01:12,  1.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 133/270 [02:24<01:11,  1.92it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 134/270 [02:25<01:10,  1.92it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:25<01:04,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0005102517316117883, 'eval_f1': 1.0, 'eval_runtime': 1.3632, 'eval_samples_per_second': 220.067, 'eval_steps_per_second': 3.668, 'epoch': 4.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.21it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.95it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  5.14it/s][A                                                 
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:27<01:04,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  5.14it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 136/270 [02:43<13:01,  5.83s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137/270 [02:44<09:23,  4.24s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 138/270 [02:44<06:52,  3.12s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/270 [02:45<05:07,  2.35s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/270 [02:45<03:54,  1.80s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/270 [02:46<03:02,  1.42s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 142/270 [02:46<02:26,  1.15s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 143/270 [02:47<02:01,  1.04it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144/270 [02:48<01:44,  1.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/270 [02:48<01:32,  1.36it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 146/270 [02:49<01:23,  1.49it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 147/270 [02:49<01:17,  1.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/270 [02:50<01:12,  1.68it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/270 [02:50<01:10,  1.72it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:51<01:08,  1.75it/s]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:51<01:08,  1.75it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 151/270 [02:51<01:06,  1.78it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/270 [02:52<01:04,  1.83it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153/270 [02:52<01:02,  1.86it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 154/270 [02:53<01:01,  1.88it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 155/270 [02:53<01:01,  1.87it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 156/270 [02:54<01:00,  1.88it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157/270 [02:54<00:59,  1.89it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 158/270 [02:55<00:58,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 159/270 [02:55<00:58,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 160/270 [02:56<00:57,  1.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 161/270 [02:56<00:56,  1.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [02:57<00:51,  2.10it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.000420915283029899, 'eval_f1': 1.0, 'eval_runtime': 1.5219, 'eval_samples_per_second': 197.128, 'eval_steps_per_second': 3.285, 'epoch': 5.0}
{'loss': 0.0009, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 11.11it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.91it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.87it/s][A                                                 
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [02:58<00:51,  2.10it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.87it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-27 (score: 1.0).
                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:15<00:51,  2.10it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:15<02:10,  1.21s/it]
{'eval_loss': 0.00037073856219649315, 'eval_f1': 1.0, 'eval_runtime': 1.3475, 'eval_samples_per_second': 222.627, 'eval_steps_per_second': 3.71, 'epoch': 6.0}
{'train_runtime': 195.5038, 'train_samples_per_second': 86.955, 'train_steps_per_second': 1.381, 'train_loss': 0.0776905196454422, 'epoch': 6.0}
  0%|          | 0/75 [00:00<?, ?it/s]  7%|â–‹         | 5/75 [00:00<00:01, 42.56it/s] 13%|â–ˆâ–Ž        | 10/75 [00:00<00:01, 44.41it/s] 20%|â–ˆâ–ˆ        | 15/75 [00:00<00:01, 45.66it/s] 27%|â–ˆâ–ˆâ–‹       | 20/75 [00:00<00:01, 46.89it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 25/75 [00:00<00:01, 47.89it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 30/75 [00:00<00:00, 48.55it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 35/75 [00:00<00:00, 49.00it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 41/75 [00:00<00:00, 49.36it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 46/75 [00:00<00:00, 49.52it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 52/75 [00:01<00:00, 49.68it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 58/75 [00:01<00:00, 49.80it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 63/75 [00:01<00:00, 49.71it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 68/75 [00:01<00:00, 49.51it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 73/75 [00:01<00:00, 49.47it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 48.70it/s]
Model finished with accuracy: 1.0, macro-f1: 1.0
Confusion matrix:
[[135   0]
 [  0 165]]
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
explanations.csv,   0%|          | 0/2 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.56ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 15.53ba/s]
  0%|          | 0/1 [00:00<?, ?ba/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 41.31ba/s]
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 1700
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 270
  0%|          | 0/270 [00:00<?, ?it/s]/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  0%|          | 1/270 [00:05<25:08,  5.61s/it]  1%|          | 2/270 [00:06<12:53,  2.88s/it]  1%|          | 3/270 [00:07<08:00,  1.80s/it]  1%|â–         | 4/270 [00:07<05:45,  1.30s/it]  2%|â–         | 5/270 [00:08<04:30,  1.02s/it]  2%|â–         | 6/270 [00:08<03:45,  1.17it/s]  3%|â–Ž         | 7/270 [00:09<03:15,  1.35it/s]  3%|â–Ž         | 8/270 [00:09<02:55,  1.50it/s]  3%|â–Ž         | 9/270 [00:10<02:42,  1.61it/s]  4%|â–Ž         | 10/270 [00:10<02:33,  1.69it/s]  4%|â–         | 11/270 [00:11<02:28,  1.75it/s]  4%|â–         | 12/270 [00:11<02:23,  1.80it/s]  5%|â–         | 13/270 [00:12<02:19,  1.84it/s]  5%|â–Œ         | 14/270 [00:12<02:16,  1.88it/s]  6%|â–Œ         | 15/270 [00:13<02:14,  1.90it/s]  6%|â–Œ         | 16/270 [00:13<02:14,  1.89it/s]  6%|â–‹         | 17/270 [00:14<02:13,  1.89it/s]  7%|â–‹         | 18/270 [00:14<02:13,  1.89it/s]  7%|â–‹         | 19/270 [00:15<02:11,  1.91it/s]  7%|â–‹         | 20/270 [00:15<02:09,  1.93it/s]  8%|â–Š         | 21/270 [00:16<02:20,  1.77it/s]  8%|â–Š         | 22/270 [00:17<02:18,  1.79it/s]  9%|â–Š         | 23/270 [00:17<02:15,  1.82it/s]  9%|â–‰         | 24/270 [00:18<02:13,  1.85it/s]  9%|â–‰         | 25/270 [00:18<02:10,  1.88it/s] 10%|â–‰         | 26/270 [00:19<02:07,  1.91it/s] 10%|â–ˆ         | 27/270 [00:19<01:56,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.91it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.81it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.72it/s][A                                                
                                             [A 10%|â–ˆ         | 27/270 [00:21<01:56,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.72it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-27
Configuration saved in training-output/checkpoint-27/config.json
Model weights saved in training-output/checkpoint-27/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-27/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-27/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 10%|â–ˆ         | 28/270 [00:37<23:09,  5.74s/it] 11%|â–ˆ         | 29/270 [00:38<16:46,  4.18s/it] 11%|â–ˆ         | 30/270 [00:38<12:19,  3.08s/it] 11%|â–ˆâ–        | 31/270 [00:39<09:13,  2.31s/it] 12%|â–ˆâ–        | 32/270 [00:39<07:02,  1.77s/it] 12%|â–ˆâ–        | 33/270 [00:40<05:30,  1.39s/it] 13%|â–ˆâ–Ž        | 34/270 [00:40<04:27,  1.13s/it] 13%|â–ˆâ–Ž        | 35/270 [00:41<03:42,  1.05it/s] 13%|â–ˆâ–Ž        | 36/270 [00:41<03:13,  1.21it/s] 14%|â–ˆâ–Ž        | 37/270 [00:42<02:51,  1.36it/s] 14%|â–ˆâ–        | 38/270 [00:42<02:36,  1.49it/s] 14%|â–ˆâ–        | 39/270 [00:43<02:24,  1.60it/s] 15%|â–ˆâ–        | 40/270 [00:43<02:15,  1.69it/s] 15%|â–ˆâ–Œ        | 41/270 [00:44<02:11,  1.74it/s] 16%|â–ˆâ–Œ        | 42/270 [00:44<02:07,  1.79it/s] 16%|â–ˆâ–Œ        | 43/270 [00:45<02:04,  1.82it/s] 16%|â–ˆâ–‹        | 44/270 [00:45<02:01,  1.86it/s] 17%|â–ˆâ–‹        | 45/270 [00:46<01:59,  1.89it/s] 17%|â–ˆâ–‹        | 46/270 [00:47<01:58,  1.90it/s] 17%|â–ˆâ–‹        | 47/270 [00:47<01:57,  1.90it/s] 18%|â–ˆâ–Š        | 48/270 [00:48<01:56,  1.90it/s] 18%|â–ˆâ–Š        | 49/270 [00:48<01:55,  1.91it/s] 19%|â–ˆâ–Š        | 50/270 [00:49<01:55,  1.91it/s]                                                 19%|â–ˆâ–Š        | 50/270 [00:49<01:55,  1.91it/s] 19%|â–ˆâ–‰        | 51/270 [00:49<01:54,  1.90it/s] 19%|â–ˆâ–‰        | 52/270 [00:50<01:53,  1.91it/s] 20%|â–ˆâ–‰        | 53/270 [00:50<01:53,  1.91it/s] 20%|â–ˆâ–ˆ        | 54/270 [00:51<01:44,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.06250573694705963, 'eval_f1': 1.0, 'eval_runtime': 1.4506, 'eval_samples_per_second': 206.812, 'eval_steps_per_second': 3.447, 'epoch': 1.0}
{'loss': 0.2865, 'learning_rate': 9.177439057064684e-06, 'epoch': 1.85}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.50it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.64it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.56it/s][A                                                
                                             [A 20%|â–ˆâ–ˆ        | 54/270 [00:52<01:44,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.56it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-54
Configuration saved in training-output/checkpoint-54/config.json
Model weights saved in training-output/checkpoint-54/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-54/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-54/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 20%|â–ˆâ–ˆ        | 55/270 [01:09<20:33,  5.74s/it] 21%|â–ˆâ–ˆ        | 56/270 [01:09<14:53,  4.17s/it] 21%|â–ˆâ–ˆ        | 57/270 [01:10<10:55,  3.08s/it] 21%|â–ˆâ–ˆâ–       | 58/270 [01:10<08:10,  2.31s/it] 22%|â–ˆâ–ˆâ–       | 59/270 [01:11<06:15,  1.78s/it] 22%|â–ˆâ–ˆâ–       | 60/270 [01:11<04:54,  1.40s/it] 23%|â–ˆâ–ˆâ–Ž       | 61/270 [01:12<03:57,  1.14s/it] 23%|â–ˆâ–ˆâ–Ž       | 62/270 [01:12<03:18,  1.05it/s] 23%|â–ˆâ–ˆâ–Ž       | 63/270 [01:13<02:51,  1.21it/s] 24%|â–ˆâ–ˆâ–Ž       | 64/270 [01:13<02:32,  1.35it/s] 24%|â–ˆâ–ˆâ–       | 65/270 [01:14<02:18,  1.48it/s] 24%|â–ˆâ–ˆâ–       | 66/270 [01:14<02:08,  1.59it/s] 25%|â–ˆâ–ˆâ–       | 67/270 [01:15<02:00,  1.68it/s] 25%|â–ˆâ–ˆâ–Œ       | 68/270 [01:15<01:55,  1.75it/s] 26%|â–ˆâ–ˆâ–Œ       | 69/270 [01:16<01:52,  1.79it/s] 26%|â–ˆâ–ˆâ–Œ       | 70/270 [01:16<01:50,  1.82it/s] 26%|â–ˆâ–ˆâ–‹       | 71/270 [01:17<01:48,  1.84it/s] 27%|â–ˆâ–ˆâ–‹       | 72/270 [01:18<01:47,  1.83it/s] 27%|â–ˆâ–ˆâ–‹       | 73/270 [01:18<01:46,  1.85it/s] 27%|â–ˆâ–ˆâ–‹       | 74/270 [01:19<01:45,  1.86it/s] 28%|â–ˆâ–ˆâ–Š       | 75/270 [01:19<01:44,  1.87it/s] 28%|â–ˆâ–ˆâ–Š       | 76/270 [01:20<01:42,  1.89it/s] 29%|â–ˆâ–ˆâ–Š       | 77/270 [01:20<01:41,  1.90it/s] 29%|â–ˆâ–ˆâ–‰       | 78/270 [01:21<01:41,  1.90it/s] 29%|â–ˆâ–ˆâ–‰       | 79/270 [01:21<01:39,  1.91it/s] 30%|â–ˆâ–ˆâ–‰       | 80/270 [01:22<01:39,  1.91it/s] 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:22<01:30,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0014452325413003564, 'eval_f1': 1.0, 'eval_runtime': 1.3917, 'eval_samples_per_second': 215.565, 'eval_steps_per_second': 3.593, 'epoch': 2.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.48it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.64it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.65it/s][A                                                
                                             [A 30%|â–ˆâ–ˆâ–ˆ       | 81/270 [01:23<01:30,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.65it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-81
Configuration saved in training-output/checkpoint-81/config.json
Model weights saved in training-output/checkpoint-81/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-81/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-81/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 30%|â–ˆâ–ˆâ–ˆ       | 82/270 [01:40<18:11,  5.81s/it] 31%|â–ˆâ–ˆâ–ˆ       | 83/270 [01:41<13:09,  4.22s/it] 31%|â–ˆâ–ˆâ–ˆ       | 84/270 [01:41<09:38,  3.11s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 85/270 [01:42<07:11,  2.34s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 86/270 [01:42<05:29,  1.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 87/270 [01:43<04:17,  1.41s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 88/270 [01:43<03:27,  1.14s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 89/270 [01:44<02:52,  1.05it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 90/270 [01:44<02:27,  1.22it/s] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 91/270 [01:45<02:10,  1.37it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 92/270 [01:46<01:59,  1.49it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 93/270 [01:46<01:51,  1.59it/s] 35%|â–ˆâ–ˆâ–ˆâ–      | 94/270 [01:47<01:45,  1.66it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 95/270 [01:47<01:40,  1.73it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/270 [01:48<01:36,  1.79it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/270 [01:48<01:34,  1.83it/s] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 98/270 [01:49<01:32,  1.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 99/270 [01:49<01:31,  1.86it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:50<01:30,  1.87it/s]                                                  37%|â–ˆâ–ˆâ–ˆâ–‹      | 100/270 [01:50<01:30,  1.87it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 101/270 [01:50<01:29,  1.88it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 102/270 [01:51<01:28,  1.90it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 103/270 [01:51<01:26,  1.92it/s] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 104/270 [01:52<01:27,  1.90it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 105/270 [01:52<01:26,  1.90it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 106/270 [01:53<01:25,  1.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 107/270 [01:53<01:25,  1.91it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:54<01:17,  2.08it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0006844494491815567, 'eval_f1': 1.0, 'eval_runtime': 1.3485, 'eval_samples_per_second': 222.469, 'eval_steps_per_second': 3.708, 'epoch': 3.0}
{'loss': 0.0024, 'learning_rate': 6.980398830195785e-06, 'epoch': 3.7}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.20it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.75it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.68it/s][A                                                 
                                             [A 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 108/270 [01:55<01:17,  2.08it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.68it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-108
Configuration saved in training-output/checkpoint-108/config.json
Model weights saved in training-output/checkpoint-108/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-108/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-108/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 109/270 [02:12<15:41,  5.85s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 110/270 [02:13<11:19,  4.25s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 111/270 [02:13<08:17,  3.13s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 112/270 [02:14<06:10,  2.35s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 113/270 [02:14<04:42,  1.80s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/270 [02:15<03:40,  1.41s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 115/270 [02:15<02:57,  1.15s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 116/270 [02:16<02:28,  1.04it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 117/270 [02:16<02:06,  1.21it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 118/270 [02:17<01:51,  1.36it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 119/270 [02:17<01:40,  1.50it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 120/270 [02:18<01:33,  1.60it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 121/270 [02:18<01:28,  1.68it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 122/270 [02:19<01:25,  1.74it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 123/270 [02:19<01:22,  1.78it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 124/270 [02:20<01:19,  1.83it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 125/270 [02:20<01:18,  1.86it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 126/270 [02:21<01:17,  1.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 127/270 [02:22<01:16,  1.87it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 128/270 [02:22<01:15,  1.88it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 129/270 [02:23<01:14,  1.89it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 130/270 [02:23<01:13,  1.90it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/270 [02:24<01:12,  1.91it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 132/270 [02:24<01:11,  1.92it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 133/270 [02:25<01:11,  1.91it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 134/270 [02:25<01:11,  1.91it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:26<01:04,  2.09it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0004990987945348024, 'eval_f1': 1.0, 'eval_runtime': 1.4709, 'eval_samples_per_second': 203.959, 'eval_steps_per_second': 3.399, 'epoch': 4.0}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.85it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.90it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.95it/s][A                                                 
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 135/270 [02:27<01:04,  2.09it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.95it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-135
Configuration saved in training-output/checkpoint-135/config.json
Model weights saved in training-output/checkpoint-135/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-135/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-135/special_tokens_map.json
/vol/bitbucket/rp218/thesis-venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 136/270 [02:44<13:02,  5.84s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 137/270 [02:44<09:24,  4.24s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 138/270 [02:45<06:52,  3.12s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 139/270 [02:45<05:06,  2.34s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 140/270 [02:46<03:53,  1.80s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 141/270 [02:46<03:02,  1.41s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 142/270 [02:47<02:26,  1.15s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 143/270 [02:48<02:01,  1.04it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 144/270 [02:48<01:44,  1.21it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 145/270 [02:49<01:31,  1.36it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 146/270 [02:49<01:22,  1.51it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 147/270 [02:50<01:16,  1.60it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 148/270 [02:50<01:12,  1.69it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 149/270 [02:51<01:09,  1.75it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:51<01:06,  1.80it/s]                                                  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 150/270 [02:51<01:06,  1.80it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 151/270 [02:52<01:04,  1.84it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 152/270 [02:52<01:03,  1.87it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 153/270 [02:53<01:02,  1.88it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 154/270 [02:53<01:01,  1.89it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 155/270 [02:54<01:00,  1.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 156/270 [02:54<01:00,  1.90it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 157/270 [02:55<00:59,  1.90it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 158/270 [02:55<00:58,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 159/270 [02:56<00:58,  1.91it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 160/270 [02:56<00:57,  1.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 161/270 [02:57<00:56,  1.92it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [02:57<00:52,  2.07it/s]***** Running Evaluation *****
  Num examples = 300
  Batch size = 32
{'eval_loss': 0.0004065000975970179, 'eval_f1': 1.0, 'eval_runtime': 1.3432, 'eval_samples_per_second': 223.348, 'eval_steps_per_second': 3.722, 'epoch': 5.0}
{'loss': 0.001, 'learning_rate': 4.131759111665349e-06, 'epoch': 5.56}

  0%|          | 0/5 [00:00<?, ?it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 10.97it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  6.75it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00,  6.67it/s][A                                                 
                                             [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [02:59<00:52,  2.07it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  6.67it/s][A
                                             [ASaving model checkpoint to training-output/checkpoint-162
Configuration saved in training-output/checkpoint-162/config.json
Model weights saved in training-output/checkpoint-162/pytorch_model.bin
tokenizer config file saved in training-output/checkpoint-162/tokenizer_config.json
Special tokens file saved in training-output/checkpoint-162/special_tokens_map.json


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from training-output/checkpoint-27 (score: 1.0).
                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:15<00:52,  2.07it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 162/270 [03:15<02:10,  1.21s/it]
{'eval_loss': 0.0003572871210053563, 'eval_f1': 1.0, 'eval_runtime': 1.3711, 'eval_samples_per_second': 218.803, 'eval_steps_per_second': 3.647, 'epoch': 6.0}
{'train_runtime': 195.9279, 'train_samples_per_second': 86.767, 'train_steps_per_second': 1.378, 'train_loss': 0.0895161860116562, 'epoch': 6.0}
  0%|          | 0/75 [00:00<?, ?it/s]  8%|â–Š         | 6/75 [00:00<00:01, 50.48it/s] 16%|â–ˆâ–Œ        | 12/75 [00:00<00:01, 50.19it/s] 24%|â–ˆâ–ˆâ–       | 18/75 [00:00<00:01, 50.17it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 24/75 [00:00<00:01, 50.20it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 30/75 [00:00<00:00, 50.19it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 36/75 [00:00<00:00, 50.12it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 42/75 [00:00<00:00, 50.10it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 48/75 [00:00<00:00, 49.80it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 53/75 [00:01<00:00, 49.59it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 58/75 [00:01<00:00, 49.67it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 64/75 [00:01<00:00, 49.86it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 70/75 [00:01<00:00, 49.95it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 49.96it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:01<00:00, 49.96it/s]
Model finished with accuracy: 1.0, macro-f1: 1.0
Confusion matrix:
[[144   0]
 [  0 156]]
 21:35:09 up 94 days,  9:52,  2 users,  load average: 30,91, 30,76, 30,76
